---
title: "Efficient and flexible simulation-based sample size determination for clinical trials with multiple design parameters"
author: D. T. Wilson
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(ggplot2)
require(gganimate)
require(reshape2)
require(pso)
require(mco)
require(randtoolbox)
require(mvtnorm)
require(DiceKriging)
require(RColorBrewer)
require(lme4)
require(xtable)
colours <- brewer.pal(8, "Dark2") 
set.seed(90307)
```

## Introduction

This RMarkdown document contains the R code which generates the results and figures included in the manuscript of the same name, plus supplementary material. We introduce the common functions `best()` and `exp_imrprove()` in the context of the first example, and call these in the remaining two examples. Simulated data have been saved to file.

## Example 1

### Simulation

The simulation model is:

```{r}
sim_trial <- function(x, h)
{
  ## x = design parameters, (n, k)
  ## h = hypothesis parameters, (r_t, r_d, v_w, p0, p1)
  
  n <- x[1]; k <- x[2]
  j <- 2*k
  
  r_t <- h[1]; r_d <- h[2]; v_w <- h[3]
  p0 <- h[4]; p1 <- h[5]
  
  ## Linear predicter logit terms
  lp0 <- log(p0/(1-p0)); lp1 <- log(p1/(1-p1)) - lp0
  
  ## Doctor effects
  v_d <- r_d*v_w/(1-r_d)
  doc <- cbind(seq(1,j), rnorm(j, 0, sqrt(v_d)))
  
  ## Therapist effects
  v_t <- (r_t*v_d + r_t*v_w)/(1-r_t)
  ther <- cbind(seq(0,k), c(0, rnorm(k, 0, sqrt(v_t))))

  ## Treatment group
  trt <- c(rep(0, n), rep(1, n))
  ## Patient ID
  p_id <- seq(1, 2*n)
  ## Doctor allocation and effect
  z <- rgamma(j, 1)
  ms <- round(z*2*n/sum(z))
  ms[which.max(ms)] <- ms[which.max(ms)] + 2*n - sum(ms)
  d_id <- sample(rep(doc[,1], ms))
  d_eff <- doc[d_id,2]
  ## Therapist allocation and effect
  z <- rgamma(k, 1)
  ms <- round(z*n/sum(z))
  ms[which.max(ms)] <- ms[which.max(ms)] + n - sum(ms)
  t_id <- c(rep(0, n), sample(rep(ther[2:(k+1),1], ms)))
  t_eff <- ther[t_id+1,2]
  ## Residual
  resid <- rlogis(2*n, 0, 1) ## Note - scale parameter = 1 ===> variance = 3.29
  
  data <- cbind(trt,p_id,d_id,d_eff,t_id,t_eff,resid)
  ## Latent variable
  data <- cbind(data, apply(data, 1, function(y) lp0 + y[1]*lp1 + y[4] + y[6] + y[7]))
  ## Outcome
  data <- cbind(data, data[,8] > 0)
    
  ## Analyse the data
  df <- as.data.frame(data) 
  names(df)[9] <- "y"
  
  result1 <- tryCatch({
    fit1 <- suppressMessages(glmer(y ~ trt + (0 + trt|t_id) + (1|d_id), family = "binomial", data=df))
    fit2 <- suppressMessages(glmer(y ~ (0 + trt|t_id) + (1|d_id), family = "binomial", data=df))
    p <- anova(fit1, fit2)[2,8]
  }, warning = function(war) {
    ## warning handler picks up where error was generated
    return(TRUE)
  }, error = function(err) {
    ## error handler picks up where error was generated
    return(TRUE)
  }, finally = {

  })
  
  if(result1==TRUE){
    return(c(1,1))
  } else {
    return(c(result1, 0))
  }

}

## For example,
sim_trial(x=c(n=150, k=10), h=c(r_t=0.05, r_d=0.1, v_w=3.29, p0=0.1, p1=0.25))
```

Given this simulation, we can then evaluate the probability of failing to reject the null hypothesis of any design `x` under any hypothesis `h`, based on `N` MC samples:

```{r}
calc_rates <- function(x, h, N)
{
  sims <- replicate(N, sim_trial(x, h) > 0.05)
  z <- sims[1,]
  ## Print out model failure rate
  print(sum(sims[2,])/N)
  ## Return proportion not rejecting null and approximate variance in estimate
  c(mean(z), var(z)/N)
}

## For example,
x <- c(n=135, k=10)
h <- c(r_t=0.05, r_d=0.1, v_w=3.29, p0=0.1, p1=0.25)

ptm <- proc.time()
calc_rates(x, h, 10)
proc.time() - ptm
```

### Initial design and GP

The first step of our method is to evaluate at an initial set of design points.

```{r}
design_space <- data.frame(name=c("n","k"), 
                           low=c(100,3), 
                           up=c(500,30)
)

DoE_num <- 20
dim <- nrow(design_space)
  
## Choose initial points using a Sobol sequence
DoE <- data.frame(sobol(DoE_num, dim))
names(DoE) <- design_space$name
for(i in 1:dim){
  DoE[,i] <-  DoE[,i]*(design_space$up[i]-design_space$low[i]) + design_space$low[i]
}
DoE[,1:2] <- round(DoE[,1:2])

## Evaluate at initial points [NOT RUN]
#N <- 100
#DoE <- cbind(DoE, t(apply(DoE, 1, calc_rates, N=N, h=h)))
#names(DoE)[3:4] <- c("beta", "beta_var")
#DoE$N <- N

## Save initial design
#saveRDS(DoE, "DoE_ex1_20_N100.Rda")
```

Given these estimates, we can model the power function over the full design space using a GP. We can then plot both the mean function and the standard deviation for each point.

```{r}
## Load the DoE
DoE <- readRDS("./data/DoE_ex1_20_N100.Rda")

constraints <- data.frame(name=c("beta"), 
                          hyp=c("H1"), 
                          nom=c(0.1),
                          delta=c(0.975))

models <- list()
for(i in 1:nrow(constraints)){
  name <- constraints$name[i]
  models <- append(models, km(~1, design=DoE[1:dim], response=DoE[,as.character(name)], 
                             noise.var=DoE[,paste0(as.character(name), "_var")]))
  names(models)[length(models)] <- as.character(name)
}

mod <- models[[1]]

df <- expand.grid(n = seq(100,500,1), k=3:30)
pred <- predict(mod, newdata=df, type="SK")
df$beta <- pred$mean
df$sd <- pred$sd
```

```{r}
## Plot the mean function
ggplot(df, aes(n, k, z=1-beta)) + geom_contour(aes(colour=..level..)) + geom_point(data=DoE, aes(colour=1-beta)) +
  scale_colour_gradientn(colours=rainbow(3)) + theme_minimal() + guides(colour=guide_legend(title="Power"))
## Plot the standard deviation
ggplot(df, aes(n, k)) + geom_contour(aes(z=sd, colour=..level..)) + geom_point(data=DoE) +
  theme_minimal() + guides(colour=guide_legend(title="SD"))
```

Note that the standard deviation of the GP prediction increases as we move away from evaluated points.

### Pareto fronts

Of the points which we have evaluated, We can identify which lead to sufficient power and which can be discarded because they are dominated by another feasible solution.

```{r}
## Set up our objectives
dim <- nrow(design_space)
nobj <- 2

obj_names <- NULL
for(i in 1:nobj){
  obj_names <- c(obj_names, paste0("f",i))
}
  
objective <- function(x)
{
  c(x[1]*2/5, x[2])
}

best <- function(design_space, models, DoE, b=NULL)
{ 
  ## Return the set of current Patero optimal solutions,
  ## penalising constrain violations considering only solutions
  ## where some evaluation has actually happened
  sols <- DoE
  
  ## Get objective values
  if(nobj > 1){
    sols <- cbind(sols, t(apply(sols, 1, objective)))
  } else {
    sols <- cbind(sols, apply(sols, 1, objective))
  }
  names(sols)[(ncol(sols)-nobj+1):ncol(sols)] <- obj_names
  
  ## Penalise constraint violations
  sols$exp_pen <- 1
  for(i in 1:nrow(constraints)){
    mod <- models[[i]]
    nom <- constraints$nom[i]
    p <- predict.km(mod, newdata=sols[,1:dim, drop=F], type="SK")
    pen <- pnorm(nom, p$mean, p$sd)
    pen <- ifelse(pen < constraints[i,4], 0.0000001, 1)
    sols$exp_pen <- sols$exp_pen*pen
  }
  sols[,obj_names] <- sols[,obj_names]/sols$exp_pen
  
  ## Drop any dominated solutions
  is_nondom <- function(x, b)
  {
    i <- 1
    while(i <= nobj & nrow(b)!= 0 ){
      ## subset b to those non-dominated solutions which are less than or equal to 
      ## x in an objective
      b <- b[b[, obj_names[i] ] <= x[[ obj_names[i] ]],]
      i <- i + 1
    }
    ## If b is now empty, then des is non-dominated
    if(nrow(b)==1 | all(apply(b[,obj_names, drop=F], 2, function(x) length(unique(x)) == 1) == TRUE) ) {
      nondom <- TRUE
    } else {
      nondom <- FALSE
    }
    return(nondom)
  }
  
  nondom_sols <- sols[apply(sols, 1, is_nondom, b=sols), ]
  ## check for duplicates
  sub <- unique(nondom_sols[,obj_names, drop=F])
  
  return(nondom_sols[row.names(sub),])
}

b <- best(design_space, models, DoE)

## Add extreme points for plotting the Pareto front
b2 <- b[,1:2]
b2 <- rbind(c(min(b2[,1]), 30), b2, c(500, min(b2[,2])))

ggplot(df, aes(n, k)) + geom_contour(aes(z=beta)) + geom_point(data=DoE) +
  geom_point(data=b2[2:(nrow(b2)-1),], colour="red") + 
  geom_step(data=b2, colour="red", linetype=2) +
  theme_minimal() 
```

### Expected improvement

We have identified a set of designs which we are ceratin will give nominal power and which are non-dominated. The task now is to improve this approximation set iteratively, evaluating one design at a time, updating our GP model, and then updating the set. When we choose the next point to evaluate we do so to maximimse the expected improvement which will result.

```{r}
exp_improve <- function(design, N, p_set, models, design_space, constraints)
{
  names(design) <- design_space$name
  design <- as.data.frame(t(design))
  
  ## Get expected penalisation if we were to evaluate at design,
  ## using the models of constraint functions
  design$exp_pen <- 1
  for(i in 1:nrow(constraints)){
    mod <- models[[i]]
    nom <- constraints$nom[i]
    p <- predict.km(mod, newdata=design[,1:dim, drop=F], type="SK")
    means <- ifelse(p$mean < 0, 0, ifelse(p$mean > 1, 1, p$mean))
    mc_vars <- means*(1-means)/N
    pred_q_mean <- p$mean + qnorm(constraints[i,4])*sqrt(mc_vars*(p$sd^2)/(mc_vars+(p$sd^2)))
    pred_q_var <- ((p$sd^2)^2)/(mc_vars+(p$sd^2)) 
    design$exp_pen <- design$exp_pen*pnorm(nom, pred_q_mean, sqrt(pred_q_var))
  }
  
  ## Get objective value of design
  design <- cbind(design, t(objective(as.numeric(design))))
  names(design)[(ncol(design)-nobj+1):ncol(design)] <- obj_names
  
  ## Improvement is quantified by the number of additional
  ## solutions which would be dominated if this design was included
  ref <- objective(design_space$up)
  p_set2 <- as.matrix(p_set[,obj_names])
  current <- dominatedHypervolume(p_set2, ref)
  pos <- apply(design, 1, function(d) dominatedHypervolume(as.matrix(rbind(p_set2, d[obj_names])), ref) )
  imp <- (current-pos)*design$exp_pen
  
  ## Minimising, so keeping negative
  return(imp)
}
```


```{r, eval=F}
## Find the next point to evaluate, using a particlae swarm algorithm:
opt <- psoptim(rep(NA, 2), exp_improve, lower=design_space$low, upper=design_space$up,
               N=100, p_set=b, models=models, design_space=design_space, constraints=constraints,
               control=list(vectorize = T, max.restart=1, reltol=0.0001, trace=1))
sol <- opt$par
sol[1:2] <- round(sol[1:2])

## Evaluate the suggested point
y <- calc_rates(sol, h, N=100)
```

```{r}
## Hard code the results
sol <- c(304, 12)
y <- c(0.06, 0.000569697)
```


The recommended evaluation is at $n=$ `r sol[1]`, $k=$ `r sol[2]`. Evaluating the point using $N=100$ MC samples gives an estimated type II error rate of `r y[1]` (sd `r sqrt(y[2])`), so it appears to be feasible. We can add the new information to our records, update the GP model, and then use its predictions to confirm whether or not the new point can join the approximation set.

```{r}
DoE <- rbind(DoE, c(sol, y, 100))

for(i in 1:length(models)){
    models[[i]] <- update(models[[i]], 
                          newX=DoE[nrow(DoE),1:2], 
                          newy=DoE[nrow(DoE), 2*i+1], 
                          newnoise.var=DoE[nrow(DoE), 2*i+2])
}

b <- best(design_space, models, DoE)
```

### Algorithm

The above process can be repeated for the desired number of iterations. Here start from the beginning, with another initial design of 20 points followed by 30 iterations of the algorithm.

```{r, eval=F}
ptm <- proc.time()

design_space <- data.frame(name=c("n","k"), 
                           low=c(100,3), 
                           up=c(500,30)
)

constraints <- data.frame(name=c("beta"), 
                          hyp=c("H1"), 
                          nom=c(0.1),
                          delta=c(0.975))

DoE_num <- 20
dim <- nrow(design_space)
ref <- objective(design_space$up)
  
## Choose initial points
DoE <- data.frame(sobol(DoE_num, dim))
names(DoE) <- design_space$name
for(i in 1:dim){
  DoE[,i] <-  DoE[,i]*(design_space$up[i]-design_space$low[i]) + design_space$low[i]
}
DoE[,1:2] <- round(DoE[,1:2])

## Evaluate at initial points
N <- 100
DoE <- cbind(DoE, t(apply(DoE, 1, calc_rates, h=h, N=N)))
names(DoE)[3:4] <- c("beta", "beta_var")
DoE$N <- N

models <- list()
for(i in 1:nrow(constraints)){
  name <- constraints$name[i]
  models <- append(models, km(~1, design=DoE[1:dim], response=DoE[,as.character(name)], 
                             noise.var=DoE[,paste0(as.character(name), "_var")]))
  names(models)[length(models)] <- as.character(name)
}

b <- best(design_space, models, DoE)

DHs <- NULL

for(i in 1:30){
  opt <- psoptim(rep(NA, 2), exp_improve, lower=design_space$low, upper=design_space$up,
                 N=100, p_set=b, mod=models, design_space=design_space, constraints=constraints,
                 control=list(vectorize = T))
  sol <- opt$par
  sol[1:2] <- round(sol[1:2])
  
  ## track the objective value at each step
  p_set2 <- as.matrix(b[,obj_names])
  current <- dominatedHypervolume(p_set2, ref)
  DHs <- c(DHs, dominatedHypervolume(p_set2, ref))
  
  ## Do the evaluation and add to the design
  y <- calc_rates(sol, h, N=100)
  
  DoE <- rbind(DoE, c(sol, y, 100))
  
  models <- list()
  for(i in 1:nrow(constraints)){
    name <- constraints$name[i]
    models <- append(models, km(~1, design=DoE[1:dim], response=DoE[,as.character(name)], 
                               noise.var=DoE[,paste0(as.character(name), "_var")]))
    names(models)[length(models)] <- as.character(name)
  }
  
  b <- best(design_space, models, DoE)
}

proc.time() - ptm
## 46.759 minutes

## Save the dominated hypervolumes of the approximation sets at each iteration
#saveRDS(DHs, "./data/ex1_single_run_DHs.Rda")

## Save the final DoE table containing all the points evaluated over the whole algorithm
#saveRDS(DoE, "./data/ex1_single_run_DoE.Rda")
```

For comparison, the fixed 50 point Sobol experimental design.
```{r, eval=F}
## Choose initial points
DoE_num <- 50
DoE <- data.frame(sobol(DoE_num, dim))
names(DoE) <- design_space$name
for(i in 1:dim){
  DoE[,i] <-  DoE[,i]*(design_space$up[i]-design_space$low[i]) + design_space$low[i]
}
DoE[,1:2] <- round(DoE[,1:2])

## Evaluate at initial points
N <- 100
DoE <- cbind(DoE, t(apply(DoE, 1, calc_rates, h=h, N=N)))
names(DoE)[3:4] <- c("beta", "beta_var")
DoE$N <- N

## Discard any solutions which are not probably powered
DoE$pow_lim <- DoE$beta + qnorm(0.975)*sqrt(DoE$beta_var)
DoE <- DoE[DoE$pow_lim <= 0.1,]

## Discard any remaining solutions that are dominated
is_nondom <- function(x, b)
{
    i <- 1
    while(i <= nobj & nrow(b)!= 0 ){
      ## subset b to those non-dominated solutions which are less than or equal to 
      ## x in an objective
      b <- b[b[, obj_names[i] ] <= x[[ obj_names[i] ]],]
      i <- i + 1
    }
    ## If b is now empty, then des is non-dominated
    if(nrow(b)==1 | all(apply(b[,c("f1","f2")], 2, function(x) length(unique(x)) == 1) == TRUE) ) {
      nondom <- TRUE
    } else {
      nondom <- FALSE
    }
    return(nondom)
}
 
DoE <- cbind(DoE, t(apply(DoE, 1, objective)))
names(DoE)[(ncol(DoE)-nobj+1):ncol(DoE)] <- obj_names
DoE <- DoE[apply(DoE, 1, is_nondom, b=DoE), ]

#saveRDS(DoE, "./data/ex1_sobol_DoE.Rda")
```

### Results

Plot the final results:

```{r}
DHs <- readRDS("./data/ex1_single_run_DHs.Rda")
DoE <- readRDS("./data/ex1_single_run_DoE.Rda")
DoE_sob <- readRDS("./data/ex1_sobol_DoE.Rda")

models <- list()
for(i in 1:nrow(constraints)){
  name <- constraints$name[i]
  models <- append(models, km(~1, design=DoE[1:dim], response=DoE[,as.character(name)], 
                             noise.var=DoE[,paste0(as.character(name), "_var")]))
  names(models)[length(models)] <- as.character(name)
}

## The final approximation set
b <- best(design_space, models, DoE)
## Extend to include extreme points for plotting
b2 <- b[,1:2]
b2 <- rbind(c(min(b2[,1]), 30), b2, c(500, min(b2[,2])))

## Fixed design comparator
b3 <- DoE_sob[,1:2]
b3 <- rbind(c(min(b3[,1]), 30), b3, c(500, min(b3[,2])))

## Get the GP predictions
mod <- models[[1]]
df <- expand.grid(n = seq(100,500,10), k=3:30)
pred <- predict(mod, newdata=df, type="SK")
df$beta <- pred$mean
df$sd <- pred$sd
```

```{r}
b2$beta <- 1; b3$beta <- 1
## Plot evaluated points With the mean function
ggplot(df, aes(2*n, 3*k, z=beta)) + geom_contour(colour="light blue") + 
  geom_point(data=DoE[1:20,], colour=colours[1], shape=1) +
  geom_point(data=DoE[21:60,], colour=colours[2], shape=4) +
  
  geom_point(data=b2[2:(nrow(b2)-1),], colour=colours[3]) +
  geom_step(data=b2, colour=colours[3], linetype=2) +
  
  geom_point(data=b3[2:(nrow(b3)-1),], colour=colours[5]) +
  geom_step(data=b3, colour=colours[5], linetype=2) +
  
  theme_minimal() + xlab("Number of participants") + ylab("Number of providers")

#ggsave("./paper/figures/ex1_single_run.pdf", height=9, width=14, units="cm")
#ggsave("./paper/figures/ex1_single_run.eps", height=9, width=14, units="cm")
#ggsave("./paper/figures/ex1_single_run.png", height=3, width=5)

## Plot dominated hypervolumes over the search
df2 <- data.frame(it = 1:30, DH=DHs)
ggplot(df2, aes(it, DH)) + geom_point() + geom_line() + 
  theme_minimal() + ylab("Dominated hypervolume") + xlab("Iteration")

#ggsave("./paper/figures/ex1_traj.pdf", height=9, width=14, units="cm")
#ggsave("./paper/figures/ex1_traj.eps", height=9, width=14, units="cm")
```

For each solution in the approximation set, we can verify whether or not it really is within the type II error rate nominal bound by re-evaluating it using $N = 50,000$ MC samples. 

```{r, eval=F}
tab <- b[,1:4]
new_eval <- NULL
for(i in 1:nrow(tab)){
  new_eval <- rbind(new_eval, calc_rates(as.numeric(tab[i, 1:2]), h=h, N=50000))
}
tab <- cbind(tab, new_eval)
names(tab)[5:6] <- c("beta_2", "beta_var_2")

#saveRDS(tab, "./data/ex1_large_N.Rda")
```

```{r}
tab <- readRDS("./data/ex1_large_N.Rda")

## Print the results in a table
tab2 <- data.frame(n=tab$n*2, k=tab$k, j=tab$k*2, 
                   beta=apply(tab[,3:4], 1, function(x) paste0(round(x[1], 2), " (", round(sqrt(x[2]), 3), ")")),
                   beta_2=apply(tab[,5:6], 1, function(x) paste0(round(x[1], 3), " (", round(sqrt(x[2]), 3), ")"))
                   )
tab2 <- tab2[order(tab2$k),]
colnames(tab2) <- c("$2n$", "$k$", "$j$", "$\\beta$ (s.e.), $N = 10^2$", "$\\beta$ (s.e.), $N = 50^4$")
tab2

# print(xtable(tab2, digits=0), booktabs = T, include.rownames = F, 
#       sanitize.text.function = function(x) {x}, floating = F,
#       file = "./paper/tables/ex1_results.txt")
```


Now plot the results as an animation:

```{r, eval=F, echo=F}
to_pred <- expand.grid(n = seq(100,500,5), k=3:30)
p_fronts <- NULL
evals <- NULL

for(i in 5:nrow(DoE)){
  sub <- DoE[1:i,]
  mod <- km(~1, design=sub[1:dim], response=sub[,"beta"], noise.var=sub[,"beta_var"])
  b <- best(design_space, models=list(mod), sub)
  
  pred <- predict(mod, newdata=to_pred[,1:2], type="SK")
  to_pred <- cbind(to_pred, pred$mean)
  p_fronts <- rbind(p_fronts, cbind(b[,c("n", "k")], it=rep(i-4, nrow(b))))
  evals<- rbind(evals, cbind(sub[,c("n", "k")], it=rep(i-4, nrow(sub))))
}

names(to_pred)[3:ncol(to_pred)] <- 1:(ncol(to_pred) - 2)
df <- melt(to_pred, c("n", "k"))
names(df)[3:4] <- c("it", "beta")

ggplot(df, aes(n, k)) + geom_raster(aes(fill=beta)) + 
  scale_fill_gradientn(colours=rainbow(3)) + 
  geom_point(data=evals) + 
  geom_point(data=p_fronts, size=3, shape=4) + 
  transition_states(
    it,
    transition_length = 1,
    state_length = 2
  )
```

## Example 2

We extend the problem above to an analysis of two binary endpoints, fatigue and disability.

### Simualtion

```{r}
sim_trial2 <- function(x, h)
{
  n <- x[1]; k <- x[2]
  j <- 2*k
  
  r_t <- h[1]; r_d <- h[2]; v_w <- h[3]; cov_res <- h[4]; cov_w <- h[5]
  p0a <- h[6]; p1a <- h[7]; p0b <- h[8]; p1b <- h[9]
  
  ## Find the terms for the linear predictor which correspond to the probabilities
  lp0a <- log(p0a/(1-p0a)); lp1a <- log(p1a/(1-p1a)) - lp0a
  lp0b <- log(p0b/(1-p0b)); lp1b <- log(p1b/(1-p1b)) - lp0b
  
  ## Doctor effects
  v_d <- r_d*v_w/(1-r_d)
  doc <- cbind(seq(1,j), rmvnorm(j, c(0,0), matrix(c(v_d, v_d*cov_res, v_d*cov_res, v_d), ncol=2, byrow = 2)))
  
  ## Therapist effects
  v_t <- (r_t*v_d + r_t*v_w)/(1-r_t)
  ther <- cbind(seq(0,k), rbind(c(0, 0), rmvnorm(k, c(0,0), matrix(c(v_t, v_t*cov_res, v_t*cov_res, v_t), ncol=2, byrow = 2))))
  
  ## 1   | 2    | 3   | 4    | 5    | 6     | 7
  ## arm | p_id |d_id | d_ef | t_id | t_eff | outcome
  
  ## Treatment group
  trt <- c(rep(0, n), rep(1, n))
  ## Patient ID
  p_id <- seq(1, 2*n)
  ## Doctor allocation and effect
  z <- rgamma(j, 1)
  ms <- round(z*2*n/sum(z))
  ms[which.max(ms)] <- ms[which.max(ms)] + 2*n - sum(ms)
  d_id <- sample(rep(doc[,1], ms))
  d_eff_a <- doc[d_id,2]
  d_eff_b <- doc[d_id,3]
  ## Therapist allocation and effect
  z <- rgamma(k, 1)
  ms <- round(z*n/sum(z))
  ms[which.max(ms)] <- ms[which.max(ms)] + n - sum(ms)
  t_id <- c(rep(0, n), sample(rep(ther[2:(k+1),1], ms)))
  t_eff_a <- ther[t_id+1,2]
  t_eff_b <- ther[t_id+1,3]
  ## Residual - now bi-variate
  resid_n <- rmvnorm(2*n, c(0,0), matrix(c(1, cov_w, cov_w, 1), ncol=2, byrow = 2))
  ps <- pnorm(resid_n)
  resid_log  <- qlogis(ps)

  data_a <- cbind(trt,p_id,d_id,d_eff_a,t_id,t_eff_a,resid_log[,1])
  data_b <- cbind(trt,p_id,d_id,d_eff_b,t_id,t_eff_b,resid_log[,2])
  
  ## Outcomes
  data_a <- cbind(data_a, apply(data_a, 1, function(y, lp1, lp0) lp0 + y[1]*lp1 + y[4] + y[6] + y[7], lp1=lp1a, lp0=lp0a))
  data_a <- cbind(data_a, data_a[,8] > 0)
  data_b <- cbind(data_b, apply(data_b, 1, function(y, lp1, lp0) lp0 + y[1]*lp1 + y[4] + y[6] + y[7], lp1=lp1b, lp0=lp0b))
  data_b <- cbind(data_b, data_b[,8] > 0)
    
  ## Analyse the data
  df_a <- as.data.frame(data_a) 
  names(df_a)[9] <- c("y")
  df_b <- as.data.frame(data_b) 
  names(df_b)[9] <- c("y")
  
  result_a <- tryCatch({
    fit1 <- suppressMessages(glmer(y ~ trt + (0 + trt|t_id) + (1|d_id), family = "binomial", data=df_a))
    fit2 <- suppressMessages(glmer(y ~ (0 + trt|t_id) + (1|d_id), family = "binomial", data=df_a))
    p <- anova(fit1, fit2)[2,8]
  }, warning = function(war) {
    ## warning handler picks up where error was generated
    return(1)
  }, error = function(err) {
    ## error handler picks up where error was generated
    print("error")
    return(1)
  }, finally = {

  })
  
  result_b <- tryCatch({
    fit1 <- suppressMessages(glmer(y ~ trt + (0 + trt|t_id) + (1|d_id), family = "binomial", data=df_b))
    fit2 <- suppressMessages(glmer(y ~ (0 + trt|t_id) + (1|d_id), family = "binomial", data=df_b))
    p <- anova(fit1, fit2)[2,8]
  }, warning = function(war) {
    ## warning handler picks up where error was generated
    return(1)
  }, error = function(err) {
    ## error handler picks up where error was generated
    print("error")
    return(1)
  }, finally = {

  })

  return(c(result_a, result_b))
}
```

```{r}
calc_rates2 <- function(x, h, N)
{
  ps <- replicate(N, sim_trial2(x, h))
  y <- ps[1,] > 0.05 | ps[2,] > 0.05
  res <- c(mean(y), var(y)/N)
  
  res
}

## For example,
x <- c(n=181, k=7)
h <- c(r_t=0.05, r_d=0.1, v_w=3.29, cov_res=0.9, cov_w=0.9, p0a=0.1, p1a=0.25, p0b=0.1, p1b=0.25)

ptm <- proc.time()
calc_rates2(x, h, 10)
proc.time() - ptm
```

### Algorithm

```{r, eval=T}
set.seed(90307)

ptm <- proc.time()

design_space <- data.frame(name=c("n","k"), 
                           low=c(100,3), 
                           up=c(500,30)
)

constraints <- data.frame(name=c("beta"), 
                          hyp=c("H1"), 
                          nom=c(0.1), 
                          delta=c(0.975))

nobj <- 2

obj_names <- NULL
for(i in 1:nobj){
  obj_names <- c(obj_names, paste0("f",i))
}

objective <- function(x)
{
  c(x[1]*2/5, x[2])
}

DoE_num <- 20
dim <- nrow(design_space)
ref <- objective(design_space$up)
  
## Choose initial points
DoE <- data.frame(sobol(DoE_num, dim))
names(DoE) <- design_space$name
for(i in 1:dim){
  DoE[,i] <-  DoE[,i]*(design_space$up[i]-design_space$low[i]) + design_space$low[i]
}
DoE[,1:2] <- round(DoE[,1:2])
```

```{r, eval=F}
## Evaluate at initial points
N <- 100
h <- c(r_t=0.05, r_d=0.1, v_w=3.29, cov_res=0.9, cov_w=0.9, p0a=0.1, p1a=0.25, p0b=0.1, p1b=0.25)
DoE <- cbind(DoE, t(apply(DoE, 1, calc_rates2, h=h, N=N)))
names(DoE)[3:4] <- c("beta", "beta_var")
DoE$N <- N

models <- list()
for(i in 1:nrow(constraints)){
  name <- constraints$name[i]
  models <- append(models, km(~1, design=DoE[1:dim], response=DoE[,as.character(name)], 
                             noise.var=DoE[,paste0(as.character(name), "_var")]))
  names(models)[length(models)] <- as.character(name)
}

## Get the current set of Pareto solutions
b <- best(design_space, models, DoE)

DHs <- NULL

proc.time() - ptm

for(i in 1:30){
  opt <- psoptim(rep(NA, 2), exp_improve, lower=design_space$low, upper=design_space$up,
                 N=100, p_set=b, models=models, design_space=design_space, constraints=constraints,
                 control=list(vectorize = T))
  sol <- opt$par
  sol[1:2] <- round(sol[1:2])
  
  ## track the objective value at each step
  p_set2 <- as.matrix(b[,obj_names])
  current <- dominatedHypervolume(p_set2, ref)
  DHs <- c(DHs, dominatedHypervolume(p_set2, ref))
  
  ## Do the evaluation and add to the design
  y <- calc_rates2(sol, h, N=100)
  
  DoE <- rbind(DoE, c(sol, y, 100))
  
  models <- list()
  for(i in 1:nrow(constraints)){
    name <- constraints$name[i]
    models <- append(models, km(~1, design=DoE[1:dim], response=DoE[,as.character(name)], 
                             noise.var=DoE[,paste0(as.character(name), "_var")]))
    names(models)[length(models)] <- as.character(name)
  }

  b <- best(design_space, models, DoE)
}

proc.time() - ptm
# 96.33883 minutes

## Save the dominated hypervolumes of the approximation sets at each iteration
#saveRDS(DHs, "./data/ex2_single_run_DHs.Rda")

## Save the final DoE table containing all the points evaluated over the whole algorithm
#saveRDS(DoE, "./data/ex2_single_run_DoE.Rda")
```

### Results

```{r}
DoE <- readRDS("./data/ex2_single_run_DoE.Rda")
DHs <- readRDS("./data/ex2_single_run_DHs.Rda")

models <- list()
for(i in 1:nrow(constraints)){
  name <- constraints$name[i]
  models <- append(models, km(~1, design=DoE[1:dim], response=DoE[,as.character(name)], 
                             noise.var=DoE[,paste0(as.character(name), "_var")]))
  names(models)[length(models)] <- as.character(name)
}

b <- best(design_space, models, DoE)
b2 <- b[,1:2]
b2 <- rbind(c(min(b2[,1]), 30), b2, c(500, min(b2[,2])))

# Get the GP predictions
mod <- models[[1]]
df <- expand.grid(n = seq(100,500,10), k=3:30)
pred <- predict(mod, newdata=df, type="SK")
df$beta <- pred$mean
df$sd <- pred$sd
```

```{r}
ggplot(df, aes(2*n, 3*k)) + geom_contour(aes(z=beta), colour = "light blue") + 
  geom_point(data=DoE[1:20,], colour=colours[1], shape=1) +
  geom_point(data=DoE[21:50,], colour=colours[2], shape=4) +
  
  geom_point(data=b2[2:(nrow(b2)-1),], colour=colours[3]) +
  geom_step(data=b2, colour=colours[3], linetype=2) +

  theme_minimal() + xlab("Number of participants") + ylab("Number of providers")

#ggsave("./paper/figures/ex2_single_run.pdf", height=9, width=14, units="cm")
#ggsave("./paper/figures/ex2_single_run.eps", height=9, width=14, units="cm")

## Plot dominated hypervolumes over the search
df2 <- data.frame(it = 1:30, DH=DHs)
ggplot(df2, aes(it, DH)) + geom_point() + geom_line() + 
  theme_minimal() + ylab("Dominated hypervolume") + xlab("Iteration")

#ggsave("./paper/figures/ex2_traj.pdf", height=9, width=14, units="cm")
#ggsave("./paper/figures/ex2_traj.eps", height=9, width=14, units="cm")
```

```{r, eval=F}
tab <- b[,1:4]
new_eval <- NULL
for(i in 1:nrow(tab)){
  new_eval <- rbind(new_eval, calc_rates2(as.numeric(tab[i, 1:2]), h=h, N=10000))
}
tab <- cbind(tab, new_eval)
names(tab)[5:6] <- c("beta_2", "beta_var_2")

#saveRDS(tab, "./data/ex2_large_N.Rda")
```

```{r}
## Print the results in a table
tab <- readRDS("./data/ex2_large_N.Rda")

tab2 <- data.frame(n=tab$n*2, k=tab$k, j=tab$k*2, 
                   beta=apply(tab[,3:4], 1, function(x) paste0(round(x[1], 2), " (", round(sqrt(x[2]), 3), ")")),
                   beta_2=apply(tab[,5:6], 1, function(x) paste0(round(x[1], 3), " (", round(sqrt(x[2]), 3), ")"))
                   )
tab2 <- tab2[order(tab2$k),]
colnames(tab2) <- c("$2n$", "$k$", "$j$", "$\\beta$ (s.e.), $N = 10^2$", "$\\beta$ (s.e.), $N = 50^4$")
tab2

# print(xtable(tab2, digits=0), booktabs = T, include.rownames = F, 
#       sanitize.text.function = function(x) {x}, floating = F,
#       file = "./paper/tables/ex2_results.txt")
```

## Example 3

### Simualtion

```{r}
sim_trial3 <- function(x, h)
{
  n_1 <- x[1]; k <- x[2]; r <- x[3]; j <- x[4]
  n_0 <- round(r*n_1); n_t <- n_0 + n_1
  
  r_t <- h[1]; r_d <- h[2]; v_w <- h[3]; cov_res <- h[4]; cov_w <- h[5]
  p0a <- h[6]; p1a <- h[7]; p0b <- h[8]; p1b <- h[9]
  
  # Find the terms for the linear predictor which correspond to the probabilities
  lp0a <- log(p0a/(1-p0a)); lp1a <- log(p1a/(1-p1a)) - lp0a
  lp0b <- log(p0b/(1-p0b)); lp1b <- log(p1b/(1-p1b)) - lp0b
  
  # Doctor effects
  v_d <- r_d*v_w/(1-r_d)
  doc <- cbind(seq(1,j), rmvnorm(j, c(0,0), matrix(c(v_d, v_d*cov_res, v_d*cov_res, v_d), ncol=2, byrow = 2)))
  
  # Therapist effects
  v_t <- (r_t*v_d + r_t*v_w)/(1-r_t)
  ther <- cbind(seq(0,k), rbind(c(0, 0), rmvnorm(k, c(0,0), matrix(c(v_t, v_t*cov_res, v_t*cov_res, v_t), ncol=2, byrow = 2))))
  
  # 1   | 2    | 3   | 4    | 5    | 6     | 7
  # arm | p_id |d_id | d_ef | t_id | t_eff | outcome
  
  # Treatment group
  trt <- c(rep(0, n_0), rep(1, n_1))
  # Patient ID
  p_id <- seq(1, n_t)
  # Doctor allocation and effect
  d_id <- sample(rep(1:j, ceiling(n_t/j)*j)[1:n_t])
  d_eff_a <- doc[d_id,2]
  d_eff_b <- doc[d_id,3]
  # Therapist allocation and effect
  t_id <- c(rep(0, n_0), rep(1:k, ceiling(n_1/k)*k)[1:n_1])
  t_eff_a <- ther[t_id+1,2]
  t_eff_b <- ther[t_id+1,3]
  # Residual - now bi-variate
  resid_n <- rmvnorm(n_t, c(0,0), matrix(c(v_w, cov_w*v_w, cov_w*v_w, v_w), ncol=2, byrow = 2))
  
  data_a <- cbind(trt,p_id,d_id,d_eff_a,t_id,t_eff_a,resid_n[,1])
  data_b <- cbind(trt,p_id,d_id,d_eff_b,t_id,t_eff_b,resid_n[,2])
  
  var(data_a[,7])
  
  # Outcomes
  data_a <- cbind(data_a, apply(data_a, 1, function(y, lp1, lp0) lp0 + y[1]*lp1 + y[4] + y[6] + y[7], lp1=lp1a, lp0=lp0a))
  data_b <- cbind(data_b, apply(data_b, 1, function(y, lp1, lp0) lp0 + y[1]*lp1 + y[4] + y[6] + y[7], lp1=lp1b, lp0=lp0b))
    
  # Analyse the data
  df_a <- as.data.frame(data_a) 
  names(df_a)[8] <- c("y")
  df_b <- as.data.frame(data_b) 
  names(df_b)[8] <- c("y")
  
  result_a <- tryCatch({
    fit1 <- suppressMessages(lmer(y ~ trt + (0 + trt|t_id) + (1|d_id), REML=F, data=df_a))
    fit2 <- suppressMessages(lmer(y ~ (0 + trt|t_id) + (1|d_id), REML=F, data=df_a))
    p <- anova(fit1, fit2)[2,8]
  }, warning = function(war) {
    # warning handler picks up where error was generated
    #print("warning")
    return(1)
  }, error = function(err) {
    # error handler picks up where error was generated
    print("error")
    return(1)
  }, finally = {

  })
  
  result_b <- tryCatch({
    fit1 <- suppressMessages(lmer(y ~ trt + (0 + trt|t_id) + (1|d_id), REML=F, data=df_b))
    fit2 <- suppressMessages(lmer(y ~ (0 + trt|t_id) + (1|d_id), REML=F, data=df_b))
    p <- anova(fit1, fit2)[2,8]
  }, warning = function(war) {
    # warning handler picks up where error was generated
    #print("warning")
    return(1)
  }, error = function(err) {
    # error handler picks up where error was generated
    print("error")
    return(1)
  }, finally = {

  })

  return(c(result_a, result_b))
}
```

```{r}
calc_rates3 <- function(x, h, N)
{
  alpha <- x[5]
  
  # Alternative hypothesis - one of the endpints is non-null
  ha <- h; ha["p1b"] <- ha["p0b"]
  psa <- replicate(N, sim_trial3(x, ha))
  ya <- psa[1,] > alpha & psa[2,] > alpha
  res_a <- c(mean(ya), var(ya)/N)
  
  # Null hypothesis - both of the endpoints are null
  hn <- h; hn["p1a"] <- hn["p0a"]; hn["p1b"] <- hn["p0b"]
  psn <- replicate(N, sim_trial3(x, hn))
  yn <- psn[1,] < alpha | psn[2,] < alpha
  res_n <- c(mean(yn), var(yn)/N)
  
  c(res_a, res_n)#, mean(psa==10))
}

# For example,
x <- c(n_1=50, k=7, r=1, j=5, a=0.2)
h <- c(r_t=0.05, r_d=0.1, v_w=3.29, cov_res=0.9, cov_w=0.9, p0a=0.1, p1a=0.25, p0b=0.1, p1b=0.25)

ptm <- proc.time()
calc_rates3(x, h, 10)
proc.time() - ptm
```

### Algorithm

```{r}
set.seed(90309)

ptm <- proc.time()

constraints <- data.frame(name=c("beta", "alpha"), 
                          hyp=c("H1", "H0"), 
                          nom=c(0.1, 0.2), 
                          delta=c(0.975,0.975))

design_space <- data.frame(name=c("n_1","k", "r","j","a"), 
                           low=c(20,2,0.5,3, 0.2), 
                           up=c(100,10,1.5,25, 0.01)
                           )

h <- c(r_t=0.05, r_d=0.1, v_w=3.29, cov_res=0.9, cov_w=0.9, p0a=0.1, p1a=0.25, p0b=0.1, p1b=0.25)

dim <- nrow(design_space)
nobj <- 3
obj_names <- NULL
for(i in 1:nobj){
  obj_names <- c(obj_names, paste0("f",i))
}
  
objective <- function(x)
{
  c((x[1]+x[1]*x[3])/5, x[2], x[4])
}

DoE_num <- 50
dim <- nrow(design_space)
  
# Choose initial points
DoE <- data.frame(sobol(DoE_num, dim))
names(DoE) <- design_space$name
for(i in 1:dim){
  DoE[,i] <-  DoE[,i]*(design_space$up[i]-design_space$low[i]) + design_space$low[i]
}
DoE[,c("n_1", "k", "j")] <- round(DoE[,c("n_1", "k", "j")])
```

```{r, eval=F}
# Evaluate at initial points
N <- 100
DoE <- cbind(DoE, t(apply(DoE, 1, calc_rates3, h=h, N=N)))
names(DoE)[6:9] <- c("beta", "beta_var", "alpha", "alpha_var")
DoE$N <- N

proc.time() - ptm

models <- list()
for(i in 1:nrow(constraints)){
  name <- constraints$name[i]
  models <- append(models, km(~1, design=DoE[1:dim], response=DoE[,as.character(name)], 
                             noise.var=DoE[,paste0(as.character(name), "_var")]))
  names(models)[length(models)] <- as.character(name)
}

# Get the current set of Pareto solutions
b <- best(design_space, models, DoE)

ref <- objective(design_space$up)
DHs <- NULL

for(i in 1:150){
  opt <- psoptim(rep(NA, 5), exp_improve, lower=design_space$low, upper=design_space$up,
                 N=100, p_set=b, models=models, design_space=design_space, constraints=constraints,
                 control=list(vectorize = T))
  sol <- opt$par
  sol[c(1,2,4)] <- round(sol[c(1,2,4)])
  
  # Do the evaluation and add to the design
  y <- calc_rates3(sol, h, N=100)
  
  DoE <- rbind(DoE, c(sol, y, 100))
  
  # track the objective value at each step
  p_set2 <- as.matrix(b[,obj_names])
  current <- dominatedHypervolume(p_set2, ref)
  DHs <- c(DHs, dominatedHypervolume(p_set2, ref))
  
  models <- list()
  for(i in 1:nrow(constraints)){
    name <- constraints$name[i]
    models <- append(models, km(~1, design=DoE[1:dim], response=DoE[,as.character(name)], 
                               noise.var=DoE[,paste0(as.character(name), "_var")]))
    names(models)[length(models)] <- as.character(name)
  }

  b <- best(design_space, models, DoE)
}

proc.time() - ptm
# 156.2025

#saveRDS(DoE, "./data/ex3_single_run_DoE.Rda")
#saveRDS(DHs, "./data/ex3_single_run_DHs.Rda")
```


### Results

```{r}
DoE <- readRDS("./data/ex3_single_run_DoE.Rda")
DHs <- readRDS("./data/ex3_single_run_DHs.Rda")

models <- list()
for(i in 1:nrow(constraints)){
  name <- constraints$name[i]
  models <- append(models, km(~1, design=DoE[1:dim], response=DoE[,as.character(name)], 
                             noise.var=DoE[,paste0(as.character(name), "_var")]))
  names(models)[length(models)] <- as.character(name)
}

# Get the current set of Pareto solutions
b <- best(design_space, models, DoE)

ggplot(b, aes(n_1+n_1*r, k, size=j)) + geom_point() +
  theme_minimal() + xlab("Number of participants") + ylab("Number of therapists") +
  #scale_colour_gradientn(name="No. of doctors", colours=rainbow(2)) +
  scale_size(name="No. of doctors") +
  xlim(c(min(b$n_1 + b$n_1*b$r), max(b$n_1 + b$n_1*b$r))) +  ylim(c(min(b$k), max(b$k)))

#ggsave("./paper/figures/ex3_single_run.pdf", height=9, width=14, units="cm")
#ggsave("./paper/figures/ex3_single_run.eps", height=9, width=14, units="cm")

## Plot dominated hypervolumes over the search
df2 <- data.frame(it = 1:150, DH=DHs)
ggplot(df2, aes(it, DH)) + geom_point() + geom_line() + 
  theme_minimal() + ylab("Dominated hypervolume") + xlab("Iteration")

#ggsave("./paper/figures/ex3_traj.pdf", height=9, width=14, units="cm")
#ggsave("./paper/figures/ex3_traj.eps", height=9, width=14, units="cm")
```

```{r, eval=F}
tab <- b[,1:9]
new_eval <- NULL
for(i in 1:nrow(tab)){
  new_eval <- rbind(new_eval, calc_rates3(as.numeric(tab[i, 1:5]), h=h, N=10000))
}
tab <- cbind(tab, new_eval)
names(tab)[10:13] <- c("beta_2", "beta_var_2", "alpha_2", "alpha_var_2")

#saveRDS(tab, "./data/ex3_large_N.Rda")
```

```{r}
## Print the results in a table
tab <- readRDS("./data/ex3_large_N.Rda")

tab2 <- data.frame(n_1=tab$n_1, n=tab$n_1*(1+tab$r), k=tab$k, j=tab$j, a=tab$a,
                   beta=apply(tab[,6:7], 1, function(x) paste0(round(x[1], 2), " (", round(sqrt(x[2]), 3), ")")),
                   alpha=apply(tab[,8:9], 1, function(x) paste0(round(x[1], 2), " (", round(sqrt(x[2]), 3), ")")),
                   beta_2=apply(tab[,10:11], 1, function(x) paste0(round(x[1], 3), " (", round(sqrt(x[2]), 3), ")")),
                   
                   alpha_2=apply(tab[,12:13], 1, function(x) paste0(round(x[1], 3), " (", round(sqrt(x[2]), 3), ")"))
                   )
tab2 <- tab2[order(tab2$k),]
colnames(tab2) <- c("$n_1$", "$n$", "$k$", "$j$", "$a$",
                    "$\\beta$ (s.e.)", "$\\alpha$ (s.e.)", 
                    "$\\beta$ (s.e.)", "$\\alpha$ (s.e.)")
tab2

#  & & & & & \multicolumn{2}{c}{$N = 10^2$} & \multicolumn{2}{c}{$N = 50^4$} \\
# print(xtable(tab2, digits=c(rep(0,5), 2, rep(0,4))),
#       booktabs = T, include.rownames = F, 
#       sanitize.text.function = function(x) {x}, floating = F,
#       file = "./paper/tables/ex3_results.txt")
```

## Example 4

As suggested by a reviewer, we here add a simple example where the exact result is known. We will look at the power of a two-sample t-test. Although we can calculate power exactly in this case, we replicate the simulation setting by adding some Monet Carlo error to the true power.

### Simualtion

```{r}
calc_rates4 <- function(x, h, N)
{
  n <- x[[1]]; k <- x[[2]]; m <- n/k
  var_t <- h[[2]]; rho <- h[[3]]
  sig_c <- sqrt(var_t*rho + var_t/m - var_t*rho/m)
  pow <- power.t.test(n = k, delta = h[[1]], sd = sig_c)$power
  
  res <- c(1 - rbinom(1, N, pow)/N, pow*(1-pow)/N)
  
  res
}

## For example,
x <- c(n=342, k = 18)
h <- c(mu = 0.3, var_t = 1, rho = 0.05)

ptm <- proc.time()
calc_rates4(x, h, 1000)
proc.time() - ptm
```

### Algorithm

The proposed method is stochastic, leading to a different approximation set each time it is run. To understand the variation in the quality of these sets, we run the algorithm 500 times and for each run record the dominated hypervolume trajectories. We will also record the number of solutions in each final set, and the proportion of these which do indeed have sufficient power. In the first run, we will save the full details of all evaluations for illustration later.

```{r, eval=T}
set.seed(90307)

ptm <- proc.time()

design_space <- data.frame(name=c("n","k"), 
                           low=c(100,10), 
                           up=c(500,100)
)

constraints <- data.frame(name=c("beta"), 
                          hyp=c("H1"), 
                          nom=c(0.1), 
                          delta=c(0.975))

nobj <- 2

obj_names <- NULL
for(i in 1:nobj){
  obj_names <- c(obj_names, paste0("f",i))
}

objective <- function(x)
{
  c(x[1]*2/5, x[2])
}

DoE_num <- 20
dim <- nrow(design_space)
ref <- objective(design_space$up)
```

```{r, eval=F}
M <- 500
all_res <- NULL
ptm <- proc.time()
DoE_num <- 20
  
for(iter in 1:M){

  ## Choose initial points
  DoE <- data.frame(sobol(DoE_num, dim))
  names(DoE) <- design_space$name
  for(i in 1:dim){
    DoE[,i] <-  DoE[,i]*(design_space$up[i]-design_space$low[i]) + design_space$low[i]
  }
  DoE[,1:2] <- round(DoE[,1:2])
  
  ## Evaluate at initial points
  N <- 100
  h <- c(mu = 0.3, var_t = 1, rho = 0.05)
  DoE <- cbind(DoE, t(apply(DoE, 1, calc_rates4, h=h, N=N)))
  names(DoE)[3:4] <- c("beta", "beta_var")
  DoE$N <- N
  
  models <- list()
  for(i in 1:nrow(constraints)){
    name <- constraints$name[i]
    models <- append(models, km(~1, design=DoE[1:dim], response=DoE[,as.character(name)], 
                               noise.var=DoE[,paste0(as.character(name), "_var")]))
    names(models)[length(models)] <- as.character(name)
  }
  
  ## Get the current set of Pareto solutions
  b <- best(design_space, models, DoE)
  
  DHs <- NULL
  
  proc.time() - ptm
  
  sink("NUL") # suppress output to the console
  
  # For the first run, do 200 iterations and save as an example
  iter <- 50 - DoE_num
  if(M == 1) iter <- 200 - DoE_num
  
  for(i in 1:iter){
    opt <- psoptim(rep(NA, nrow(design_space)), exp_improve, lower=design_space$low, upper=design_space$up,
                   N=100, p_set=b, models=models, design_space=design_space, constraints=constraints,
                   control=list(vectorize = T,max.restart=1, reltol=0.01))
    sol <- opt$par
    sol <- round(sol)
    
    ## track the objective value at each step
    p_set2 <- as.matrix(b[,obj_names])
    current <- dominatedHypervolume(p_set2, ref)
    DHs <- c(DHs, dominatedHypervolume(p_set2, ref))
    
    ## Do the evaluation and add to the design
    y <- calc_rates4(sol, h, N=100)
    
    DoE <- rbind(DoE, c(sol, y, 100))
    
    models <- list()
    for(i in 1:nrow(constraints)){
      name <- constraints$name[i]
      models <- append(models, km(~1, design=DoE[1:dim], response=DoE[,as.character(name)], 
                               noise.var=DoE[,paste0(as.character(name), "_var")]))
      names(models)[length(models)] <- as.character(name)
    }
  
    b <- best(design_space, models, DoE)
    
    if(M == 1) saveRDS(DoE, "./data/ex4_DoE.Rda")
  }
  
  sink(NULL)
  
  res <- c(50, 
           dominatedHypervolume(t(apply(b[,1:2], 1, objective)), ref),
           nrow(b),
           mean(apply(b, 1, function(x) calc_rates4_opt(x[1], x[2], h)) < 100000000))
  
  all_res <- rbind(all_res, res)
}
proc.time() - ptm

df_e <- as.data.frame(all_res)
names(df_e) <- c("i", "DH", "s", "p")

sink(NULL)
#saveRDS(df_e, "./data/ex4_many_runs_10DoE.Rda")
#saveRDS(df_e, "./data/ex4_many_runs_20DoE.Rda")
#saveRDS(df_e, "./data/ex4_many_runs_30DoE.Rda")
```


### Comparison

For comparison, we will compare the method against a fixed design approach. Specifically, we generate a fixed space filling design of 50 solutions 500 times. For each set we estimate power as above (using an artificial MC error) and then find approximation sets based on the first 30, 31, \ldots , 50 solutions. We then record trajectories, size of sets, and proportions of powered solutions as above.

First, the exact Pareto set:
```{r}
get_pow4 <- function(n, k, h)
{
  # Get the true power
  m <- n/k
  var_t <- h[[2]]; rho <- h[[3]]
  sig_c <- sqrt(var_t*rho + var_t/m - var_t*rho/m)
  power.t.test(n = k, delta = h[[1]], sd = sig_c)$power
}

calc_rates4_opt <- function(n, k, h)
{
  # An objective function to find optimal n for a given k
  m <- n/k
  var_t <- h[[2]]; rho <- h[[3]]
  sig_c <- sqrt(var_t*rho + var_t/m - var_t*rho/m)
  pow <- power.t.test(n = k, delta = h[[1]], sd = sig_c)$power

  n + 100000000*(pow < 0.9)
}

# For a set of ks, find optimal n
ks <- 10:100
ns <- sapply(ks, function(x) optim(200, calc_rates4_opt, method = "Brent", lower = 10, upper = 1000,
                                   k = x, h = h)$par)
ps <- data.frame(n = ns, k = ks)
ps <- ps[ps$n > 11 & ps$n <= 500,]
```

Now, we get approximation sets using the fixed design approach. 

```{r}
set.seed(90307)

# The fixed design approach
ref <- objective(design_space$up)

M <- 500
all_res <- NULL
  
for(iter in 1:M){
  ## Choose initial points
  DoE <- data.frame(sobol(1000, dim))
  names(DoE) <- design_space$name
  for(i in 1:dim){
    DoE[,i] <-  DoE[,i]*(design_space$up[i]-design_space$low[i]) + design_space$low[i]
  }
  DoE[,1:2] <- round(DoE[,1:2])
  
  N <- 100
  h <- c(mu = 0.3, var_t = 1, rho = 0.05)
  DoE <- cbind(DoE, t(apply(DoE, 1, calc_rates4, h=h, N=N)))
  names(DoE)[3:4] <- c("beta", "beta_var")
  DoE$N <- N
  DoE$ci <- DoE$beta + qnorm(0.975)*sqrt(DoE$beta_var)
  
  res <- NULL
  for(j in seq(50,1000,50)){
    sub <- DoE[1:j,]
    fb <- paretoFilter(as.matrix(sub[sub$ci < 0.1, 1:2]))
    # save first run as an example
    if(M == 1 & j == 50){
      saveRDS(fb, "./data/ex4_fb.Rda")
      saveRDS(sub, "./data/ex4_DoE_fb.Rda")
    }
    powered <- mean(apply(fb, 1, function(x) calc_rates4_opt(x[1], x[2], h)) < 100000000)
    dh <- dominatedHypervolume(t(apply(fb, 1, objective)), ref)
    res <- rbind(res, c(j, dh, nrow(fb), powered))
  }
  res <- cbind(res, iter)
  all_res <- rbind(all_res, res)
}

df_f <- as.data.frame(all_res)
# i = internal iteartion count
# DH = dominated hypervolume
# s = size of approximation set
# p = proportion of approximation set which is actually powered
names(df_f) <- c("i", "DH", "s", "p", "r")

#saveRDS(df_f, "./data/ex4_fixed.Rda")
```


```{r}
df_f <- readRDS("./data/ex4_fixed.Rda")
df_f <- unique(df_f[df_f$i %in% c(50, 200, 400, 600, 800, 1000), -5])

# Compare with EGO method

df <- rbind(readRDS("./data/ex4_many_runs1.Rda"),
            readRDS("./data/ex4_many_runs2.Rda"),
            readRDS("./data/ex4_many_runs3.Rda"))

df$t <- "EGO"; df_f$t <- "fix"

df$i <- 10001;
df2 <- rbind(df, df_f)
df2$i <- factor(df2$i, labels = c("50", "200", "400", "600", "800", "1000", "EGO"))

# Get Dh of the actual Pareto set
dh_ps <- dominatedHypervolume(matrix(c(objective(ps)$n, objective(ps)$k), ncol=2), ref)

ggplot(df2, aes(i, DH, fill=t)) + geom_boxplot() +
  theme_minimal() +
  ylab("Dominated area") + xlab("Number of evaluations") +
  #theme(axis.text.y=element_blank(),
   #   axis.title.y=element_blank()) +
  scale_fill_manual(name="Method", values=colours, labels = c("EGO", "Fixed")) +
  geom_hline(yintercept = dh_ps, linetype = 2)

ggplot(df2, aes(i, s, fill=t)) + geom_boxplot() +
  theme_minimal() +
  ylab("Size of appoximation set") +  xlab("Number of evaluations") +
  #theme(axis.text.y=element_blank(),
   #   axis.title.y=element_blank()) +
  scale_fill_manual(name="Method", values=colours, labels = c("EGO", "Fixed"))

df3 <- data.frame(i = unique(df2$i), t=c("EGO", rep("Fixed", 6)))
df3$pr <- sapply(df3$i, function(x) mean(df2[df2$i == x,"p"] < 1))
df3

#ggsave("./paper/figures/ex4_many_runs.pdf", height=9, width=14, units="cm")
#ggsave("./paper/figures/ex4_many_runs.eps", height=9, width=14, units="cm", device = cairo_ps())
#ggsave("./paper/figures/ex4_many_runs.png", height=3, width=5)
```

Plot example approximation sets from both methods for one of the runs:

```{r}
# Example EGO run with 50 evals
DoE <- readRDS("./data/ex4_DoE.Rda")[1:50,]
# Example EGO run with 200 evals
DoE2 <- readRDS("./data/ex4_DoE.Rda")
# Example fixed design run approximation set
fb <- readRDS("./data/ex4_fb.Rda")

models <- list()
  for(i in 1:nrow(constraints)){
    name <- constraints$name[i]
    models <- append(models, km(~1, design=DoE[1:dim], response=DoE[,as.character(name)], 
                               noise.var=DoE[,paste0(as.character(name), "_var")]))
    names(models)[length(models)] <- as.character(name)
  }

## The final approximation set after 50 - 20 iterations
b <- best(design_space, models, DoE)
## Extend to include extreme points for plotting
b2 <- b[,1:2]
b2 <- rbind(c(min(b2[,1]), 100), b2, c(500, min(b2[,2])))

## Fixed design comparator
b3 <- as.data.frame(fb)
b3 <- rbind(c(min(b3[,1]), 100), b3, c(500, min(b3[,2])))

## Actual Pareto set
b4 <- as.data.frame(ps)
b4 <- rbind(c(min(b4[,1]), 100), b4, c(500, min(b4[,2])))

## Get the GP predictions
mod <- models[[1]]
df <- expand.grid(n = seq(100,500,10), k=10:100)
pred <- predict(mod, newdata=df, type="SK")
df$beta <- pred$mean
df$sd <- pred$sd

models <- list()
  for(i in 1:nrow(constraints)){
    name <- constraints$name[i]
    models <- append(models, km(~1, design=DoE2[1:dim], response=DoE2[,as.character(name)], 
                               noise.var=DoE2[,paste0(as.character(name), "_var")]))
    names(models)[length(models)] <- as.character(name)
  }

## The final approximation set after 200 - 20 iterations
b5 <- best(design_space, models, DoE2)
## Extend to include extreme points for plotting
b5 <- b5[,1:2]
b5 <- rbind(c(min(b5[,1]), 100), b5, c(500, min(b5[,2])))

b2$beta <- 1; b3$beta <- 1; b4$beta <- 1; b5$beta <- 1
## Plot evaluated points With the mean function
ggplot(df, aes(2*n, 2*k, z=beta)) + geom_contour(colour="lightblue") + 
  # Initial EGO points
  geom_point(data=DoE[1:20,], shape=1) +
  # Subsequent EGO evals
  geom_point(data=DoE[21:nrow(DoE),], shape=4) +
  # EGO approximation set (50 evals)
  geom_point(data=b2[2:(nrow(b2)-1),], aes(colour="EGO50")) +
  geom_step(data=b2, aes(colour="EGO50"), linetype=2) +
  # Fixed design approximation set
  geom_point(data=b3[2:(nrow(b3)-1),], aes(colour="FD")) +
  geom_step(data=b3, aes(colour="FD"), linetype=2) +
  # EGO approximation set (200 evals)
  geom_point(data=b5[2:(nrow(b5)-1),], aes(colour="EGO200")) +
  geom_step(data=b5, aes(colour="EGO200"), linetype=2) +
  # True Pareto set
  geom_step(data=b4, aes(colour="PS"), linetype=2) +

  scale_color_manual(name = "",
                        breaks = c("EGO50", "FD", "EGO200", "PS"),
                        values = c("EGO50" = colours[2],
                                   "FD" = colours[3],
                                   "EGO200" = colours[1],
                                   "PS" = colours[4]),
                     labels = c("EGO (50)", "Fixed design", "EGO (200)", "Pareto set")) +
  
  theme_minimal() + xlab("Number of participants") + ylab("Number of clusters") +
  xlim(c(500, 1000)) + ylim(c(45,200))

#ggsave("./paper/figures/ex4_single_run.pdf", height=9, width=17, units="cm")
#ggsave("./paper/figures/ex4_single_run.eps", height=9, width=17, units="cm")
#ggsave("./paper/figures/ex4_single_run.png", height=3, width=5)

# In this example the dominated areas are:
dominatedHypervolume(matrix(c(objective(b2)$n, objective(b2)$k), ncol=2), ref)
dominatedHypervolume(matrix(c(objective(b5)$n, objective(b5)$k), ncol=2), ref)
dominatedHypervolume(matrix(c(objective(b3)$n, objective(b3)$k), ncol=2), ref)
dominatedHypervolume(matrix(c(objective(ps)$n, objective(ps)$k), ncol=2), ref)
```

Print the individual EGO (50 evaluations) solutions for a table:

```{r}
DoE <- readRDS("./data/ex4_DoE.Rda")[1:50,]

models <- list()
  for(i in 1:nrow(constraints)){
    name <- constraints$name[i]
    models <- append(models, km(~1, design=DoE[1:dim], response=DoE[,as.character(name)], 
                               noise.var=DoE[,paste0(as.character(name), "_var")]))
    names(models)[length(models)] <- as.character(name)
  }

## The final approximation set after 50 - 20 iterations
b <- best(design_space, models, DoE)
tab <- DoE[row.names(b),]

## Print the results in a table
tab2 <- data.frame(n=tab$n*2, k=2*tab$k,
                   beta_est=apply(tab[,3:4], 1, function(x) paste0(round(x[1], 2), " (", round(sqrt(x[2]), 3), ")")))

tab2$beta <- round(1-apply(tab2, 1, function(x) get_pow4(as.numeric(x[1])/2, as.numeric(x[2])/2, h)), 3)

tab2 <- tab2[order(tab2$k),]
colnames(tab2) <- c("$2n$", "$2k$", "$\\hat{\\beta}$ (s.e.)", "$\\beta$")
tab2
```

And similarly, for the fixed design approximation set:
```{r}
DoE <- readRDS("./data/ex4_DoE_fb.Rda")

tab <- DoE[row.names(fb),]

## Print the results in a table
tab2 <- data.frame(n=tab$n*2, k=2*tab$k,
                   beta_est=apply(tab[,3:4], 1, function(x) paste0(round(x[1], 2), " (", round(sqrt(x[2]), 3), ")")))

tab2$beta <- round(1-apply(tab2, 1, function(x) get_pow4(as.numeric(x[1])/2, as.numeric(x[2])/2, h)), 3)

tab2 <- tab2[order(tab2$k),]
colnames(tab2) <- c("$2n$", "$2k$", "$\\hat{\\beta}$ (s.e.)", "$\\beta$")
tab2
```

## Figures

```{r, eval=T}
# Plot an example optimisation problem, minimising clusters and patients, with
# a true Pareto front and an approximation front, with the dominated hypervolume
# shaded in.

pf <- data.frame(f1 = c(472, 472, 473, 474, 476, 478, 481, 483, 486, 492, 498,
           508, 518, 528, 536, 548, 560, 573, 595, 624, 658, 706, 782, 1150, 1200),
           f2 = c(30:7, 7))
pf$t <- "p"

as <- data.frame(f2=c(30, 24,20,12,10, 10),
                 f1=c(589, 589, 705, 810, 982, 1200))
as$t <- "a"

df <- rbind(pf, as)

pg <- data.frame(f1 = c(1200, 982,982, 810, 810, 705, 705, 589, 589, 1200),
                 f2 = c(10, 10, 12, 12, 20, 20, 24, 24, 30, 30))

ggplot(df, aes(f1, f2)) + geom_polygon(data=pg, alpha=0.1) + #fill="grey90") + 
  geom_step(aes(colour=t)) + 
  geom_point(data=df[!(df$f1==1200 | df$f2==30),], aes(colour=t)) +
  theme_minimal() + xlab("Number of participants") + ylab("Number of clusters") +
  scale_colour_manual(name= "",
                      values=c(colours[3], colours[4]), 
                      labels=c("Approximation set", "Pareto set")) #+
  #geom_point(data=data.frame(f1=1200, f2=30), shape=4, size=3, stroke=2) +
  #annotate("text", x = 900, y = 23, label = "H(A)")
  
# ggsave("./paper/figures/fake_pareto.pdf", width= 5, height = 3)
# ggsave("./paper/figures/fake_pareto.eps", width= 5, height = 3, device=cairo_ps)
# ggsave("./paper/figures/fake_pareto.png", width= 4, height = 2)
```

Plot of a GP constraint, annotating some expected improvemets at a couple of points
```{r}
get_power <- function(n)
{
  return(power.t.test(n=n, delta=0.3)$power)
}

df <- data.frame(n = c(10, 110, 260, 290))
df$f<- apply(df, 1, get_power)


model <- km(design = df[,1, drop=FALSE], response = df[,2])

x <- data.frame(n=seq(100, 300))

p <- predict.km(model, newdata=x, type="SK")

x$f <- p$mean
x$sd <- p$sd
x$min <- x$f - 1.96*x$sd
x$max <- x$f + 1.96*x$sd

get_p <- function(input)
{
  return(1-pnorm(0.8, input[2], input[3]))
}

probs <- cbind(x, V1=apply(x, 1, get_p))
probs$EFI <- (260-probs$n)*probs$V1/450 + 0.5
probs$EFI <- ifelse(probs$EFI < 0.5 , 0.5, probs$EFI)

# Optimal expected feasible improvement is at 190, with
# m = 0.8388252, sd = 3.464424e-02

get_n <- function(y, m, s, n)
{
  n + 1.2*dnorm(y, m, s)
}

m <- 0.8388252
s <- 3.464424e-02

x6 <- data.frame(f=c(seq(0,m,0.001), m))
x6$n <- apply(x6, 1, get_n, m=m, s=s, n=190)
x6 <- subset(x6, f>0.5)

x7 <- data.frame(f=seq(m,1,0.001))
x7$n <- apply(x7, 1, get_n, m=m, s=s, n=190)

# For paper
ggplot(x6, aes(n, f)) + geom_line(colour="#009E73", linetype=2) + geom_line(data=x7, colour="#009E73", linetype=2) + 
  geom_line(data=x, colour="#D55E00") + geom_ribbon(data=x, aes(ymin=min, ymax=max), alpha=0.1) + 
  #geom_line(data=x4, colour="red") + geom_line(data=x5, colour="red") +
  geom_point(data=subset(df, n>100)) +
  scale_y_continuous(limits = c(0.5, 1)) +
  geom_line(data=probs, aes(n, EFI), colour="#CC79A7", linetype=3) +
  theme_minimal() + ylab("Power") + xlab("Sample size")

#ggsave("./paper/figures/GP_example.pdf", width= 5, height = 3.3)
#ggsave("./paper/figures/GP_example.eps", width= 5, height = 3.3, device=cairo_ps)
```

1D animation for Twitter

```{r}
set.seed(8239709)

N <- 500

get_power <- function(n)
{
  p <- power.t.test(n=n, delta=0.3245459)$power
  c(rbinom(1, N, p)/N, p*(1-p)/N, N)
}

get_p <- function(input)
{
  # prob of a point being feasible
  m <- input[2]; sd <- input[3]
  mc_var <- m*(1-m)/N
  pred_q_mean <- m - qnorm(0.95)*sqrt(mc_var*(sd^2)/(mc_var + sd^2))
  pred_q_var <- ((sd^2)^2)/(mc_var + sd^2) 
  
  1 - pnorm(0.8, pred_q_mean, sqrt(pred_q_var))
}

consolidate <- function(df)
{
  if(sum(duplicated(df$n)) > 0){
    n_dup <- df[duplicated(df$n), "n"]
    p <- power.t.test(n=n_dup, delta=0.3245459)$power
    m <- mean(df[df$n == n_dup, "f"])
    df <- df[!duplicated(df$n),]
    new_N <- df[df$n == n_dup, "N"] + N
    df[df$n == n_dup, 2:4] <- c(m, p*(1-p)/new_N, new_N)
  }
  
  df
}

df <- data.frame(n = c(2, 200, 400))
df <- cbind(df, t(apply(df, 1, get_power)))
names(df)[2:4] <- c("f", "v", "N")
df$t <- 1

model <- km(design = df[,1, drop=FALSE], response = df[,2], noise.var = df[,3])

x <- data.frame(n=2:400)

p <- predict.km(model, newdata=x, type="SK")

x$f <- p$mean
x$sd <- p$sd
x$min <- x$f - qnorm(0.95)*x$sd
x$max <- x$f + qnorm(0.95)*x$sd
x$t <- 1

best <- x[x$n %in% df$n & x$min >= 0.8, ][1,"n"]
if(is.na(best)) best <- 400
df_best <- data.frame(n = best, t = 1)

probs <- cbind(x, V1=apply(x, 1, get_p))
probs$EFI <- (best-probs$n)*probs$V1
probs$EFI <- ifelse(probs$EFI < 0, 0, probs$EFI)
probs$t <- 1

to_try <- probs[which.max(probs$EFI), "n"]

for(i in 2:8){
  
  df1 <- df[df$t == (i-1), ]
  df1 <- rbind(df1, c(to_try, get_power(to_try), i))
  df1$t <- i
  
  #model <- km(design = df[,1, drop=FALSE], response = df[,2], noise.var = df[,3],
  #             parinit = coef(model))
  
  model <- update(model, df1[nrow(df1),1, drop=FALSE], df1[nrow(df1),2], 
                  newX.alreadyExist = max(duplicated(df1$n)),
                  newnoise.var = df1[nrow(df1),3])
  
  df1 <- consolidate(df1)
  df <- rbind(df, df1)
  
  x1 <- data.frame(n=2:400)
  
  p <- predict.km(model, newdata=x1, type="SK")
  
  x1$f <- p$mean
  x1$sd <- p$sd
  x1$min <- x1$f - 1.96*x1$sd
  x1$max <- x1$f + 1.96*x1$sd
  x1$t <- i
  x <- rbind(x, x1)
  
  best <- x1[x1$n %in% df1$n & x1$min >= 0.8, ][1,"n"]
  if(is.na(best)) best <- 400
  df_best <- rbind(df_best, c(best, i))
  
  probs1 <- cbind(x1, V1=apply(x1, 1, get_p))
  probs1$EFI <- (best-probs1$n)*probs1$V1
  probs1$EFI <- ifelse(probs1$EFI < 0, 0, probs1$EFI)
  probs1$t <- i
  probs <- rbind(probs, probs1)
  
  to_try <- probs1[which.max(probs1$EFI), "n"] 
}

probs$p <- sapply(probs$n, function(x) power.t.test(n=x, delta=0.32)$power)
df_best$n <- paste("Optimal n: ", as.character(df_best$n))

p <- ggplot(x, aes(n)) + 
  geom_point(data = df, aes(y = f)) +
  geom_line(aes(y = f)) + 
  geom_ribbon(aes(ymin=min, ymax=max), alpha=0.1) +
  geom_line(data = probs, aes(y = p), linetype = 2) + 
  theme_minimal() +
  xlab("Sample size") + ylab("Power") +
  #labs(title = "{df[nrow(df), 1]}") +
  geom_text(data = df_best, aes(label = n), x = 300, y = 0.3) +
  transition_states(
    t,
    wrap = FALSE
    #transition_length = 2,
    #state_length = 1
  ) +
  scale_y_continuous(breaks = seq(0,1.2,0.4)) +
  coord_cartesian(ylim = c(0, 1.2), xlim = c(0,400))

animate(p,
        nframes = 100,
        #fps = 20,
        start_pause = 5,
        end_pause = 15,
        height = 2, width = 4, units = "in", res = 300)

anim_save("1D_EGO.gif")
```