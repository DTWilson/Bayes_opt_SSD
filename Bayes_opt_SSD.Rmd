---
title: "Efficient and flexible simulation-based sample size determination for clinical trials with multiple design parameters"
author: D. T. Wilson
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(ggplot2)
require(gganimate)
require(reshape2)
require(pso)
require(mco)
require(randtoolbox)
require(mvtnorm)
require(DiceKriging)
require(RColorBrewer)
require(lme4)
require(xtable)
colours <- brewer.pal(8, "Dark2") 
set.seed(90307)
```

## Introduction

This RMarkdown document contains the R code which generates the results and figures included in the manuscript of the same name, plus supplementary material. We introduce the common functions `best()` and `exp_imrprove()` in the context of the first example, and call these in the remaining two examples. Simulated data have been saved to file.

## Example 1

### Simulation

The simulation model is:

```{r}
sim_trial <- function(x, h)
{
  ## x = design parameters, (n, k)
  ## h = hypothesis parameters, (r_t, r_d, v_w, p0, p1)
  
  n <- x[1]; k <- x[2]
  j <- 2*k
  
  r_t <- h[1]; r_d <- h[2]; v_w <- h[3]
  p0 <- h[4]; p1 <- h[5]
  
  ## Linear predicter logit terms
  lp0 <- log(p0/(1-p0)); lp1 <- log(p1/(1-p1)) - lp0
  
  ## Doctor effects
  v_d <- r_d*v_w/(1-r_d)
  doc <- cbind(seq(1,j), rnorm(j, 0, sqrt(v_d)))
  
  ## Therapist effects
  v_t <- (r_t*v_d + r_t*v_w)/(1-r_t)
  ther <- cbind(seq(0,k), c(0, rnorm(k, 0, sqrt(v_t))))

  ## Treatment group
  trt <- c(rep(0, n), rep(1, n))
  ## Patient ID
  p_id <- seq(1, 2*n)
  ## Doctor allocation and effect
  z <- rgamma(j, 1)
  ms <- round(z*2*n/sum(z))
  ms[which.max(ms)] <- ms[which.max(ms)] + 2*n - sum(ms)
  d_id <- sample(rep(doc[,1], ms))
  d_eff <- doc[d_id,2]
  ## Therapist allocation and effect
  z <- rgamma(k, 1)
  ms <- round(z*n/sum(z))
  ms[which.max(ms)] <- ms[which.max(ms)] + n - sum(ms)
  t_id <- c(rep(0, n), sample(rep(ther[2:(k+1),1], ms)))
  t_eff <- ther[t_id+1,2]
  ## Residual
  resid <- rlogis(2*n, 0, 1) ## Note - scale parameter = 1 ===> variance = 3.29
  
  data <- cbind(trt,p_id,d_id,d_eff,t_id,t_eff,resid)
  ## Latent variable
  data <- cbind(data, apply(data, 1, function(y) lp0 + y[1]*lp1 + y[4] + y[6] + y[7]))
  ## Outcome
  data <- cbind(data, data[,8] > 0)
    
  ## Analyse the data
  df <- as.data.frame(data) 
  names(df)[9] <- "y"
  
  result1 <- tryCatch({
    fit1 <- suppressMessages(glmer(y ~ trt + (0 + trt|t_id) + (1|d_id), family = "binomial", data=df))
    fit2 <- suppressMessages(glmer(y ~ (0 + trt|t_id) + (1|d_id), family = "binomial", data=df))
    p <- anova(fit1, fit2)[2,8]
  }, warning = function(war) {
    ## warning handler picks up where error was generated
    return(TRUE)
  }, error = function(err) {
    ## error handler picks up where error was generated
    return(TRUE)
  }, finally = {

  })
  
  if(result1==TRUE){
    return(c(1,1))
  } else {
    return(c(result1, 0))
  }

}

## For example,
sim_trial(x=c(n=150, k=10), h=c(r_t=0.05, r_d=0.1, v_w=3.29, p0=0.1, p1=0.25))
```

Given this simulation, we can then evaluate the probability of failing to reject the null hypothesis of any design `x` under any hypothesis `h`, based on `N` MC samples:

```{r}
calc_rates <- function(x, h, N)
{
  sims <- replicate(N, sim_trial(x, h) > 0.05)
  z <- sims[1,]
  ## Print out model failure rate
  print(sum(sims[2,])/N)
  ## Return proportion not rejecting null and approximate variance in estimate
  c(mean(z), var(z)/N)
}

## For example,
x <- c(n=135, k=10)
h <- c(r_t=0.05, r_d=0.1, v_w=3.29, p0=0.1, p1=0.25)

ptm <- proc.time()
calc_rates(x, h, 10)
proc.time() - ptm
```

### Initial design and GP

The first step of our method is to evaluate at an initial set of design points.

```{r}
design_space <- data.frame(name=c("n","k"), 
                           low=c(100,3), 
                           up=c(500,30)
)

DoE_num <- 20
dim <- nrow(design_space)
  
## Choose initial points using a Sobol sequence
DoE <- data.frame(sobol(DoE_num, dim))
names(DoE) <- design_space$name
for(i in 1:dim){
  DoE[,i] <-  DoE[,i]*(design_space$up[i]-design_space$low[i]) + design_space$low[i]
}
DoE[,1:2] <- round(DoE[,1:2])

## Evaluate at initial points [NOT RUN]
#N <- 100
#DoE <- cbind(DoE, t(apply(DoE, 1, calc_rates, N=N, h=h)))
#names(DoE)[3:4] <- c("beta", "beta_var")
#DoE$N <- N

## Save initial design
#saveRDS(DoE, "DoE_ex1_20_N100.Rda")
```

Given these estimates, we can model the power function over the full design space using a GP. We can then plot both the mean function and the standard deviation for each point.

```{r}
## Load the DoE
DoE <- readRDS("./data/DoE_ex1_20_N100.Rda")

constraints <- data.frame(name=c("beta"), 
                          hyp=c("H1"), 
                          nom=c(0.1),
                          delta=c(0.975))

models <- list()
for(i in 1:nrow(constraints)){
  name <- constraints$name[i]
  models <- append(models, km(~1, design=DoE[1:dim], response=DoE[,as.character(name)], 
                             noise.var=DoE[,paste0(as.character(name), "_var")]))
  names(models)[length(models)] <- as.character(name)
}

mod <- models[[1]]

df <- expand.grid(n = seq(100,500,1), k=3:30)
pred <- predict(mod, newdata=df, type="SK")
df$beta <- pred$mean
df$sd <- pred$sd
```

```{r}
## Plot the mean function
ggplot(df, aes(n, k, z=1-beta)) + geom_contour(aes(colour=..level..)) + geom_point(data=DoE, aes(colour=1-beta)) +
  scale_colour_gradientn(colours=rainbow(3)) + theme_minimal() + guides(colour=guide_legend(title="Power"))
## Plot the standard deviation
ggplot(df, aes(n, k)) + geom_contour(aes(z=sd, colour=..level..)) + geom_point(data=DoE) +
  theme_minimal() + guides(colour=guide_legend(title="SD"))
```

Note that the standard deviation of the GP prediction increases as we move away from evaluated points.

### Pareto fronts

Of the points which we have evaluated, We can identify which lead to sufficient power and which can be discarded because they are dominated by another feasible solution.

```{r}
## Set up our objectives
dim <- nrow(design_space)
nobj <- 2

obj_names <- NULL
for(i in 1:nobj){
  obj_names <- c(obj_names, paste0("f",i))
}
  
objective <- function(x)
{
  c(x[1]*2/5, x[2])
}

best <- function(design_space, models, DoE, b=NULL)
{ 
  ## Return the set of current Patero optimal solutions,
  ## penalising constrain violations considering only solutions
  ## where some evaluation has actually happened
  sols <- DoE
  
  ## Get objective values
  sols <- cbind(sols, t(apply(sols, 1, objective)))
  names(sols)[(ncol(sols)-nobj+1):ncol(sols)] <- obj_names
  
  ## Penalise constraint violations
  sols$exp_pen <- 1
  for(i in 1:nrow(constraints)){
    mod <- models[[i]]
    nom <- constraints$nom[i]
    p <- predict.km(mod, newdata=sols[,1:dim], type="SK")
    pen <- pnorm(nom, p$mean, p$sd)
    pen <- ifelse(pen < constraints[i,4], 0.0000001, 1)
    sols$exp_pen <- sols$exp_pen*pen
  }
  sols[,obj_names] <- sols[,obj_names]/sols$exp_pen
  
  ## Drop any dominated solutions
  is_nondom <- function(x, b)
  {
    i <- 1
    while(i <= nobj & nrow(b)!= 0 ){
      ## subset b to those non-dominated solutions which are less than or equal to 
      ## x in an objective
      b <- b[b[, obj_names[i] ] <= x[[ obj_names[i] ]],]
      i <- i + 1
    }
    ## If b is now empty, then des is non-dominated
    if(nrow(b)==1 | all(apply(b[,c("f1","f2")], 2, function(x) length(unique(x)) == 1) == TRUE) ) {
      nondom <- TRUE
    } else {
      nondom <- FALSE
    }
    return(nondom)
  }
  
  nondom_sols <- sols[apply(sols, 1, is_nondom, b=sols), ]
  ## check for duplicates
  sub <- unique(nondom_sols[,obj_names])
  
  return(nondom_sols[row.names(sub),])
}

b <- best(design_space, models, DoE)

## Add extreme points for plotting the Pareto front
b2 <- b[,1:2]
b2 <- rbind(c(min(b2[,1]), 30), b2, c(500, min(b2[,2])))

ggplot(df, aes(n, k)) + geom_contour(aes(z=beta)) + geom_point(data=DoE) +
  geom_point(data=b2[2:(nrow(b2)-1),], colour="red") + 
  geom_step(data=b2, colour="red", linetype=2) +
  theme_minimal() 
```

### Expected improvement

We have identified a set of designs which we are ceratin will give nominal power and which are non-dominated. The task now is to improve this approximation set iteratively, evaluating one design at a time, updating our GP model, and then updating the set. When we choose the next point to evaluate we do so to maximimse the expected improvement which will result.

```{r}
exp_improve <- function(design, N, p_set, models, design_space, constraints)
{
  names(design) <- design_space$name
  design <- as.data.frame(t(design))
  
  ## Get expected penalisation if we were to evaluate at design,
  ## using the models of constraint functions
  design$exp_pen <- 1
  for(i in 1:nrow(constraints)){
    mod <- models[[i]]
    nom <- constraints$nom[i]
    p <- predict.km(mod, newdata=design[,1:dim], type="SK")
    means <- ifelse(p$mean < 0, 0, ifelse(p$mean > 1, 1, p$mean))
    mc_vars <- means*(1-means)/N
    pred_q_mean <- p$mean + qnorm(constraints[i,4])*sqrt(mc_vars*(p$sd^2)/(mc_vars+(p$sd^2)))
    pred_q_var <- ((p$sd^2)^2)/(mc_vars+(p$sd^2)) 
    design$exp_pen <- design$exp_pen*pnorm(nom, pred_q_mean, sqrt(pred_q_var))
  }
  
  ## Get objective value of design
  design <- cbind(design, t(objective(as.numeric(design))))
  names(design)[(ncol(design)-nobj+1):ncol(design)] <- obj_names
  
  ## Improvement is quantified by the number of additional
  ## solutions which would be dominated if this design was included
  ref <- objective(design_space$up)
  p_set2 <- as.matrix(p_set[,obj_names])
  current <- dominatedHypervolume(p_set2, ref)
  pos <- apply(design, 1, function(d) dominatedHypervolume(as.matrix(rbind(p_set2, d[obj_names])), ref) )
  imp <- (current-pos)*design$exp_pen
  
  ## Minimising, so keeping negative
  return(imp)
}
```


```{r, eval=F}
## Find the next point to evaluate, using a particlae swarm algorithm:
opt <- psoptim(rep(NA, 2), exp_improve, lower=design_space$low, upper=design_space$up,
               N=100, p_set=b, models=models, design_space=design_space, constraints=constraints,
               control=list(vectorize = T))
sol <- opt$par
sol[1:2] <- round(sol[1:2])

## Evaluate the suggested point
y <- calc_rates(sol, h, N=100)
```

```{r}
## Hard code the results
sol <- c(304, 12)
y <- c(0.06, 0.000569697)
```


The recommended evaluation is at $n=$ `r sol[1]`, $k=$ `r sol[2]`. Evaluating the point using $N=100$ MC samples gives an estimated type II error rate of `r y[1]` (sd `r sqrt(y[2])`), so it appears to be feasible. We can add the new information to our records, update the GP model, and then use its predictions to confirm whether or not the new point can join the approximation set.

```{r}
DoE <- rbind(DoE, c(sol, y, 100))

for(i in 1:length(models)){
    models[[i]] <- update(models[[i]], 
                          newX=DoE[nrow(DoE),1:2], 
                          newy=DoE[nrow(DoE), 2*i+1], 
                          newnoise.var=DoE[nrow(DoE), 2*i+2])
}

b <- best(design_space, models, DoE)
```

### Algorithm

The above process can be repeated for the desired number of iterations. Here start from the beginning, with another initial design of 20 points followed by 30 iterations of the algorithm.

```{r, eval=F}
ptm <- proc.time()

design_space <- data.frame(name=c("n","k"), 
                           low=c(100,3), 
                           up=c(500,30)
)

constraints <- data.frame(name=c("beta"), 
                          hyp=c("H1"), 
                          nom=c(0.1),
                          delta=c(0.975))

DoE_num <- 20
dim <- nrow(design_space)
ref <- objective(design_space$up)
  
## Choose initial points
DoE <- data.frame(sobol(DoE_num, dim))
names(DoE) <- design_space$name
for(i in 1:dim){
  DoE[,i] <-  DoE[,i]*(design_space$up[i]-design_space$low[i]) + design_space$low[i]
}
DoE[,1:2] <- round(DoE[,1:2])

## Evaluate at initial points
N <- 100
DoE <- cbind(DoE, t(apply(DoE, 1, calc_rates, h=h, N=N)))
names(DoE)[3:4] <- c("beta", "beta_var")
DoE$N <- N

models <- list()
for(i in 1:nrow(constraints)){
  name <- constraints$name[i]
  models <- append(models, km(~1, design=DoE[1:dim], response=DoE[,as.character(name)], 
                             noise.var=DoE[,paste0(as.character(name), "_var")]))
  names(models)[length(models)] <- as.character(name)
}

b <- best(design_space, models, DoE)

DHs <- NULL

for(i in 1:30){
  opt <- psoptim(rep(NA, 2), exp_improve, lower=design_space$low, upper=design_space$up,
                 N=100, p_set=b, mod=models, design_space=design_space, constraints=constraints,
                 control=list(vectorize = T))
  sol <- opt$par
  sol[1:2] <- round(sol[1:2])
  
  ## track the objective value at each step
  p_set2 <- as.matrix(b[,obj_names])
  current <- dominatedHypervolume(p_set2, ref)
  DHs <- c(DHs, dominatedHypervolume(p_set2, ref))
  
  ## Do the evaluation and add to the design
  y <- calc_rates(sol, h, N=100)
  
  DoE <- rbind(DoE, c(sol, y, 100))
  
  models <- list()
  for(i in 1:nrow(constraints)){
    name <- constraints$name[i]
    models <- append(models, km(~1, design=DoE[1:dim], response=DoE[,as.character(name)], 
                               noise.var=DoE[,paste0(as.character(name), "_var")]))
    names(models)[length(models)] <- as.character(name)
  }
  
  b <- best(design_space, models, DoE)
}

proc.time() - ptm
## 46.759 minutes

## Save the dominated hypervolumes of the approximation sets at each iteration
#saveRDS(DHs, "./data/ex1_single_run_DHs.Rda")

## Save the final DoE table containing all the points evaluated over the whole algorithm
#saveRDS(DoE, "./data/ex1_single_run_DoE.Rda")
```

For comparison, the fixed 50 point Sobol experimental design.
```{r, eval=F}
## Choose initial points
DoE_num <- 50
DoE <- data.frame(sobol(DoE_num, dim))
names(DoE) <- design_space$name
for(i in 1:dim){
  DoE[,i] <-  DoE[,i]*(design_space$up[i]-design_space$low[i]) + design_space$low[i]
}
DoE[,1:2] <- round(DoE[,1:2])

## Evaluate at initial points
N <- 100
DoE <- cbind(DoE, t(apply(DoE, 1, calc_rates, h=h, N=N)))
names(DoE)[3:4] <- c("beta", "beta_var")
DoE$N <- N

## Discard any solutions which are not probably powered
DoE$pow_lim <- DoE$beta + qnorm(0.975)*sqrt(DoE$beta_var)
DoE <- DoE[DoE$pow_lim <= 0.1,]

## Discard any remaining solutions that are dominated
is_nondom <- function(x, b)
{
    i <- 1
    while(i <= nobj & nrow(b)!= 0 ){
      ## subset b to those non-dominated solutions which are less than or equal to 
      ## x in an objective
      b <- b[b[, obj_names[i] ] <= x[[ obj_names[i] ]],]
      i <- i + 1
    }
    ## If b is now empty, then des is non-dominated
    if(nrow(b)==1 | all(apply(b[,c("f1","f2")], 2, function(x) length(unique(x)) == 1) == TRUE) ) {
      nondom <- TRUE
    } else {
      nondom <- FALSE
    }
    return(nondom)
}
 
DoE <- cbind(DoE, t(apply(DoE, 1, objective)))
names(DoE)[(ncol(DoE)-nobj+1):ncol(DoE)] <- obj_names
DoE <- DoE[apply(DoE, 1, is_nondom, b=DoE), ]

#saveRDS(DoE, "./data/ex1_sobol_DoE.Rda")
```

### Results

Plot the final results:

```{r}
DHs <- readRDS("./data/ex1_single_run_DHs.Rda")
DoE <- readRDS("./data/ex1_single_run_DoE.Rda")
DoE_sob <- readRDS("./data/ex1_sobol_DoE.Rda")

models <- list()
for(i in 1:nrow(constraints)){
  name <- constraints$name[i]
  models <- append(models, km(~1, design=DoE[1:dim], response=DoE[,as.character(name)], 
                             noise.var=DoE[,paste0(as.character(name), "_var")]))
  names(models)[length(models)] <- as.character(name)
}

## The final approximation set
b <- best(design_space, models, DoE)
## Extend to include extreme points for plotting
b2 <- b[,1:2]
b2 <- rbind(c(min(b2[,1]), 30), b2, c(500, min(b2[,2])))

## Fixed design comparator
b3 <- DoE_sob[,1:2]
b3 <- rbind(c(min(b3[,1]), 30), b3, c(500, min(b3[,2])))

## Get the GP predictions
mod <- models[[1]]
df <- expand.grid(n = seq(100,500,10), k=3:30)
pred <- predict(mod, newdata=df, type="SK")
df$beta <- pred$mean
df$sd <- pred$sd
```

```{r}
b2$beta <- 1; b3$beta <- 1
## Plot evaluated points With the mean function
ggplot(df, aes(2*n, 3*k, z=beta)) + geom_contour(colour="light blue") + 
  geom_point(data=DoE[1:20,], colour=colours[1], shape=1) +
  geom_point(data=DoE[21:60,], colour=colours[2], shape=4) +
  
  geom_point(data=b2[2:(nrow(b2)-1),], colour=colours[3]) +
  geom_step(data=b2, colour=colours[3], linetype=2) +
  
  geom_point(data=b3[2:(nrow(b3)-1),], colour=colours[5]) +
  geom_step(data=b3, colour=colours[5], linetype=2) +
  
  theme_minimal() + xlab("Number of participants") + ylab("Number of providers")

#ggsave("./paper/figures/ex1_single_run.pdf", height=9, width=14, units="cm")
#ggsave("./paper/figures/ex1_single_run.eps", height=9, width=14, units="cm")
#ggsave("./paper/figures/ex1_single_run.png", height=3, width=5)

## Plot dominated hypervolumes over the search
df2 <- data.frame(it = 1:30, DH=DHs)
ggplot(df2, aes(it, DH)) + geom_point() + geom_line() + 
  theme_minimal() + ylab("Dominated hypervolume") + xlab("Iteration")

#ggsave("./paper/figures/ex1_traj.pdf", height=9, width=14, units="cm")
#ggsave("./paper/figures/ex1_traj.eps", height=9, width=14, units="cm")
```

For each solution in the approximation set, we can verify whether or not it really is within the type II error rate nominal bound by re-evaluating it using $N = 50,000$ MC samples. 

```{r, eval=F}
tab <- b[,1:4]
new_eval <- NULL
for(i in 1:nrow(tab)){
  new_eval <- rbind(new_eval, calc_rates(as.numeric(tab[i, 1:2]), h=h, N=50000))
}
tab <- cbind(tab, new_eval)
names(tab)[5:6] <- c("beta_2", "beta_var_2")

#saveRDS(tab, "./data/ex1_large_N.Rda")
```

```{r}
tab <- readRDS("./data/ex1_large_N.Rda")

## Print the results in a table
tab2 <- data.frame(n=tab$n*2, k=tab$k, j=tab$k*2, 
                   beta=apply(tab[,3:4], 1, function(x) paste0(round(x[1], 2), " (", round(sqrt(x[2]), 3), ")")),
                   beta_2=apply(tab[,5:6], 1, function(x) paste0(round(x[1], 3), " (", round(sqrt(x[2]), 3), ")"))
                   )
tab2 <- tab2[order(tab2$k),]
colnames(tab2) <- c("$2n$", "$k$", "$j$", "$\\beta$ (s.e.), $N = 10^2$", "$\\beta$ (s.e.), $N = 50^4$")
tab2

# print(xtable(tab2, digits=0), booktabs = T, include.rownames = F, 
#       sanitize.text.function = function(x) {x}, floating = F,
#       file = "./paper/tables/ex1_results.txt")
```


Now plot the results as an animation:

```{r, eval=F, echo=F}
to_pred <- expand.grid(n = seq(100,500,5), k=3:30)
p_fronts <- NULL
evals <- NULL

for(i in 5:nrow(DoE)){
  sub <- DoE[1:i,]
  mod <- km(~1, design=sub[1:dim], response=sub[,"beta"], noise.var=sub[,"beta_var"])
  b <- best(design_space, models=list(mod), sub)
  
  pred <- predict(mod, newdata=to_pred[,1:2], type="SK")
  to_pred <- cbind(to_pred, pred$mean)
  p_fronts <- rbind(p_fronts, cbind(b[,c("n", "k")], it=rep(i-4, nrow(b))))
  evals<- rbind(evals, cbind(sub[,c("n", "k")], it=rep(i-4, nrow(sub))))
}

names(to_pred)[3:ncol(to_pred)] <- 1:(ncol(to_pred) - 2)
df <- melt(to_pred, c("n", "k"))
names(df)[3:4] <- c("it", "beta")

ggplot(df, aes(n, k)) + geom_raster(aes(fill=beta)) + 
  scale_fill_gradientn(colours=rainbow(3)) + 
  geom_point(data=evals) + 
  geom_point(data=p_fronts, size=3, shape=4) + 
  transition_states(
    it,
    transition_length = 1,
    state_length = 2
  )
```

## Example 2

We extend the problem above to an analysis of two binary endpoints, fatigue and disability.

### Simualtion

```{r}
sim_trial2 <- function(x, h)
{
  n <- x[1]; k <- x[2]
  j <- 2*k
  
  r_t <- h[1]; r_d <- h[2]; v_w <- h[3]; cov_res <- h[4]; cov_w <- h[5]
  p0a <- h[6]; p1a <- h[7]; p0b <- h[8]; p1b <- h[9]
  
  ## Find the terms for the linear predictor which correspond to the probabilities
  lp0a <- log(p0a/(1-p0a)); lp1a <- log(p1a/(1-p1a)) - lp0a
  lp0b <- log(p0b/(1-p0b)); lp1b <- log(p1b/(1-p1b)) - lp0b
  
  ## Doctor effects
  v_d <- r_d*v_w/(1-r_d)
  doc <- cbind(seq(1,j), rmvnorm(j, c(0,0), matrix(c(v_d, v_d*cov_res, v_d*cov_res, v_d), ncol=2, byrow = 2)))
  
  ## Therapist effects
  v_t <- (r_t*v_d + r_t*v_w)/(1-r_t)
  ther <- cbind(seq(0,k), rbind(c(0, 0), rmvnorm(k, c(0,0), matrix(c(v_t, v_t*cov_res, v_t*cov_res, v_t), ncol=2, byrow = 2))))
  
  ## 1   | 2    | 3   | 4    | 5    | 6     | 7
  ## arm | p_id |d_id | d_ef | t_id | t_eff | outcome
  
  ## Treatment group
  trt <- c(rep(0, n), rep(1, n))
  ## Patient ID
  p_id <- seq(1, 2*n)
  ## Doctor allocation and effect
  z <- rgamma(j, 1)
  ms <- round(z*2*n/sum(z))
  ms[which.max(ms)] <- ms[which.max(ms)] + 2*n - sum(ms)
  d_id <- sample(rep(doc[,1], ms))
  d_eff_a <- doc[d_id,2]
  d_eff_b <- doc[d_id,3]
  ## Therapist allocation and effect
  z <- rgamma(k, 1)
  ms <- round(z*n/sum(z))
  ms[which.max(ms)] <- ms[which.max(ms)] + n - sum(ms)
  t_id <- c(rep(0, n), sample(rep(ther[2:(k+1),1], ms)))
  t_eff_a <- ther[t_id+1,2]
  t_eff_b <- ther[t_id+1,3]
  ## Residual - now bi-variate
  resid_n <- rmvnorm(2*n, c(0,0), matrix(c(1, cov_w, cov_w, 1), ncol=2, byrow = 2))
  ps <- pnorm(resid_n)
  resid_log  <- qlogis(ps)

  data_a <- cbind(trt,p_id,d_id,d_eff_a,t_id,t_eff_a,resid_log[,1])
  data_b <- cbind(trt,p_id,d_id,d_eff_b,t_id,t_eff_b,resid_log[,2])
  
  ## Outcomes
  data_a <- cbind(data_a, apply(data_a, 1, function(y, lp1, lp0) lp0 + y[1]*lp1 + y[4] + y[6] + y[7], lp1=lp1a, lp0=lp0a))
  data_a <- cbind(data_a, data_a[,8] > 0)
  data_b <- cbind(data_b, apply(data_b, 1, function(y, lp1, lp0) lp0 + y[1]*lp1 + y[4] + y[6] + y[7], lp1=lp1b, lp0=lp0b))
  data_b <- cbind(data_b, data_b[,8] > 0)
    
  ## Analyse the data
  df_a <- as.data.frame(data_a) 
  names(df_a)[9] <- c("y")
  df_b <- as.data.frame(data_b) 
  names(df_b)[9] <- c("y")
  
  result_a <- tryCatch({
    fit1 <- suppressMessages(glmer(y ~ trt + (0 + trt|t_id) + (1|d_id), family = "binomial", data=df_a))
    fit2 <- suppressMessages(glmer(y ~ (0 + trt|t_id) + (1|d_id), family = "binomial", data=df_a))
    p <- anova(fit1, fit2)[2,8]
  }, warning = function(war) {
    ## warning handler picks up where error was generated
    return(1)
  }, error = function(err) {
    ## error handler picks up where error was generated
    print("error")
    return(1)
  }, finally = {

  })
  
  result_b <- tryCatch({
    fit1 <- suppressMessages(glmer(y ~ trt + (0 + trt|t_id) + (1|d_id), family = "binomial", data=df_b))
    fit2 <- suppressMessages(glmer(y ~ (0 + trt|t_id) + (1|d_id), family = "binomial", data=df_b))
    p <- anova(fit1, fit2)[2,8]
  }, warning = function(war) {
    ## warning handler picks up where error was generated
    return(1)
  }, error = function(err) {
    ## error handler picks up where error was generated
    print("error")
    return(1)
  }, finally = {

  })

  return(c(result_a, result_b))
}
```

```{r}
calc_rates2 <- function(x, h, N)
{
  ps <- replicate(N, sim_trial2(x, h))
  y <- ps[1,] > 0.05 | ps[2,] > 0.05
  res <- c(mean(y), var(y)/N)
  
  res
}

## For example,
x <- c(n=181, k=7)
h <- c(r_t=0.05, r_d=0.1, v_w=3.29, cov_res=0.9, cov_w=0.9, p0a=0.1, p1a=0.25, p0b=0.1, p1b=0.25)

ptm <- proc.time()
calc_rates2(x, h, 10)
proc.time() - ptm
```

### Algorithm

```{r, eval=T}
set.seed(90307)

ptm <- proc.time()

design_space <- data.frame(name=c("n","k"), 
                           low=c(100,3), 
                           up=c(500,30)
)

constraints <- data.frame(name=c("beta"), 
                          hyp=c("H1"), 
                          nom=c(0.1), 
                          delta=c(0.975))

nobj <- 2

obj_names <- NULL
for(i in 1:nobj){
  obj_names <- c(obj_names, paste0("f",i))
}

objective <- function(x)
{
  c(x[1]*2/5, x[2])
}

DoE_num <- 20
dim <- nrow(design_space)
ref <- objective(design_space$up)
  
## Choose initial points
DoE <- data.frame(sobol(DoE_num, dim))
names(DoE) <- design_space$name
for(i in 1:dim){
  DoE[,i] <-  DoE[,i]*(design_space$up[i]-design_space$low[i]) + design_space$low[i]
}
DoE[,1:2] <- round(DoE[,1:2])
```

```{r, eval=F}
## Evaluate at initial points
N <- 100
h <- c(r_t=0.05, r_d=0.1, v_w=3.29, cov_res=0.9, cov_w=0.9, p0a=0.1, p1a=0.25, p0b=0.1, p1b=0.25)
DoE <- cbind(DoE, t(apply(DoE, 1, calc_rates2, h=h, N=N)))
names(DoE)[3:4] <- c("beta", "beta_var")
DoE$N <- N

models <- list()
for(i in 1:nrow(constraints)){
  name <- constraints$name[i]
  models <- append(models, km(~1, design=DoE[1:dim], response=DoE[,as.character(name)], 
                             noise.var=DoE[,paste0(as.character(name), "_var")]))
  names(models)[length(models)] <- as.character(name)
}

## Get the current set of Pareto solutions
b <- best(design_space, models, DoE)

DHs <- NULL

proc.time() - ptm

for(i in 1:30){
  opt <- psoptim(rep(NA, 2), exp_improve, lower=design_space$low, upper=design_space$up,
                 N=100, p_set=b, models=models, design_space=design_space, constraints=constraints,
                 control=list(vectorize = T))
  sol <- opt$par
  sol[1:2] <- round(sol[1:2])
  
  ## track the objective value at each step
  p_set2 <- as.matrix(b[,obj_names])
  current <- dominatedHypervolume(p_set2, ref)
  DHs <- c(DHs, dominatedHypervolume(p_set2, ref))
  
  ## Do the evaluation and add to the design
  y <- calc_rates2(sol, h, N=100)
  
  DoE <- rbind(DoE, c(sol, y, 100))
  
  models <- list()
  for(i in 1:nrow(constraints)){
    name <- constraints$name[i]
    models <- append(models, km(~1, design=DoE[1:dim], response=DoE[,as.character(name)], 
                             noise.var=DoE[,paste0(as.character(name), "_var")]))
    names(models)[length(models)] <- as.character(name)
  }

  b <- best(design_space, models, DoE)
}

proc.time() - ptm
# 96.33883 minutes

## Save the dominated hypervolumes of the approximation sets at each iteration
#saveRDS(DHs, "./data/ex2_single_run_DHs.Rda")

## Save the final DoE table containing all the points evaluated over the whole algorithm
#saveRDS(DoE, "./data/ex2_single_run_DoE.Rda")
```

### Results

```{r}
DoE <- readRDS("./data/ex2_single_run_DoE.Rda")
DHs <- readRDS("./data/ex2_single_run_DHs.Rda")

models <- list()
for(i in 1:nrow(constraints)){
  name <- constraints$name[i]
  models <- append(models, km(~1, design=DoE[1:dim], response=DoE[,as.character(name)], 
                             noise.var=DoE[,paste0(as.character(name), "_var")]))
  names(models)[length(models)] <- as.character(name)
}

b <- best(design_space, models, DoE)
b2 <- b[,1:2]
b2 <- rbind(c(min(b2[,1]), 30), b2, c(500, min(b2[,2])))

# Get the GP predictions
mod <- models[[1]]
df <- expand.grid(n = seq(100,500,10), k=3:30)
pred <- predict(mod, newdata=df, type="SK")
df$beta <- pred$mean
df$sd <- pred$sd
```

```{r}
ggplot(df, aes(2*n, 3*k)) + geom_contour(aes(z=beta), colour = "light blue") + 
  geom_point(data=DoE[1:20,], colour=colours[1], shape=1) +
  geom_point(data=DoE[21:50,], colour=colours[2], shape=4) +
  
  geom_point(data=b2[2:(nrow(b2)-1),], colour=colours[3]) +
  geom_step(data=b2, colour=colours[3], linetype=2) +

  theme_minimal() + xlab("Number of participants") + ylab("Number of providers")

#ggsave("./paper/figures/ex2_single_run.pdf", height=9, width=14, units="cm")
#ggsave("./paper/figures/ex2_single_run.eps", height=9, width=14, units="cm")

## Plot dominated hypervolumes over the search
df2 <- data.frame(it = 1:30, DH=DHs)
ggplot(df2, aes(it, DH)) + geom_point() + geom_line() + 
  theme_minimal() + ylab("Dominated hypervolume") + xlab("Iteration")

#ggsave("./paper/figures/ex2_traj.pdf", height=9, width=14, units="cm")
#ggsave("./paper/figures/ex2_traj.eps", height=9, width=14, units="cm")
```

```{r, eval=F}
tab <- b[,1:4]
new_eval <- NULL
for(i in 1:nrow(tab)){
  new_eval <- rbind(new_eval, calc_rates2(as.numeric(tab[i, 1:2]), h=h, N=10000))
}
tab <- cbind(tab, new_eval)
names(tab)[5:6] <- c("beta_2", "beta_var_2")

#saveRDS(tab, "./data/ex2_large_N.Rda")
```

```{r}
## Print the results in a table
tab <- readRDS("./data/ex2_large_N.Rda")

tab2 <- data.frame(n=tab$n*2, k=tab$k, j=tab$k*2, 
                   beta=apply(tab[,3:4], 1, function(x) paste0(round(x[1], 2), " (", round(sqrt(x[2]), 3), ")")),
                   beta_2=apply(tab[,5:6], 1, function(x) paste0(round(x[1], 3), " (", round(sqrt(x[2]), 3), ")"))
                   )
tab2 <- tab2[order(tab2$k),]
colnames(tab2) <- c("$2n$", "$k$", "$j$", "$\\beta$ (s.e.), $N = 10^2$", "$\\beta$ (s.e.), $N = 50^4$")
tab2

# print(xtable(tab2, digits=0), booktabs = T, include.rownames = F, 
#       sanitize.text.function = function(x) {x}, floating = F,
#       file = "./paper/tables/ex2_results.txt")
```

## Example 3

### Simualtion

```{r}
sim_trial3 <- function(x, h)
{
  n_1 <- x[1]; k <- x[2]; r <- x[3]; j <- x[4]
  n_0 <- round(r*n_1); n_t <- n_0 + n_1
  
  r_t <- h[1]; r_d <- h[2]; v_w <- h[3]; cov_res <- h[4]; cov_w <- h[5]
  p0a <- h[6]; p1a <- h[7]; p0b <- h[8]; p1b <- h[9]
  
  # Find the terms for the linear predictor which correspond to the probabilities
  lp0a <- log(p0a/(1-p0a)); lp1a <- log(p1a/(1-p1a)) - lp0a
  lp0b <- log(p0b/(1-p0b)); lp1b <- log(p1b/(1-p1b)) - lp0b
  
  # Doctor effects
  v_d <- r_d*v_w/(1-r_d)
  doc <- cbind(seq(1,j), rmvnorm(j, c(0,0), matrix(c(v_d, v_d*cov_res, v_d*cov_res, v_d), ncol=2, byrow = 2)))
  
  # Therapist effects
  v_t <- (r_t*v_d + r_t*v_w)/(1-r_t)
  ther <- cbind(seq(0,k), rbind(c(0, 0), rmvnorm(k, c(0,0), matrix(c(v_t, v_t*cov_res, v_t*cov_res, v_t), ncol=2, byrow = 2))))
  
  # 1   | 2    | 3   | 4    | 5    | 6     | 7
  # arm | p_id |d_id | d_ef | t_id | t_eff | outcome
  
  # Treatment group
  trt <- c(rep(0, n_0), rep(1, n_1))
  # Patient ID
  p_id <- seq(1, n_t)
  # Doctor allocation and effect
  d_id <- sample(rep(1:j, ceiling(n_t/j)*j)[1:n_t])
  d_eff_a <- doc[d_id,2]
  d_eff_b <- doc[d_id,3]
  # Therapist allocation and effect
  t_id <- c(rep(0, n_0), rep(1:k, ceiling(n_1/k)*k)[1:n_1])
  t_eff_a <- ther[t_id+1,2]
  t_eff_b <- ther[t_id+1,3]
  # Residual - now bi-variate
  resid_n <- rmvnorm(n_t, c(0,0), matrix(c(v_w, cov_w*v_w, cov_w*v_w, v_w), ncol=2, byrow = 2))
  
  data_a <- cbind(trt,p_id,d_id,d_eff_a,t_id,t_eff_a,resid_n[,1])
  data_b <- cbind(trt,p_id,d_id,d_eff_b,t_id,t_eff_b,resid_n[,2])
  
  var(data_a[,7])
  
  # Outcomes
  data_a <- cbind(data_a, apply(data_a, 1, function(y, lp1, lp0) lp0 + y[1]*lp1 + y[4] + y[6] + y[7], lp1=lp1a, lp0=lp0a))
  data_b <- cbind(data_b, apply(data_b, 1, function(y, lp1, lp0) lp0 + y[1]*lp1 + y[4] + y[6] + y[7], lp1=lp1b, lp0=lp0b))
    
  # Analyse the data
  df_a <- as.data.frame(data_a) 
  names(df_a)[8] <- c("y")
  df_b <- as.data.frame(data_b) 
  names(df_b)[8] <- c("y")
  
  result_a <- tryCatch({
    fit1 <- suppressMessages(lmer(y ~ trt + (0 + trt|t_id) + (1|d_id), REML=F, data=df_a))
    fit2 <- suppressMessages(lmer(y ~ (0 + trt|t_id) + (1|d_id), REML=F, data=df_a))
    p <- anova(fit1, fit2)[2,8]
  }, warning = function(war) {
    # warning handler picks up where error was generated
    #print("warning")
    return(1)
  }, error = function(err) {
    # error handler picks up where error was generated
    print("error")
    return(1)
  }, finally = {

  })
  
  result_b <- tryCatch({
    fit1 <- suppressMessages(lmer(y ~ trt + (0 + trt|t_id) + (1|d_id), REML=F, data=df_b))
    fit2 <- suppressMessages(lmer(y ~ (0 + trt|t_id) + (1|d_id), REML=F, data=df_b))
    p <- anova(fit1, fit2)[2,8]
  }, warning = function(war) {
    # warning handler picks up where error was generated
    #print("warning")
    return(1)
  }, error = function(err) {
    # error handler picks up where error was generated
    print("error")
    return(1)
  }, finally = {

  })

  return(c(result_a, result_b))
}
```

```{r}
calc_rates3 <- function(x, h, N)
{
  alpha <- x[5]
  
  # Alternative hypothesis - one of the endpints is non-null
  ha <- h; ha["p1b"] <- ha["p0b"]
  psa <- replicate(N, sim_trial3(x, ha))
  ya <- psa[1,] > alpha & psa[2,] > alpha
  res_a <- c(mean(ya), var(ya)/N)
  
  # Null hypothesis - both of the endpoints are null
  hn <- h; hn["p1a"] <- hn["p0a"]; hn["p1b"] <- hn["p0b"]
  psn <- replicate(N, sim_trial3(x, hn))
  yn <- psn[1,] < alpha | psn[2,] < alpha
  res_n <- c(mean(yn), var(yn)/N)
  
  c(res_a, res_n)#, mean(psa==10))
}

# For example,
x <- c(n_1=50, k=7, r=1, j=5, a=0.2)
h <- c(r_t=0.05, r_d=0.1, v_w=3.29, cov_res=0.9, cov_w=0.9, p0a=0.1, p1a=0.25, p0b=0.1, p1b=0.25)

ptm <- proc.time()
calc_rates3(x, h, 10)
proc.time() - ptm
```

### Algorithm

```{r}
set.seed(90309)

ptm <- proc.time()

constraints <- data.frame(name=c("beta", "alpha"), 
                          hyp=c("H1", "H0"), 
                          nom=c(0.1, 0.2), 
                          delta=c(0.975,0.975))

design_space <- data.frame(name=c("n_1","k", "r","j","a"), 
                           low=c(20,2,0.5,3, 0.2), 
                           up=c(100,10,1.5,25, 0.01)
                           )

h <- c(r_t=0.05, r_d=0.1, v_w=3.29, cov_res=0.9, cov_w=0.9, p0a=0.1, p1a=0.25, p0b=0.1, p1b=0.25)

dim <- nrow(design_space)
nobj <- 3
obj_names <- NULL
for(i in 1:nobj){
  obj_names <- c(obj_names, paste0("f",i))
}
  
objective <- function(x)
{
  c((x[1]+x[1]*x[3])/5, x[2], x[4])
}

DoE_num <- 50
dim <- nrow(design_space)
  
# Choose initial points
DoE <- data.frame(sobol(DoE_num, dim))
names(DoE) <- design_space$name
for(i in 1:dim){
  DoE[,i] <-  DoE[,i]*(design_space$up[i]-design_space$low[i]) + design_space$low[i]
}
DoE[,c("n_1", "k", "j")] <- round(DoE[,c("n_1", "k", "j")])
```

```{r, eval=F}
# Evaluate at initial points
N <- 100
DoE <- cbind(DoE, t(apply(DoE, 1, calc_rates3, h=h, N=N)))
names(DoE)[6:9] <- c("beta", "beta_var", "alpha", "alpha_var")
DoE$N <- N

proc.time() - ptm

models <- list()
for(i in 1:nrow(constraints)){
  name <- constraints$name[i]
  models <- append(models, km(~1, design=DoE[1:dim], response=DoE[,as.character(name)], 
                             noise.var=DoE[,paste0(as.character(name), "_var")]))
  names(models)[length(models)] <- as.character(name)
}

# Get the current set of Pareto solutions
b <- best(design_space, models, DoE)

ref <- objective(design_space$up)
DHs <- NULL

for(i in 1:150){
  opt <- psoptim(rep(NA, 5), exp_improve, lower=design_space$low, upper=design_space$up,
                 N=100, p_set=b, models=models, design_space=design_space, constraints=constraints,
                 control=list(vectorize = T))
  sol <- opt$par
  sol[c(1,2,4)] <- round(sol[c(1,2,4)])
  
  # Do the evaluation and add to the design
  y <- calc_rates3(sol, h, N=100)
  
  DoE <- rbind(DoE, c(sol, y, 100))
  
  # track the objective value at each step
  p_set2 <- as.matrix(b[,obj_names])
  current <- dominatedHypervolume(p_set2, ref)
  DHs <- c(DHs, dominatedHypervolume(p_set2, ref))
  
  models <- list()
  for(i in 1:nrow(constraints)){
    name <- constraints$name[i]
    models <- append(models, km(~1, design=DoE[1:dim], response=DoE[,as.character(name)], 
                               noise.var=DoE[,paste0(as.character(name), "_var")]))
    names(models)[length(models)] <- as.character(name)
  }

  b <- best(design_space, models, DoE)
}

proc.time() - ptm
# 156.2025

#saveRDS(DoE, "./data/ex3_single_run_DoE.Rda")
#saveRDS(DHs, "./data/ex3_single_run_DHs.Rda")
```


### Results

```{r}
DoE <- readRDS("./data/ex3_single_run_DoE.Rda")
DHs <- readRDS("./data/ex3_single_run_DHs.Rda")

models <- list()
for(i in 1:nrow(constraints)){
  name <- constraints$name[i]
  models <- append(models, km(~1, design=DoE[1:dim], response=DoE[,as.character(name)], 
                             noise.var=DoE[,paste0(as.character(name), "_var")]))
  names(models)[length(models)] <- as.character(name)
}

# Get the current set of Pareto solutions
b <- best(design_space, models, DoE)

ggplot(b, aes(n_1+n_1*r, k, size=j)) + geom_point() +
  theme_minimal() + xlab("Number of participants") + ylab("Number of therapists") +
  #scale_colour_gradientn(name="No. of doctors", colours=rainbow(2)) +
  scale_size(name="No. of doctors") +
  xlim(c(min(b$n_1 + b$n_1*b$r), max(b$n_1 + b$n_1*b$r))) +  ylim(c(min(b$k), max(b$k)))

#ggsave("./paper/figures/ex3_single_run.pdf", height=9, width=14, units="cm")
#ggsave("./paper/figures/ex3_single_run.eps", height=9, width=14, units="cm")

## Plot dominated hypervolumes over the search
df2 <- data.frame(it = 1:150, DH=DHs)
ggplot(df2, aes(it, DH)) + geom_point() + geom_line() + 
  theme_minimal() + ylab("Dominated hypervolume") + xlab("Iteration")

#ggsave("./paper/figures/ex3_traj.pdf", height=9, width=14, units="cm")
#ggsave("./paper/figures/ex3_traj.eps", height=9, width=14, units="cm")
```

```{r, eval=F}
tab <- b[,1:9]
new_eval <- NULL
for(i in 1:nrow(tab)){
  new_eval <- rbind(new_eval, calc_rates3(as.numeric(tab[i, 1:5]), h=h, N=10000))
}
tab <- cbind(tab, new_eval)
names(tab)[10:13] <- c("beta_2", "beta_var_2", "alpha_2", "alpha_var_2")

#saveRDS(tab, "./data/ex3_large_N.Rda")
```

```{r}
## Print the results in a table
tab <- readRDS("./data/ex3_large_N.Rda")

tab2 <- data.frame(n_1=tab$n_1, n=tab$n_1*(1+tab$r), k=tab$k, j=tab$j, a=tab$a,
                   beta=apply(tab[,6:7], 1, function(x) paste0(round(x[1], 2), " (", round(sqrt(x[2]), 3), ")")),
                   alpha=apply(tab[,8:9], 1, function(x) paste0(round(x[1], 2), " (", round(sqrt(x[2]), 3), ")")),
                   beta_2=apply(tab[,10:11], 1, function(x) paste0(round(x[1], 3), " (", round(sqrt(x[2]), 3), ")")),
                   
                   alpha_2=apply(tab[,12:13], 1, function(x) paste0(round(x[1], 3), " (", round(sqrt(x[2]), 3), ")"))
                   )
tab2 <- tab2[order(tab2$k),]
colnames(tab2) <- c("$n_1$", "$n$", "$k$", "$j$", "$a$",
                    "$\\beta$ (s.e.)", "$\\alpha$ (s.e.)", 
                    "$\\beta$ (s.e.)", "$\\alpha$ (s.e.)")
tab2

#  & & & & & \multicolumn{2}{c}{$N = 10^2$} & \multicolumn{2}{c}{$N = 50^4$} \\
# print(xtable(tab2, digits=c(rep(0,5), 2, rep(0,4))),
#       booktabs = T, include.rownames = F, 
#       sanitize.text.function = function(x) {x}, floating = F,
#       file = "./paper/tables/ex3_results.txt")
```

## Figures

```{r, eval=T}
# Plot an example optimisation problem, minimising clusters and patients, with
# a true Pareto front and an approximation front, with the dominated hypervolume
# shaded in.

pf <- data.frame(f1 = c(472, 472, 473, 474, 476, 478, 481, 483, 486, 492, 498,
           508, 518, 528, 536, 548, 560, 573, 595, 624, 658, 706, 782, 1150, 1200),
           f2 = c(30:7, 7))
pf$t <- "p"

as <- data.frame(f2=c(30, 24,20,12,10, 10),
                 f1=c(589, 589, 705, 810, 982, 1200))
as$t <- "a"

df <- rbind(pf, as)

pg <- data.frame(f1 = c(1200, 982,982, 810, 810, 705, 705, 589, 589, 1200),
                 f2 = c(10, 10, 12, 12, 20, 20, 24, 24, 30, 30))

ggplot(df, aes(f1, f2)) + geom_polygon(data=pg, alpha=0.1) + #fill="grey90") + 
  geom_step(aes(colour=t)) + 
  geom_point(data=df[!(df$f1==1200 | df$f2==30),], aes(colour=t)) +
  theme_minimal() + xlab("Number of participants") + ylab("Number of clusters") +
  scale_colour_manual(name= "",
                      values=c(colours[3], colours[4]), 
                      labels=c("Approximation set", "Pareto set")) #+
  #geom_point(data=data.frame(f1=1200, f2=30), shape=4, size=3, stroke=2) +
  #annotate("text", x = 900, y = 23, label = "H(A)")
  
# ggsave("./paper/figures/fake_pareto.pdf", width= 5, height = 3)
# ggsave("./paper/figures/fake_pareto.eps", width= 5, height = 3, device=cairo_ps)
# ggsave("./paper/figures/fake_pareto.png", width= 4, height = 2)
```

Plot of a GP constraint, annotating some expected improvemets at a couple of points
```{r}
get_power <- function(n)
{
  return(power.t.test(n=n, delta=0.3)$power)
}

df <- data.frame(n = c(10, 110, 260, 290))
df$f<- apply(df, 1, get_power)


model <- km(design = df[,1, drop=FALSE], response = df[,2])

x <- data.frame(n=seq(100, 300))

p <- predict.km(model, newdata=x, type="SK")

x$f <- p$mean
x$sd <- p$sd
x$min <- x$f - 1.96*x$sd
x$max <- x$f + 1.96*x$sd

get_p <- function(input)
{
  return(1-pnorm(0.8, input[2], input[3]))
}

probs <- cbind(x, V1=apply(x, 1, get_p))
probs$EFI <- (260-probs$n)*probs$V1/450 + 0.5
probs$EFI <- ifelse(probs$EFI < 0.5 , 0.5, probs$EFI)

# Optimal expected feasible improvement is at 190, with
# m = 0.8388252, sd = 3.464424e-02

get_n <- function(y, m, s, n)
{
  n + 1.2*dnorm(y, m, s)
}

m <- 0.8388252
s <- 3.464424e-02

x6 <- data.frame(f=c(seq(0,m,0.001), m))
x6$n <- apply(x6, 1, get_n, m=m, s=s, n=190)
x6 <- subset(x6, f>0.5)

x7 <- data.frame(f=seq(m,1,0.001))
x7$n <- apply(x7, 1, get_n, m=m, s=s, n=190)

# For paper
ggplot(x6, aes(n, f)) + geom_line(colour="#009E73", linetype=2) + geom_line(data=x7, colour="#009E73", linetype=2) + 
  geom_line(data=x, colour="#D55E00") + geom_ribbon(data=x, aes(ymin=min, ymax=max), alpha=0.1) + 
  #geom_line(data=x4, colour="red") + geom_line(data=x5, colour="red") +
  geom_point(data=subset(df, n>100)) +
  scale_y_continuous(limits = c(0.5, 1)) +
  geom_line(data=probs, aes(n, EFI), colour="#CC79A7", linetype=3) +
  theme_minimal() + ylab("Power") + xlab("Sample size")

#ggsave("./paper/figures/GP_example.pdf", width= 5, height = 3.3)
#ggsave("./paper/figures/GP_example.eps", width= 5, height = 3.3, device=cairo_ps)
```