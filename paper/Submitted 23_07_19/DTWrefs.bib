% Encoding: UTF-8

@ARTICLE{AHern2001,
  author = {A'Hern, R. P.},
  title = {Sample size tables for exact single-stage phase {II} designs},
  journal = {Statistics in Medicine},
  year = {2001},
  volume = {20},
  pages = {859--866},
  number = {6},
  doi = {10.1002/sim.721},
  file = {:Papers\\AHern01.pdf:PDF},
  issn = {1097-0258},
  owner = {meddwilb},
  publisher = {John Wiley \& Sons, Ltd.},
  timestamp = {2013.09.16},
  url = {http://dx.doi.org/10.1002/sim.721}
}

@ARTICLE{Abdul-Baki2009,
  author = {Abdul-Baki, H and El Hajj, II and Elzahabi, L and Azar, C and Aoun,
	E and Skoury, A and Chaar, H and Sharara, AI},
  title = {A randomized controlled trial of imipramine in patients with irritable
	bowel syndrome},
  journal = {World journal of gastroenterology : WJG},
  year = {2009},
  volume = {15},
  pages = {3636â€”3642},
  number = {29},
  month = {August},
  doi = {10.3748/wjg.15.3636},
  issn = {1007-9327},
  owner = {meddwilb},
  timestamp = {2014.06.11},
  url = {http://europepmc.org/abstract/MED/19653341}
}

@Article{Adcock1997,
  author    = {Adcock, C. J.},
  title     = {Sample size determination: a review},
  year      = {1997},
  volume    = {46},
  number    = {2},
  pages     = {261--283},
  issn      = {1467-9884},
  doi       = {10.1111/1467-9884.00082},
  url       = {http://dx.doi.org/10.1111/1467-9884.00082},
  abstract  = {This paper is concerned with methods of sample size determination.
	The approach is to cover a small number of simple problems, such
	as estimating the mean of a normal distribution or the slope in a
	regression equation, and to present some key techniques. The methods
	covered are in two groups: frequentist and Bayesian. Frequentist
	methods specify a null and alternative hypothesis for the parameter
	of interest and then find the sample size by controlling both size
	and power. These methods often need to use prior information but
	cannot allow for the uncertainty that is associated with it. By contrast,
	the Bayesian approach offers a wide variety of techniques, all of
	which offer the ability to deal with uncertainty associated with
	prior information.},
  groups    = {SSR},
  journal   = {Journal of the Royal Statistical Society: Series D (The Statistician)},
  keywords  = {Average coverage criterion, Average length criterion, Bayes factors, Bayesian methods, Binomial distribution, Coherence, Hypothesis testing, Maximum expected utility, McNemar's test, Multinomial distribution, Multivariate analysis, Normal distribution, Pivots, Regression, Sample size determination, Tolerance intervals, Worst outcome criterion},
  owner     = {meddwilb},
  publisher = {Blackwell Publishers Ltd},
  timestamp = {2014.10.21},
}

@ARTICLE{Ainsworth2013,
  author = {Ainsworth, Hannah and Shah, Sarwat and Ahmed, Faraz and Amos, Amanda
	and Cameron, Ian and Fairhurst, Caroline and King, Rebecca and Mir,
	Ghazala and Parrott, Steve and Sheikh, Aziz and Torgerson, David
	and Thomson, Heather and Siddiqi, Kamran},
  title = {Muslim communities learning about second-hand smoke (MCLASS): study
	protocol for a pilot cluster randomised controlled trial},
  journal = {Trials},
  year = {2013},
  volume = {14},
  pages = {295},
  number = {1},
  abstract = {BACKGROUND:In the UK, 40% of Bangladeshi and 29% of Pakistani men
	smoke cigarettes regularly compared to the national average of 24%.
	As a consequence, second-hand smoking is also widespread in their
	households which is a serious health hazard to non-smokers, especially
	children. Smoking restrictions in households can help reduce exposure
	to second-hand smoking. This is a pilot trial of 'Smoke Free Homes',
	an educational programme which has been adapted for use by Muslim
	faith leaders, in an attempt to find an innovative solution to encourage
	Pakistani- and Bangladeshi-origin communities to implement smoking
	restrictions in their homes. The primary objectives for this pilot
	trial are to establish the feasibility of conducting such an evaluation
	and provide information to inform the design of a future definitive
	study.METHODS/DESIGN:This is a pilot cluster randomised controlled
	trial of 'Smoke Free Homes', with an embedded preliminary health
	economic evaluation and a qualitative analysis. The trial will be
	carried out in around 14 Islamic religious settings. Equal randomisation
	will be employed to allocate each cluster to a trial arm. The intervention
	group will be offered the Smoke Free Homes package (Smoke Free Homes:
	a resource for Muslim religious teachers), trained in its use, and
	will subsequently implement the package in their religious settings.
	The remaining clusters will not be offered the package until the
	completion of the study and will form the control group. At each
	cluster, we aim to recruit around 50 households with at least one
	adult resident who smokes tobacco and at least one child or a non-smoking
	adult. Households will complete a household survey and a non-smoking
	individual will provide a saliva sample which will be tested for
	cotinine. All participant outcomes will be measured before and after
	the intervention period in both arms of the trial. In addition, a
	purposive sample of participants and religious leaders/teachers will
	take part in interviews and focus groups.DISCUSSION:The results of
	this pilot study will inform the protocol for a definitive trial.TRIAL
	REGISTRATION:Current Controlled Trials ISRCTN03035510},
  doi = {10.1186/1745-6215-14-295},
  issn = {1745-6215},
  owner = {meddwilb},
  pubmedid = {24034853},
  timestamp = {2014.08.01},
  url = {http://www.trialsjournal.com/content/14/1/295}
}

@ARTICLE{Alosh2014,
  author = {Alosh, Mohamed and Bretz, Frank and Huque, Mohammad},
  title = {Advanced multiplicity adjustment methods in clinical trials},
  journal = {Statistics in Medicine},
  year = {2014},
  volume = {33},
  pages = {693--713},
  number = {4},
  abstract = {During the last decade, many novel approaches for addressing multiplicity
	problems arising in clinical trials have been introduced in the literature.
	These approaches provide great flexibility in addressing given clinical
	trial objectives and yet maintain strong control of the familywise
	error rate. In this tutorial article, we review multiple testing
	strategies that are related to the following: (a) recycling local
	significance levels to test hierarchically ordered hypotheses; (b)
	adapting the significance level for testing a hypothesis to the findings
	of testing previous hypotheses within a given test sequence, also
	in view of certain consistency requirements; (c) grouping hypotheses
	into hierarchical families of hypotheses along with recycling the
	significance level between those families; and (d) graphical methods
	that permit repeated recycling of the significance level. These four
	different methodologies are related to each other, and we point out
	some connections as we describe and illustrate them. By contrasting
	the main features of these approaches, our objective is to help practicing
	statisticians to select an appropriate method for their applications.
	In this regard, we discuss how to apply some of these strategies
	to clinical trial settings and provide algorithms to calculate critical
	values and adjusted p-values for their use in practice. The methods
	are illustrated with several numerical examples. Copyright Â© 2013
	John Wiley & Sons, Ltd.},
  doi = {10.1002/sim.5974},
  issn = {1097-0258},
  keywords = {multiple testing, Î±-propagation, adaptive alpha, gatekeeping, graphical
	methods},
  owner = {meddwilb},
  timestamp = {2014.04.23},
  url = {http://dx.doi.org/10.1002/sim.5974}
}

@Article{Anisimov2011,
  author    = {Vladimir V. Anisimov},
  title     = {Statistical Modeling of Clinical Trials (Recruitment and Randomization)},
  year      = {2011},
  volume    = {40},
  number    = {19-20},
  month     = {oct},
  pages     = {3684--3699},
  doi       = {10.1080/03610926.2011.581189},
  url       = {http://dx.doi.org/10.1080/03610926.2011.581189},
  groups    = {Pilot/feasibility},
  journal   = {Communications in Statistics - Theory and Methods},
  owner     = {meddwilb},
  publisher = {Informa {UK} Limited},
  timestamp = {2016.08.10},
}

@Article{Anisimov2007,
  author    = {Vladimir V. Anisimov and Valerii V. Fedorov},
  title     = {Modelling, prediction and adaptive adjustment of recruitment in multicentre trials},
  year      = {2007},
  volume    = {26},
  number    = {27},
  pages     = {4958--4975},
  doi       = {10.1002/sim.2956},
  url       = {http://dx.doi.org/10.1002/sim.2956},
  groups    = {Pilot/feasibility},
  journal   = {Statist. Med.},
  owner     = {meddwilb},
  publisher = {Wiley-Blackwell},
  timestamp = {2016.08.10},
}

@Article{Arain2010,
  author    = {Arain, Mubashir and Campbell, Michael and Cooper, Cindy and Lancaster, Gillian},
  title     = {What is a pilot or feasibility study? A review of current practice and editorial policy},
  year      = {2010},
  volume    = {10},
  number    = {1},
  pages     = {67},
  issn      = {1471-2288},
  doi       = {10.1186/1471-2288-10-67},
  url       = {http://www.biomedcentral.com/1471-2288/10/67},
  abstract  = {BACKGROUND:In 2004, a review of pilot studies published in seven major
	medical journals during 2000-01 recommended that the statistical
	analysis of such studies should be either mainly descriptive or focus
	on sample size estimation, while results from hypothesis testing
	must be interpreted with caution. We revisited these journals to
	see whether the subsequent recommendations have changed the practice
	of reporting pilot studies. We also conducted a survey to identify
	the methodological components in registered research studies which
	are described as 'pilot' or 'feasibility' studies. We extended this
	survey to grant-awarding bodies and editors of medical journals to
	discover their policies regarding the function and reporting of pilot
	studies.METHODS:Papers from 2007-08 in seven medical journals were
	screened to retrieve published pilot studies. Reports of registered
	and completed studies on the UK Clinical Research Network (UKCRN)
	Portfolio database were retrieved and scrutinized. Guidance on the
	conduct and reporting of pilot studies was retrieved from the websites
	of three grant giving bodies and seven journal editors were canvassed.RESULTS:54
	pilot or feasibility studies published in 2007-8 were found, of which
	26 (48%) were pilot studies of interventions and the remainder feasibility
	studies. The majority incorporated hypothesis-testing (81%), a control
	arm (69%) and a randomization procedure (62%). Most (81%) pointed
	towards the need for further research. Only 8 out of 90 pilot studies
	identified by the earlier review led to subsequent main studies.
	Twelve studies which were interventional pilot/feasibility studies
	and which included testing of some component of the research process
	were identified through the UKCRN Portfolio database. There was no
	clear distinction in use of the terms 'pilot' and 'feasibility'.
	Five journal editors replied to our entreaty. In general they were
	loathe to publish studies described as 'pilot'.CONCLUSION:Pilot studies
	are still poorly reported, with inappropriate emphasis on hypothesis-testing.
	Authors should be aware of the different requirements of pilot studies,
	feasibility studies and main studies and report them appropriately.
	Authors should be explicit as to the purpose of a pilot study. The
	definitions of feasibility and pilot studies vary and we make proposals
	here to clarify terminology.},
  groups    = {Pilot/feasibility},
  journal   = {BMC Medical Research Methodology},
  owner     = {meddwilb},
  pubmedid  = {20637084},
  timestamp = {2013.09.17},
}

@Article{Arnold2009,
  author    = {Arnold, Donald M. and Burns, Karen E. A. and Adhikari, Neill K. J. and Kho, Michelle E. and Meade, Maureen O. and Cook, Deborah J. and for the McMaster Critical Care Interest Group},
  title     = {The design and interpretation of pilot trials in clinical research in critical care},
  year      = {2009},
  volume    = {37},
  number    = {1},
  pages     = {S69-S74},
  doi       = {10.1097/CCM.0b013e3181920e33},
  abstract  = {Background: Pilot trials are important to ensure that large randomized
	trials are rigorous, feasible, and economically justifiable. The
	objective of this review is to highlight the importance of randomized
	pilot trials and to describe key features of their design and interpretation
	using examples from critical care.
	
	
	Methods: We searched MEDLINE (1997–2007) and contacted experts to
	identify pilot randomized trials to exemplify and summarize their
	key methodologic features including objectives, sample size determination,
	outcomes, analysis, and reporting.
	
	
	Results: Pilot trials can have distinct and broad objectives. Investigators
	can predefine explicit criteria for determining their success. Surrogate
	outcome analyses are common in pilot trials, yet are usually underpowered
	to detect meaningful differences in clinically important end points
	and thus, should be cautiously interpreted. Pilot trials can facilitate
	successful conduct of large clinical trials by informing study design
	and streamlining protocol implementation.
	
	
	Recommendations: We recommend that investigators define suitable objectives,
	determine sample size estimates, and select outcomes that will address
	their specific pilot trial objectives. Clinical effects documented
	in pilot trials should be reported with caution to avoid undue enthusiasm
	or pessimism about unstable estimates. Further methodologic work
	is required to identify optimal pilot trial design, indexing, and
	reporting.},
  journal   = {Critical Care Medicine:},
  owner     = {meddwilb},
  timestamp = {2014.08.05},
}

@Article{Avery2014,
  author    = {Avery, Kerry and Metcalfe, Chris and Berrisford, Richard and Barham, C and Donovan, Jenny and Elliott, Jackie and Falk, Stephen and Goldin, Rob and Hanna, George and Hollowood, Andrew and Krysztopik, Richard and Noble, Sian and Sanders, Grant and Streets, Christopher and Titcomb, Dan and Wheatley, Tim and Blazeby, Jane},
  title     = {The feasibility of a randomized controlled trial of esophagectomy for esophageal cancer - the ROMIO (Randomized Oesophagectomy: Minimally Invasive or Open) study: protocol for a randomized controlled trial},
  journal   = {Trials},
  year      = {2014},
  volume    = {15},
  number    = {1},
  pages     = {200},
  issn      = {1745-6215},
  doi       = {10.1186/1745-6215-15-200},
  url       = {http://www.trialsjournal.com/content/15/1/200},
  abstract  = {BACKGROUND:There is a need for evidence of the clinical effectiveness
	of minimally invasive surgery for the treatment of esophageal cancer,
	but randomized controlled trials in surgery are often difficult to
	conduct. The ROMIO (Randomized Open or Minimally Invasive Oesophagectomy)
	study will establish the feasibility of a main trial which will examine
	the clinical and cost-effectiveness of minimally invasive and open
	surgical procedures for the treatment of esophageal cancer.METHODS/DESIGN:A
	pilot randomized controlled trial (RCT), in two centers (University
	Hospitals Bristol NHS Foundation Trust and Plymouth Hospitals NHS
	Trust) will examine numbers of incident and eligible patients who
	consent to participate in the ROMIO study. Interventions will include
	esophagectomy by: (1) open gastric mobilization and right thoracotomy,
	(2) laparoscopic gastric mobilization and right thoracotomy, and
	(3) totally minimally invasive surgery (in the Bristol center only).
	The primary outcomes of the feasibility study will be measures of
	recruitment, successful development of methods to monitor quality
	of surgery and fidelity to a surgical protocol, and development of
	a core outcome set to evaluate esophageal cancer surgery. The study
	will test patient-reported outcomes measures to assess recovery,
	methods to blind participants, assessments of surgical morbidity,
	and methods to capture cost and resource use. ROMIO will integrate
	methods to monitor and improve recruitment using audio recordings
	of consultations between recruiting surgeons, nurses, and patients
	to provide feedback for recruiting staff.DISCUSSION:The ROMIO study
	aims to establish efficient methods to undertake a main trial of
	minimally invasive surgery versus open surgery for esophageal cancer.TRIAL
	REGISTRATION:The pilot trial has Current Controlled Trials registration
	number ISRCTN59036820(25/02/2013) at http://www.controlled-trials.com
	webcite; the ROMIO trial record at that site gives a link to the
	original version of the study protocol.},
  owner     = {meddwilb},
  pubmedid  = {24888266},
  timestamp = {2014.08.01},
}

@ARTICLE{Baldwin2011,
  author = { Scott A. Baldwin and David M. Murray and William R. Shadish and
	Sherri L. Pals and Jason M. Holland and Jonathan S. Abramowitz and
	Gerhard Andersson and David C. Atkins and Per Carlbring and Kathleen
	M. Carroll and Andrew Christensen and Kari M. Eddington and Anke
	Ehlers and Daniel J. Feaster and Ger P. J. Keijsers and Ellen Koch
	and Willem Kuyken and Alfred Lange and Tania Lincoln and Robert S.
	Stephens and Steven Taylor and Chris Trepka and Jeanne Watson },
  title = {Intraclass Correlation Associated with Therapists: Estimates and
	Applications in Planning Psychotherapy Research},
  journal = {Cognitive Behaviour Therapy},
  year = {2011},
  volume = {40},
  pages = {15-33},
  number = {1},
  note = {PMID: 21337212},
  abstract = { It is essential that outcome research permit clear conclusions to
	be drawn about the efficacy of interventions. The common practice
	of nesting therapists within conditions can pose important methodological
	challenges that affect interpretation, particularly if the study
	is not powered to account for the nested design. An obstacle to the
	optimal design of these studies is the lack of data about the intraclass
	correlation coefficient (ICC), which measures the statistical dependencies
	introduced by nesting. To begin the development of a public database
	of ICC estimates, the authors investigated ICCs for a variety outcomes
	reported in 20 psychotherapy outcome studies. The magnitude of the
	495 ICC estimates varied widely across measures and studies. The
	authors provide recommendations regarding how to select and aggregate
	ICC estimates for power calculations and show how researchers can
	use ICC estimates to choose the number of patients and therapists
	that will optimize power. Attention to these recommendations will
	strengthen the validity of inferences drawn from psychotherapy studies
	that nest therapists within conditions. },
  doi = {10.1080/16506073.2010.520731},
  eprint = { http://dx.doi.org/10.1080/16506073.2010.520731 },
  owner = {meddwilb},
  timestamp = {2015.09.11},
  url = { http://dx.doi.org/10.1080/16506073.2010.520731 
}
}

@Article{Barkham2006,
  author    = {Barkham, Michael and Connell, Janice and Stiles, William B. and Miles, Jeremy N. V. and Margison, Frank and Evans, Chris and Mellor-Clark, John},
  title     = {Dose-effect relations and responsive regulation of treatment duration: The good enough level},
  year      = {2006},
  volume    = {74},
  number    = {1},
  pages     = {160-167},
  url       = {http://empower-daphne.psy.unipd.it/userfiles/file/pdf/Barkham%20M_%20-%202006.pdf},
  groups    = {Barkham},
  journal   = {Journal of Consulting and Clinical Psychology},
  owner     = {meddwilb},
  timestamp = {2015.01.23},
}

@Article{Barkham2002,
  author    = {M. Barkham and A. Rees and W.B. Stiles and G.E. Hardy and D.A. Shapiro},
  title     = {Dose-Effect Relations for Psychotherapy of Mild Depression: A Quasi-Experimental Comparison of Effects of 2, 8, and 16 Sessions},
  journal   = {Psychotherapy Research, 12:4, 463-474},
  year      = {2002},
  volume    = {12},
  number    = {4},
  pages     = {463-474},
  url       = {http://www.tandfonline.com/doi/pdf/10.1093/ptr/12.4.463},
  comment   = {Argues that the dose (number of sessions) of psychotherapy should
	be an independant variable. Contrasts with the usual practice of
	setting dose "natuaristically", i.e. observing progress and deciding
	to stop at some point.
	
	
	First study to systematically control the dose of psychotherapy in
	this manner.},
  groups    = {Barkham},
  owner     = {meddwilb},
  timestamp = {2015.01.23},
}

@Article{Barkham1996,
  author    = {Barkham, Michael and Rees, Anne and Stiles, William B. and Shapiro, David A. and Hardy, Gillian E. and Reynolds, Shirley},
  title     = {Dose–effect relations in time-limited psychotherapy for depression},
  year      = {1996},
  volume    = {64},
  number    = {5},
  pages     = {927-935},
  url       = {http://dx.doi.org/10.1037/0022-006X.64.5.927},
  groups    = {Barkham},
  journal   = {Journal of Consulting and Clinical Psychology},
  owner     = {meddwilb},
  timestamp = {2015.01.23},
}

@ARTICLE{Barkun2009,
  author = {Jeffrey S Barkun and Jeffrey K Aronson and Liane S Feldman and Guy
	J Maddern and Steven M Strasberg},
  title = {Evaluation and stages of surgical innovations},
  journal = {The Lancet},
  year = {2009},
  volume = {374},
  pages = {1089 - 1096},
  number = {9695},
  abstract = {Summary Surgical innovation is an important part of surgical practice.
	Its assessment is complex because of idiosyncrasies related to surgical
	practice, but necessary so that introduction and adoption of surgical
	innovations can derive from evidence-based principles rather than
	trial and error. A regulatory framework is also desirable to protect
	patients against the potential harms of any novel procedure. In this
	first of three Series papers on surgical innovation and evaluation,
	we propose a five-stage paradigm to describe the development of innovative
	surgical procedures. },
  doi = {http://dx.doi.org/10.1016/S0140-6736(09)61083-7},
  issn = {0140-6736},
  owner = {meddwilb},
  timestamp = {2013.11.21},
  url = {http://www.sciencedirect.com/science/article/pii/S0140673609610837}
}

@ARTICLE{Berger1994,
  author = {Berger, JamesO. and Moreno, ElÃ­as and Pericchi, LuisRaul and Bayarri,
	M.JesÃºs and Bernardo, JosÃ©M. and Cano, JuanA. and De la Horra,
	JuliÃ¡n and MartÃ­n, Jacinto and RÃ­os-InsÃºa, David and BetrÃ²,
	Bruno and Dasgupta, A. and Gustafson, Paul and Wasserman, Larry and
	Kadane, JosephB. and Srinivasan, Cid and Lavine, Michael and Oâ€™Hagan,
	Anthony and Polasek, Wolfgang and Robert, ChristianP. and Goutis,
	Constantinos and Ruggeri, Fabrizio and Salinetti, Gabriella and Sivaganesan,
	Siva},
  title = {An overview of robust Bayesian analysis},
  journal = {Test},
  year = {1994},
  volume = {3},
  pages = {5-124},
  number = {1},
  doi = {10.1007/BF02562676},
  issn = {1133-0686},
  language = {English},
  owner = {meddwilb},
  publisher = {Springer-Verlag},
  timestamp = {2014.06.16},
  url = {http://dx.doi.org/10.1007/BF02562676}
}

@BOOK{Berger1985,
  title = {Statistical Decision Theory and Bayesian Analysis},
  publisher = {Springer-Verlag},
  year = {1985},
  author = {James O. Berger},
  edition = {2nd},
  owner = {meddwilb},
  timestamp = {2013.11.19}
}

@ARTICLE{Berger2003,
  author = {Berger, James O.},
  title = {Could Fisher, Jeffreys and Neyman Have Agreed on Testing?},
  journal = {Statistical Science},
  year = {2003},
  volume = {18},
  pages = {1--32},
  number = {1},
  month = {02},
  ajournal = {Statist. Sci.},
  doi = {10.1214/ss/1056397485},
  owner = {meddwilb},
  publisher = {The Institute of Mathematical Statistics},
  timestamp = {2014.07.17},
  url = {http://dx.doi.org/10.1214/ss/1056397485}
}

@ARTICLE{Berry2004,
  author = {Berry, Donald A.},
  title = {Bayesian Statistics and the Efficiency and Ethics of Clinical Trials},
  journal = {Statistical Science},
  year = {2004},
  volume = {19},
  pages = {pp. 175-187},
  number = {1},
  abstract = {The Bayesian approach is being used increasingly in medical research.
	The flexibility of the Bayesian approach allows for building designs
	of clinical trials that have good properties of any desired sort.
	Examples include maximizing effective treatment of patients in the
	trial, maximizing information about the slope of a dose-response
	curve, minimizing costs, minimizing the number of patients treated,
	minimizing the length of the trial and combinations of these desiderata.
	They also include standard frequentist operating characteristics
	when these are important considerations. Posterior probabilities
	are updated via Bayes' theorem on the basis of accumulating data.
	These are used to effect modifications of the trial's course, including
	stopping accrual, extending accrual beyond that originally planned,
	dropping treatment arms, adding arms, etc. An important aspect of
	the approach I advocate is modeling the relationship between a trial's
	primary endpoint and early indications of patient performance-auxiliary
	endpoints. This has several highly desirable consequences. One is
	that it improves the efficiency of adaptive trials because information
	is available sooner than otherwise.},
  owner = {meddwilb},
  timestamp = {2014.01.14},
  url = {http://www.jstor.org/stable/4144381}
}

@Article{Bersimis2014,
  author    = {S. Bersimis and A. Sachlas and T. Papaioannou},
  title     = {Flexible designs for phase {II} comparative clinical trials involving two response variables},
  year      = {2014},
  volume    = {34},
  number    = {2},
  month     = {oct},
  pages     = {197--214},
  doi       = {10.1002/sim.6317},
  url       = {http://dx.doi.org/10.1002/sim.6317},
  groups    = {Multiple endpoints, Phase II},
  journal   = {Statist. Med.},
  owner     = {meddwilb},
  publisher = {Wiley-Blackwell},
  timestamp = {2016.07.25},
}

@Article{Billingham2012,
  author    = {Lucinda Billingham and Kinga Malottki and Neil Steven},
  title     = {Small sample sizes in clinical trials: a statistician’s perspective},
  year      = {2012},
  volume    = {2},
  pages     = {655-657},
  doi       = {10.4155/cli.12.62},
  journal   = {Clinical Investigation},
  owner     = {meddwilb},
  timestamp = {2014.06.13},
}

@Article{Billingham2013,
  author    = {Billingham, Sophie and Whitehead, Amy and Julious, Steven},
  title     = {An audit of sample sizes for pilot and feasibility trials being undertaken in the {United Kingdom registered in the United Kingdom Clinical Research Network database}},
  year      = {2013},
  volume    = {13},
  number    = {1},
  pages     = {104},
  issn      = {1471-2288},
  doi       = {10.1186/1471-2288-13-104},
  url       = {http://www.biomedcentral.com/1471-2288/13/104},
  abstract  = {BACKGROUND:There is little published guidance as to the sample size
	required for a pilot or feasibility trial despite the fact that a
	sample size justification is a key element in the design of a trial.
	A sample size justification should give the minimum number of participants
	needed in order to meet the objectives of the trial. This paper seeks
	to describe the target sample sizes set for pilot and feasibility
	randomised controlled trials, currently running within the United
	Kingdom.METHODS:Data were gathered from the United Kingdom Clinical
	Research Network (UKCRN) database using the search terms 'pilot'
	and 'feasibility'. From this search 513 studies were assessed for
	eligibility of which 79 met the inclusion criteria. Where the data
	summary on the UKCRN Database was incomplete, data were also gathered
	from: the International Standardised Randomised Controlled Trial
	Number (ISRCTN) register; the clinicaltrials.gov website and the
	website of the funders. For 62 of the trials, it was necessary to
	contact members of the research team by email to ensure completeness.RESULTS:Of
	the 79 trials analysed, 50 (63.3%) were labelled as pilot trials,
	25 (31.6%) feasibility and 14 were described as both pilot and feasibility
	trials. The majority had two arms (n=68, 86.1%) and the two most
	common endpoints were continuous (n=45, 57.0%) and dichotomous (n=31,
	39.2%). Pilot trials were found to have a smaller sample size per
	arm (median=30, range=8 to 114 participants) than feasibility trials
	(median=36, range=10 to 300 participants). By type of endpoint, across
	feasibility and pilot trials, the median sample size per arm was
	36 (range=10 to 300 participants) for trials with a dichotomous endpoint
	and 30 (range=8 to 114 participants) for trials with a continuous
	endpoint. Publicly funded pilot trials appear to be larger than industry
	funded pilot trials: median sample sizes of 33 (range=15 to 114 participants)
	and 25 (range=8 to 100 participants) respectively.CONCLUSION:All
	studies should have a sample size justification. Not all studies
	however need to have a sample size calculation. For pilot and feasibility
	trials, while a sample size justification is important, a formal
	sample size calculation may not be appropriate. The results in this
	paper describe the observed sample sizes in feasibility and pilot
	randomised controlled trials on the UKCRN Database.},
  groups    = {Pilot/feasibility},
  journal   = {BMC Medical Research Methodology},
  owner     = {meddwilb},
  pubmedid  = {23961782},
  timestamp = {2014.12.04},
}

@ARTICLE{Blair1986,
  author = {Blair, R. Clifford and Higgins, James J.},
  title = {Comment on `Statistical Power with Group Mean as the Unit of Analysis'},
  journal = {Journal of Educational and Behavioral Statistics},
  year = {1986},
  volume = {11},
  pages = {161-169},
  number = {2},
  abstract = {Barcikowski (1981) has provided tables for use in situations where
	unit (e.g., class or school) means are to be used as the unit of
	analysis. Apparently unrecognized by Barcikowski, and perhaps by
	the research community at large, is the fact that under the conditions
	specified for use of these tables (i.e., knowledge of the intraclass
	correlation), one would not generally wish to carry out analyses
	based on group means since a superior analytic strategy is available
	in this situation. This paper explicates this methodology.},
  doi = {10.3102/10769986011002161},
  eprint = {http://jeb.sagepub.com/content/11/2/161.full.pdf+html},
  owner = {meddwilb},
  timestamp = {2014.09.09},
  url = {http://jeb.sagepub.com/content/11/2/161.abstract}
}

@ARTICLE{Bland2004,
  author = {J. M. Bland},
  title = {Cluster randomised trials in the medical literature: two bibliometric
	surveys},
  journal = {BMC Medical Research Methodology},
  year = {2004},
  volume = {4},
  pages = {21},
  abstract = {Background
	
	Several reviews of published cluster randomised trials have reported
	that about half did not take clustering into account in the analysis,
	which was thus incorrect and potentially misleading. In this paper
	I ask whether cluster randomised trials are increasing in both number
	and quality of reporting.
	
	
	Methods
	
	Computer search for papers on cluster randomised trials since 1980,
	hand search of trial reports published in selected volumes of the
	British Medical Journal over 20 years.
	
	
	Results
	
	There has been a large increase in the numbers of methodological papers
	and of trial reports using the term 'cluster random' in recent years,
	with about equal numbers of each type of paper. The British Medical
	Journal contained more such reports than any other journal. In this
	journal there was a corresponding increase over time in the number
	of trials where subjects were randomised in clusters. In 2003 all
	reports showed awareness of the need to allow for clustering in the
	analysis. In 1993 and before clustering was ignored in most such
	trials.
	
	
	Conclusion
	
	Cluster trials are becoming more frequent and reporting is of higher
	quality. Perhaps statistician pressure works.},
  doi = {10.1186/1471-2288-4-21},
  owner = {meddwilb},
  timestamp = {2013.09.06}
}

@Article{Bo2009,
  author    = {Liefeng Bo and Cristian Sminchisescu},
  title     = {Twin {Gauss}ian Processes for Structured Prediction},
  year      = {2009},
  volume    = {87},
  number    = {1-2},
  month     = {feb},
  pages     = {28--52},
  doi       = {10.1007/s11263-008-0204-y},
  url       = {http://dx.doi.org/10.1007/s11263-008-0204-y},
  groups    = {Optimisation},
  journal   = {International Journal of Computer Vision},
  owner     = {meddwilb},
  publisher = {Springer Science $\mathplus$ Business Media},
  timestamp = {2016.07.14},
}

@ARTICLE{Bond2011,
  author = {Frank W. Bond and Steven C. Hayes and Ruth A. Baer and Kenneth M.
	Carpenter and Nigel Guenole and Holly K. Orcutt and Tom Waltz and
	Robert D. Zettle},
  title = {Preliminary Psychometric Properties of the Acceptance and Action
	Questionnaireâ€“II: A Revised Measure of Psychological Inflexibility
	and Experiential Avoidance},
  journal = {Behavior Therapy },
  year = {2011},
  volume = {42},
  pages = {676 - 688},
  number = {4},
  doi = {http://dx.doi.org/10.1016/j.beth.2011.03.007},
  issn = {0005-7894},
  keywords = {psychological flexibility},
  owner = {meddwilb},
  timestamp = {2014.02.27},
  url = {http://www.sciencedirect.com/science/article/pii/S0005789411000888}
}

@MANUAL{Bosker2003,
  title = {PINT: Estimating standard errors of regression coefficients in hierarchical
	linear models for power calculations},
  author = {Roel J. Bosker and Tom A.B. Snijders and Henk Guldemond},
  edition = {2.1},
  year = {2003},
  note = {Accesed 4th December 2014},
  owner = {meddwilb},
  timestamp = {2014.12.04},
  url = {http://www.stats.ox.ac.uk/~snijders/Pint21_UsersManual.pdf}
}

@ARTICLE{Boutron2008,
  author = {Boutron, Isabelle and Moher, David and Altman, Douglas G. and Schulz,
	Kenneth F. and Ravaud, Philippe},
  title = {Methods and Processes of the CONSORT Group: Example of an Extension
	for Trials Assessing Nonpharmacologic Treatments},
  journal = {Annals of Internal Medicine},
  year = {2008},
  volume = {148},
  pages = {W-60-W-66},
  number = {4},
  doi = {10.7326/0003-4819-148-4-200802190-00008-w1},
  owner = {meddwilb},
  timestamp = {2013.10.24},
  url = { + http://dx.doi.org/10.7326/0003-4819-148-4-200802190-00008-w1}
}

@ARTICLE{Bouwmeester2013,
  author = {Bouwmeester, W. and Moons, K. G. M. and Kappen, T. H. and van Klei,
	W. A. and Twisk, J. W. R. and Eijkemans, M. J. C. and Vergouwe, Y.},
  title = {Internal Validation of Risk Models in Clustered Data: A Comparison
	of Bootstrap Schemes},
  journal = {American Journal of Epidemiology},
  year = {2013},
  volume = {177},
  pages = {1209-1217},
  number = {11},
  abstract = {Internal validity of a risk model can be studied efficiently with
	bootstrapping to assess possible optimism in model performance. Assumptions
	of the regular bootstrap are violated when the development data are
	clustered. We compared alternative resampling schemes in clustered
	data for the estimation of optimism in model performance. A simulation
	study was conducted to compare regular resampling on only the patient
	level with resampling on only the cluster level and with resampling
	sequentially on both the cluster and patient levels (2-step approach).
	Optimism for the concordance index and calibration slope was estimated.
	Resampling of only patients or only clusters showed accurate estimates
	of optimism in model performance. The 2-step approach overestimated
	the optimism in model performance. If the number of centers or intraclass
	correlation coefficient was high, resampling of clusters showed more
	accurate estimates than resampling of patients. The 3 bootstrap schemes
	also were applied to empirical data that were clustered. The results
	presented in this paper support the use of resampling on only the
	clusters for estimation of optimism in model performance when data
	are clustered.},
  doi = {10.1093/aje/kws396},
  eprint = {http://aje.oxfordjournals.org/content/177/11/1209.full.pdf+html},
  owner = {meddwilb},
  timestamp = {2015.03.05},
  url = {http://aje.oxfordjournals.org/content/177/11/1209.abstract}
}

@ARTICLE{Box1954,
  author = {G. E. P. Box},
  title = {Some Theorems on Quadratic Forms Applied in the Study of Analysis
	of Variance Problems, I. Effect of Inequality of Variance in the
	One-Way Classification},
  journal = {The Annals of Mathematical Statistics},
  year = {1954},
  volume = {25},
  pages = {290-302},
  number = {2},
  abstract = {This is the first of two papers describing a study of the effect of
	departures from assumptions, other than normality, on the null-distribution
	of the F-statistic in the analysis of variance. In this paper, certain
	theorems required in the study and concerning the distribution of
	quadratic forms in multi-normally distributed variables are first
	enunciated and simple approximations tested numerically. The results
	are then applied to determine the effect of group-to-group inequality
	of variance in the one-way classification. It appears that if the
	groups are equal, moderate inequality of variance does not seriously
	affect the test. However, with unequal groups, much larger discrepancies
	appear. In a second paper, similar methods are used to determine
	the effect of inequality of variance and serial correlation between
	errors in the two-way classification.},
  owner = {meddwilb},
  timestamp = {2014.04.10},
  url = {http://www.jstor.org/stable/2236731}
}

@Article{Brant1992,
  author    = {Brant, Larry J. and Duncan, David B. and Dixon, Dennis O.},
  title     = {k-ratio t tests for multiple comparisons involving several treatments and a control},
  year      = {1992},
  volume    = {11},
  number    = {7},
  pages     = {863--873},
  issn      = {1097-0258},
  doi       = {10.1002/sim.4780110704},
  url       = {http://dx.doi.org/10.1002/sim.4780110704},
  abstract  = {We consider the problem of simultaneously comparing several treatment
	means with a control mean and also with one another. Following an
	elementary decision-theoretic Bayesian approach requiring the choice
	of a type-I to type-II error-seriousness ratio k, a posteriori t
	tests are derived for testing both treatment versus control (TvC)
	and treatment versus treatment (TvT) differences. These k-ratio t
	tests are strictly comparisonwise in nature. That is, the test applied
	to any TvC or TvT difference d, depends in no way at all on whether
	the other differences are being tested. The test for d, however,
	does depend on the sizes of the other differences through tG, the
	standardized average of the observed TvC differences, and through
	FT, the observed between-treatments F ratio. From these adaptive
	dependences on tG and FT, the critical t values can be large or small,
	thus avoiding the intuitive objections of under- or over-conservatism
	in classical comparisonwise or experimentwise level testing rules.},
  groups    = {Multiple endpoints},
  journal   = {Statistics in Medicine},
  owner     = {meddwilb},
  publisher = {Wiley Subscription Services, Inc., A Wiley Company},
  timestamp = {2014.04.24},
}

@ARTICLE{Braun2001,
  author = {Braun, Thomas M and Feng, Ziding},
  title = {Optimal Permutation Tests for the Analysis of Group Randomized Trials},
  journal = {Journal of the American Statistical Association},
  year = {2001},
  volume = {96},
  pages = {1424-1432},
  number = {456},
  doi = {10.1198/016214501753382336},
  eprint = {http://www.tandfonline.com/doi/pdf/10.1198/016214501753382336},
  owner = {meddwilb},
  timestamp = {2013.09.11},
  url = {http://www.tandfonline.com/doi/abs/10.1198/016214501753382336}
}

@Article{Breukelen2012,
  author    = {Gerard J.P. van Breukelen and Math J.J.M. Candel},
  title     = {Calculating sample sizes for cluster randomized trials: We can keep it simple and efficient!},
  year      = {2012},
  volume    = {65},
  number    = {11},
  pages     = {1212 - 1218},
  issn      = {0895-4356},
  doi       = {http://dx.doi.org/10.1016/j.jclinepi.2012.06.002},
  url       = {http://www.sciencedirect.com/science/article/pii/S0895435612001692},
  abstract  = {Objective Simple guidelines for calculating efficient sample sizes in cluster randomized trials with unknown intraclass correlation (ICC) and varying cluster sizes. Methods A simple equation is given for the optimal number of clusters and sample size per cluster. Here, optimal means maximizing power for a given budget or minimizing total cost for a given power. The problems of cluster size variation and specification of the \{ICC\} of the outcome are solved in a simple yet efficient way. Results The optimal number of clusters goes up, and the optimal sample size per cluster goes down as the \{ICC\} goes up or as the cluster-to-person cost ratio goes down. The available budget, desired power, and effect size only affect the number of clusters and not the sample size per cluster, which is between 7 and 70 for a wide range of cost ratios and ICCs. Power loss because of cluster size variation is compensated by sampling 10% more clusters. The optimal design for the \{ICC\} halfway the range of realistic \{ICC\} values is a good choice for the first stage of a two-stage design. The second stage is needed only if the first stage shows the \{ICC\} to be higher than assumed. Conclusion Efficient sample sizes for cluster randomized trials are easily computed, provided the cost per cluster and cost per person are specified. },
  groups    = {multi-D / sim SS},
  journal   = {Journal of Clinical Epidemiology},
  keywords  = {Cluster randomized trials},
  owner     = {meddwilb},
  timestamp = {2016.02.16},
}

@BOOK{Brown2014,
  title = {A Practical Guide to Designing Phase {II} Trials in Oncology},
  publisher = {John Wiley \& Sons, Ltd.},
  year = {2014},
  author = {Brown, S.R. and Gregory, W.M. and Twelves, C. and Brown, J.},
  owner = {meddwilb},
  timestamp = {2014.08.05}
}

@ARTICLE{Brown2011,
  author = {Brown, S R and Gregory, W M and Twelves, C J and Buyse, M and Collinson,
	F and Parmar, M and Seymour, M T and Brown, J M},
  title = {Designing phase {II} trials in cancer: a systematic review and guidance},
  journal = {Br J Cancer},
  year = {2011},
  volume = {105},
  pages = {194--199},
  number = {2},
  month = jul,
  issn = {0007-0920},
  owner = {meddwilb},
  publisher = {Cancer Research UK},
  timestamp = {2014.08.05},
  url = {http://dx.doi.org/10.1038/bjc.2011.235}
}

@Article{Browne1995,
  author    = {Browne, Richard H.},
  title     = {On the use of a pilot sample for sample size determination},
  year      = {1995},
  volume    = {14},
  number    = {17},
  pages     = {1933--1940},
  issn      = {1097-0258},
  doi       = {10.1002/sim.4780141709},
  url       = {http://dx.doi.org/10.1002/sim.4780141709},
  abstract  = {To compute the sample size needed to achieve the planned power for
	a t-test, one needs an estimate of the population standard deviation
	Î´. If one uses the sample standard deviation from a small pilot
	study as an estimate of Î´, it is quite likely that the actual power
	for the planned study will be less than the planned power. Monte
	Carlo simulations indicate that using a 100(1 âˆ’ Î³) per cent upper
	one-sided confidence limit on Î´ will provide a sample size sufficient
	to achieve the planned power in at least 100(1 âˆ’ Î³) per cent of
	such trials.},
  groups    = {SSR, Pilot/feasibility},
  journal   = {Statistics in Medicine},
  owner     = {meddwilb},
  publisher = {Wiley Subscription Services, Inc., A Wiley Company},
  timestamp = {2014.08.18},
}

@ARTICLE{Browne2006,
  author = {Browne, William J. and Draper, David},
  title = {A comparison of Bayesian and likelihood-based methods for fitting
	multilevel models},
  journal = {Bayesian Analysis},
  year = {2006},
  volume = {1},
  pages = {473--514},
  number = {3},
  month = {09},
  ajournal = {Bayesian Anal.},
  doi = {10.1214/06-BA117},
  owner = {meddwilb},
  publisher = {International Society for Bayesian Analysis},
  timestamp = {2014.11.04},
  url = {http://dx.doi.org/10.1214/06-BA117}
}

@ARTICLE{Browne2001,
  author = {Browne, William J and Goldstein, Harvey and Rasbash, Jon},
  title = {Multiple membership multiple classification ({MMMC}) models},
  journal = {Statistical Modelling},
  year = {2001},
  volume = {1},
  pages = {103-124},
  number = {2},
  abstract = {In the social and other sciences many data are collected with a known
	but complex underlying structure. Over the past two decades there
	has been an increase in the use of multilevel modelling techniques
	that account for nested data structures. Often however the underlying
	data structures are more complex and cannot be fitted into a nested
	structure. First, there are cross-classified models where the classifications
	in the data are not nested. Secondly, we consider multiple membership
	models where an observation does not belong simply to one member
	of a classification. These two extensions when combined allow us
	to fit models to a large array of underlying structures. Existing
	frequentist modelling approaches to fitting such data have some important
	computational limitations. In this paper we consider ways of overcoming
	such limitations using Bayesian methods, since Bayesian model fitting
	is easily accomplished using Monte Carlo Markov chain (MCMC) techniques.
	In examples where we have been able to make direct comparisons, Bayesian
	methods in conjunction with suitable â€˜diffuseâ€™ prior distributions
	lead to similar inferences to existing frequentist techniques. In
	this paper we illustrate our techniques with examples in the fields
	of education, veterinary epidemiology, demography, and public health
	illustrating the diversity of models that fit into our framework.},
  doi = {10.1177/1471082X0100100202},
  eprint = {http://smj.sagepub.com/content/1/2/103.full.pdf+html},
  owner = {meddwilb},
  timestamp = {2014.07.21},
  url = {http://smj.sagepub.com/content/1/2/103.abstract}
}

@Manual{Browne2009,
  author    = {William J Browne and Mousa Golalizadeh Lahi and Richard MA Parker},
  title     = {A Guide to Sample Size Calculations for Random Effect Models via Simulation and the MLPowSim Software Package},
  year      = {2009},
  url       = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.408.6314&rep=rep1&type=pdf},
  owner     = {meddwilb},
  timestamp = {2014.11.05},
}

@ARTICLE{Brunier1994,
  author = {Brunier, Hazel C. and Whitehead, John},
  title = {Sample sizes for phase {II} clinical trials derived from Bayesian
	decision theory},
  journal = {Statistics in Medicine},
  year = {1994},
  volume = {13},
  pages = {2493--2502},
  number = {23-24},
  doi = {10.1002/sim.4780132312},
  issn = {1097-0258},
  owner = {meddwilb},
  publisher = {Wiley Subscription Services, Inc., A Wiley Company},
  timestamp = {2013.10.17},
  url = {http://dx.doi.org/10.1002/sim.4780132312}
}

@ARTICLE{Brutti2008,
  author = {Brutti, Pierpaolo and De Santis, Fulvio and Gubbiotti, Stefania},
  title = {Robust Bayesian sample size determination in clinical trials},
  journal = {Statistics in Medicine},
  year = {2008},
  volume = {27},
  pages = {2290--2306},
  number = {13},
  abstract = {This article deals with determination of a sample size that guarantees
	the success of a trial. We follow a Bayesian approach and we say
	an experiment is successful if it yields a large posterior probability
	that an unknown parameter of interest (an unknown treatment effect
	or an effects-difference) is greater than a chosen threshold. In
	this context, a straightforward sample size criterion is to select
	the minimal number of observations so that the predictive probability
	of a successful trial is sufficiently large. In the paper we address
	the most typical criticism to Bayesian methodsâ€”their sensitivity
	to prior assumptionsâ€”by proposing a robust version of this sample
	size criterion. Specifically, instead of a single distribution, we
	consider a class of plausible priors for the parameter of interest.
	Robust sample sizes are then selected by looking at the predictive
	distribution of the lower bound of the posterior probability that
	the unknown parameter is greater than a chosen threshold. For their
	flexibility and mathematical tractability, we consider classes of
	Îµ-contamination priors. As specific applications we consider sample
	size determination for a Phase III trial. Copyright Â© 2008 John
	Wiley & Sons, Ltd.},
  doi = {10.1002/sim.3175},
  issn = {1097-0258},
  keywords = {analysis and design priors, Bayesian power, Bayesian robustness, conditional
	and predictive power, evidence, Îµ-contaminated priors, Phase II
	and Phase III clinical trials, sample size determination},
  owner = {meddwilb},
  publisher = {John Wiley \& Sons, Ltd.},
  timestamp = {2014.10.27},
  url = {http://dx.doi.org/10.1002/sim.3175}
}

@ARTICLE{Brutti2014,
  author = {Brutti, Pierpaolo and De Santis, Fulvio and Gubbiotti, Stefania},
  title = {Bayesian-frequentist sample size determination: a game of two priors},
  journal = {METRON},
  year = {2014},
  volume = {72},
  pages = {133-151},
  number = {2},
  doi = {10.1007/s40300-014-0043-2},
  file = {:P\:\\CTRU\\Projects\\Health_Sciences\\Methodology\\2013 NIHR Research Methods Fellowship\\Literature\\Papers\\Brutti2014.pdf:PDF},
  issn = {0026-1424},
  keywords = {Clinical trials; Sample size; Simulation-based approach; Two priors;
	Robust methods},
  language = {English},
  owner = {meddwilb},
  publisher = {Springer Milan},
  timestamp = {2014.12.10},
  url = {http://dx.doi.org/10.1007/s40300-014-0043-2}
}

@Article{Bryant1995,
  author    = {Bryant, John and Day, Roger},
  title     = {Incorporating Toxicity Considerations Into the Design of Two-Stage Phase {II} Clinical Trials},
  journal   = {Biometrics},
  year      = {1995},
  volume    = {51},
  number    = {4},
  pages     = {1372-1383},
  url       = {http://www.jstor.org/stable/2533268},
  abstract  = {Phase II study designs are proposed that evaluate both clinical response
	and toxicity, and that are similar in structure to Simon's two-stage
	designs. Sample sizes and decision criteria are chosen to minimize
	the maximum expected accrual, given that the treatment is unacceptable
	either in terms of clinical response or toxicity. This is achieved
	subject to control of error rates, either uniformly over all possible
	correlation structures linking response and toxicity, or alternatively,
	under an assumption of independence between response and toxicity.
	In the latter case, bounds on the error rates show that effective
	control is still uniformly achieved even if the independence assumption
	is relaxed.},
  comment   = {Two binary outcomes (e.g. response and toxicity), two stages, at each stage we proceed if \emph{both} endpoints exceed critical values.  

Operating characteristics are: two type I error rates (proceeding past stage 2 under 01 and 10); one type II error rate (not proceeding past stage 2 under 11).

Criteria to be minimised is the maximum of the expected number of patients accrued under either 01 or 10.},
  groups    = {Multiple endpoints},
  owner     = {meddwilb},
  timestamp = {2014.01.22},
}

@ARTICLE{Bryant2014,
  author = {Bryant, M and Ashton, L and Brown, J and Jebb, S and Wright, J and
	Roberts, K and Nixon, J},
  title = {Systematic review to identify and appraise outcome measures used
	to evaluate childhood obesity treatment interventions {(CoOR)}: evidence
	of purpose, application, validity, reliability and sensitivity},
  journal = {Health Technol Assess},
  year = {2014},
  volume = {18},
  pages = {--},
  number = {51},
  abstract = {Background: Lack of uniformity in outcome measures used in evaluations
	of childhood obesity treatment interventions can impede the ability
	to assess effectiveness and limits comparisons across trials. Lack
	of uniformity in outcome measures used in evaluations of childhood
	obesity treatment interventions can impede the ability to assess
	effectiveness and limits comparisons across trials. Objective: To
	identify and appraise outcome measures to produce a framework of
	recommended measures for use in evaluations of childhood obesity
	treatment interventions. To identify and appraise outcome measures
	to produce a framework of recommended measures for use in evaluations
	of childhood obesity treatment interventions. Data sources: Eleven
	electronic databases were searched between August and December 2011,
	including MEDLINE; MEDLINE In-Process and Other Non-Indexed Citations;
	EMBASE; PsycINFO; Health Management Information Consortium (HMIC);
	Allied and Complementary Medicine Database (AMED); Global Health,
	Maternity and Infant Care (all Ovid); Cumulative Index to Nursing
	and Allied Health Literature (CINAHL) (EBSCOhost); Science Citation
	Index (SCI) [Web of Science (WoS)]; and The Cochrane Library (Wiley)
	- from the date of inception, with no language restrictions. This
	was supported by review of relevant grey literature and trial databases.
	Eleven electronic databases were searched between August and December
	2011, including MEDLINE; MEDLINE In-Process and Other Non-Indexed
	Citations; EMBASE; PsycINFO; Health Management Information Consortium
	(HMIC); Allied and Complementary Medicine Database (AMED); Global
	Health, Maternity and Infant Care (all Ovid); Cumulative Index to
	Nursing and Allied Health Literature (CINAHL) (EBSCOhost); Science
	Citation Index (SCI) [Web of Science (WoS)]; and The Cochrane Library
	(Wiley) - from the date of inception, with no language restrictions.
	This was supported by review of relevant grey literature and trial
	databases. Review methods: Two searches were conducted to identify
	(1) outcome measures and corresponding citations used in published
	childhood obesity treatment evaluations and (2) manuscripts describing
	the development and/or evaluation of the outcome measures used in
	the childhood intervention obesity evaluations. Search 1 search strategy
	(review of trials) was modelled on elements of a review by Luttikhuis
	et al. (Oude Luttikhuis H, Baur L, Jansen H, Shrewsbury VA, O'Malley
	C, Stolk RP, et al. Interventions for treating obesity in children.
	Cochrane Database Syst Rev 2009;1:CD001872). Search 2 strategy (methodology
	papers) was built on Terwee et al.'s search filter (Terwee CB, Jansma
	EP, Riphagen II, de Vet HCW. Development of a methodological PubMed
	search filter for finding studies on measurement properties of measurement
	instruments. Qual Life Res 2009;18:1115-23). Eligible papers were
	appraised for quality initially by the internal project team. This
	was followed by an external appraisal by expert collaborators in
	order to agree which outcome measures should be recommended for the
	Childhood obesity Outcomes Review (CoOR) outcome measures framework.
	Two searches were conducted to identify (1) outcome measures and
	corresponding citations used in published childhood obesity treatment
	evaluations and (2) manuscripts describing the development and/or
	evaluation of the outcome measures used in the childhood intervention
	obesity evaluations. Search 1 search strategy (review of trials)
	was modelled on elements of a review by Luttikhuis et al. (Oude Luttikhuis
	H, Baur L, Jansen H, Shrewsbury VA, O'Malley C, Stolk RP, et al.
	Interventions for treating obesity in children. Cochrane Database
	Syst Rev 2009;1:CD001872). Search 2 strategy (methodology papers)
	was built on Terwee et al.'s search filter (Terwee CB, Jansma EP,
	Riphagen II, de Vet HCW. Development of a methodological PubMed search
	filter for finding studies on measurement properties of measurement
	instruments. Qual Life Res 2009;18:1115-23). Eligible papers were
	appraised for quality initially by the internal project team. This
	was followed by an external appraisal by expert collaborators in
	order to agree which outcome measures should be recommended for the
	Childhood obesity Outcomes Review (CoOR) outcome measures framework.
	Results: Three hundred and seventy-nine manuscripts describing 180
	outcome measures met eligibility criteria. Appraisal of these resulted
	in the recommendation of 36 measures for the CoOR outcome measures
	framework. Recommended primary outcome measures were body mass index
	(BMI) and dual-energy X-ray absorptiometry (DXA). Experts did not
	advocate any self-reported measures where objective measurement was
	possible (e.g. physical activity). Physiological outcomes hold potential
	to be primary outcomes, as they are indicators of cardiovascular
	health, but without evidence of what constitutes a minimally importance
	difference they have remained as secondary outcomes (although the
	corresponding lack of evidence for BMI and DXA is acknowledged).
	No preference-based quality-of-life measures were identified that
	would enable economic evaluation via calculation of quality-adjusted
	life-years. Few measures reported evaluating responsiveness. Three
	hundred and seventy-nine manuscripts describing 180 outcome measures
	met eligibility criteria. Appraisal of these resulted in the recommendation
	of 36 measures for the CoOR outcome measures framework. Recommended
	primary outcome measures were body mass index (BMI) and dual-energy
	X-ray absorptiometry (DXA). Experts did not advocate any self-reported
	measures where objective measurement was possible (e.g. physical
	activity). Physiological outcomes hold potential to be primary outcomes,
	as they are indicators of cardiovascular health, but without evidence
	of what constitutes a minimally importance difference they have remained
	as secondary outcomes (although the corresponding lack of evidence
	for BMI and DXA is acknowledged). No preference-based quality-of-life
	measures were identified that would enable economic evaluation via
	calculation of quality-adjusted life-years. Few measures reported
	evaluating responsiveness. Limitations: Proposed recommended measures
	are fit for use as outcome measures within studies that evaluate
	childhood obesity treatment evaluations specifically. These may or
	may not be suitable for other study designs, and some excluded measures
	may be more suitable in other study designs. Proposed recommended
	measures are fit for use as outcome measures within studies that
	evaluate childhood obesity treatment evaluations specifically. These
	may or may not be suitable for other study designs, and some excluded
	measures may be more suitable in other study designs. Conclusions:
	The CoOR outcome measures framework provides clear guidance of recommended
	primary and secondary outcome measures. This will enhance comparability
	between treatment evaluations and ensure that appropriate measures
	are being used. Where possible, future work should focus on modification
	and evaluation of existing measures rather than development of tools
	de nova. In addition, it is recommended that a similar outcome measures
	framework is produced to support evaluation of adult obesity programmes.
	The CoOR outcome measures framework provides clear guidance of recommended
	primary and secondary outcome measures. This will enhance comparability
	between treatment evaluations and ensure that appropriate measures
	are being used. Where possible, future work should focus on modification
	and evaluation of existing measures rather than development of tools
	de nova. In addition, it is recommended that a similar outcome measures
	framework is produced to support evaluation of adult obesity programmes.
	Funding: The National Institute for Health Research Health Technology
	Assessment programme. The National Institute for Health Research
	Health Technology Assessment programme.},
  owner = {meddwilb},
  timestamp = {2014.10.09},
  url = {http://journalslibrary.nihr.ac.uk/hta/hta18510}
}

@ARTICLE{Bryant2014a,
  author = {Bryant, M and Ashton, L and Nixon, J and Wright, J and Jebb, S and
	Roberts, K and Brown, J},
  title = {The use and reporting of primary and secondary outcome measures in
	trials evaluating childhood obesity treatment interventions},
  journal = {Clinical Trials},
  year = {2014},
  note = {In press},
  owner = {meddwilb},
  timestamp = {2014.10.09}
}

@ARTICLE{Bryant2014b,
  author = {Bryant, M and Santorelli, G and Fairley, L and Petherick, E S and
	Bhopal, R and Lawlor, D A and Tilling, K and Howe, L D and Farrar,
	D and Cameron, N and Mohammed, M and Wright, J and the Born in Bradford
	Childhood Obesity Scientific Group},
  title = {Agreement between routine and research measurement of infant height
	and weight},
  journal = {Archives of Disease in Childhood},
  year = {2014},
  abstract = {In many countries, routine data relating to growth of infants are
	collected as a means of tracking health and illness up to school
	age. These have potential to be used in research. For health monitoring
	and research, data should be accurate and reliable. This study aimed
	to determine the agreement between length/height and weight measurements
	from routine infant records and researcher-collected data.Methods
	Height/length and weight at ages 6, 12 and 24â€…months from the longitudinal
	UK birth cohort (born in Bradford; n=836â€“1280) were compared with
	routine data collected by health visitors within 2â€…months of the
	research data (n=104â€“573 for different comparisons). Data were
	age adjusted and compared using Bland Altman plots.Results There
	was agreement between data sources, albeit weaker for height than
	for weight. Routine data tended to underestimate length/height at
	6â€…months (0.5â€…cm (95% CI âˆ’4.0 to 4.9)) and overestimate it
	at 12 (âˆ’0.3â€…cm (95% CI âˆ’0.5 to 4.0)) and 24â€…months (0.3â€…cm
	(95% CI âˆ’4.0 to 3.4)). Routine data slightly overestimated weight
	at all three ages (range âˆ’0.04â€…kg (95% CI âˆ’1.2 to 0.9) to âˆ’0.04
	(95% CI âˆ’0.7 to 0.6)). Limits of agreement were wide, particularly
	for height. Differences were generally random, although routine data
	tended to underestimate length in taller infants and underestimate
	weight in lighter infants.Conclusions Routine data can provide an
	accurate and feasible method of data collection for research, though
	wide limits of agreement between data sources may be observed. Differences
	could be due to methodological issues; but may relate to variability
	in clinical practice. Continued provision of appropriate training
	and assessment is essential for health professionals responsible
	for collecting routine data.},
  doi = {10.1136/archdischild-2014-305970},
  eprint = {http://adc.bmj.com/content/early/2014/09/29/archdischild-2014-305970.full.pdf+html},
  owner = {meddwilb},
  timestamp = {2014.10.09},
  url = {http://adc.bmj.com/content/early/2014/09/29/archdischild-2014-305970.abstract}
}

@ARTICLE{Bugge2013,
  author = {Bugge, Carol and Williams, Brian and Hagen, Suzanne and Logan, Janet
	and Glazener, Cathryn and Pringle, Stewart and Sinclair, Lesley},
  title = {A process for Decision-making after Pilot and feasibility Trials
	({ADePT}): development following a feasibility study of a complex
	intervention for pelvic organ prolapse},
  journal = {Trials},
  year = {2013},
  volume = {14},
  pages = {353},
  number = {1},
  abstract = {BACKGROUND:Current Medical Research Council (MRC) guidance on complex
	interventions advocates pilot trials and feasibility studies as part
	of a phased approach to the development, testing, and evaluation
	of healthcare interventions. In this paper we discuss the results
	of a recent feasibility study and pilot trial for a randomized controlled
	trial (RCT) of pelvic floor muscle training for prolapse (ClinicalTrials.gov:
	NCT01136889). The ways in which researchers decide to respond to
	the results of feasibility work may have significant repercussions
	for both the nature and degree of tension between internal and external
	validity in a definitive trial.METHODS:We used methodological issues
	to classify and analyze the problems that arose in the feasibility
	study. Four centers participated with the aim of randomizing 50 women.
	Women were eligible if they had prolapse of any type, of stage I
	to IV, and had a pessary successfully fitted. Postal questionnaires
	were administered at baseline, 6 months, and 7 months post-randomization.
	After identifying problems arising within the pilot study we then
	sought to locate potential solutions that might minimize the trade-off
	between a subsequent explanatory versus pragmatic trial.RESULTS:The
	feasibility study pointed to significant potential problems in relation
	to participant recruitment, features of the intervention, acceptability
	of the intervention to participants, and outcome measurement. Finding
	minimal evidence to support our decision-making regarding the transition
	from feasibility work to a trial, we developed a systematic process
	(A process for Decision-making after Pilot and feasibility Trials
	(ADePT)) which we subsequently used as a guide. The process sought
	to: 1) encourage the systematic identification and appraisal of problems
	and potential solutions; 2) improve the transparency of decision-making
	processes; and 3) reveal the tensions that exist between pragmatic
	and explanatory choices.CONCLUSIONS:We have developed a process that
	may aid researchers in their attempt to identify the most appropriate
	solutions to problems identified within future pilot and feasibility
	RCTs. The process includes three key steps: a decision about the
	type of problem, the identification of all solutions (whether addressed
	within the intervention, trial design or clinical context), and a
	systematic appraisal of these solutions.},
  doi = {10.1186/1745-6215-14-353},
  issn = {1745-6215},
  owner = {meddwilb},
  timestamp = {2013.11.26},
  url = {http://www.trialsjournal.com/content/14/1/353}
}

@Article{Burton2006,
  author    = {Burton, Andrea and Altman, Douglas G. and Royston, Patrick and Holder, Roger L.},
  title     = {The design of simulation studies in medical statistics},
  year      = {2006},
  volume    = {25},
  number    = {24},
  pages     = {4279--4292},
  issn      = {1097-0258},
  doi       = {10.1002/sim.2673},
  url       = {http://dx.doi.org/10.1002/sim.2673},
  abstract  = {Simulation studies use computer intensive procedures to assess the
	performance of a variety of statistical methods in relation to a
	known truth. Such evaluation cannot be achieved with studies of real
	data alone. Designing high-quality simulations that reflect the complex
	situations seen in practice, such as in prognostic factors studies,
	is not a simple process. Unfortunately, very few published simulation
	studies provide sufficient details to allow readers to understand
	fully all the processes required to design a simulation study. When
	planning a simulation study, it is recommended that a detailed protocol
	be produced, giving full details of how the study will be performed,
	analysed and reported. This paper details the important considerations
	necessary when designing any simulation study, including defining
	specific objectives of the study, determining the procedures for
	generating the data sets and the number of simulations to perform.
	A checklist highlighting the important considerations when designing
	a simulation study is provided. A small review of the literature
	identifies the current practices within published simulation studies.
	Copyright Â© 2006 John Wiley & Sons, Ltd.},
  groups    = {multi-D / sim SS},
  journal   = {Statistics in Medicine},
  keywords  = {simulation study, design, protocol, bias, mean square error, coverage},
  owner     = {meddwilb},
  publisher = {John Wiley \& Sons, Ltd.},
  timestamp = {2014.04.03},
}

@ARTICLE{Buyse2000,
  author = {Buyse, M. and Molenberghs, G. and Burzykowski, T. and Renard, D.
	and Geys, H.},
  title = {The validation of surrogate endpoints in meta-analyses of randomized
	experiments},
  journal = {Biostatistics},
  year = {2000},
  volume = {1},
  pages = {49-67},
  number = {1},
  abstract = {The validation of surrogate endpoints has been studied by Prentice
	(1989). He presented a definition as well as a set of criteria, which
	are equivalent only if the surrogate and true endpoints are binary.
	Freedman et al. (1992) supplemented these criteria with the so-called
	â€˜proportion explainedâ€™. Buyse and Molenberghs (1998) proposed
	replacing the proportion explained by two quantities: (1) the relative
	effect linking the effect of treatment on both endpoints and (2)
	an individual-level measure of agreement between both endpoints.
	The latter quantity carries over when data are available on several
	randomized trials, while the former can be extended to be a trial-level
	measure of agreement between the effects of treatment of both endpoints.
	This approach suggests a new method for the validation of surrogate
	endpoints, and naturally leads to the prediction of the effect of
	treatment upon the true endpoint, given its observed effect upon
	the surrogate endpoint. These ideas are illustrated using data from
	two sets of multicenter trials: one comparing chemotherapy regimens
	for patients with advanced ovarian cancer, the other comparing interferon-Î±
	with placebo for patients with age-related macular degeneration.},
  doi = {10.1093/biostatistics/1.1.49},
  eprint = {http://biostatistics.oxfordjournals.org/content/1/1/49.full.pdf+html},
  owner = {meddwilb},
  timestamp = {2013.11.20},
  url = {http://biostatistics.oxfordjournals.org/content/1/1/49.abstract}
}

@Article{Calafiore2013,
  author    = {Calafiore, L. and Amoroso, L. and Della Casa Alberighi, O. and Luksch, R. and Zanazzo, G. and Castellano, A. and Podda, M. and Dominici, C. and Haupt, R. and Corrias, M. V. and Garaventa, A.},
  title     = {Two-stage phase {II} study of imatinib mesylate in subjects with refractory or relapsing neuroblastoma},
  journal   = {Annals of Oncology},
  year      = {2013},
  volume    = {24},
  number    = {5},
  pages     = {1406-1413},
  doi       = {10.1093/annonc/mds648},
  eprint    = {http://annonc.oxfordjournals.org/content/24/5/1406.full.pdf+html},
  url       = {http://annonc.oxfordjournals.org/content/24/5/1406.abstract},
  abstract  = {Background Cure rate for subjects with refractory or relapsing metastatic
	neuroblastoma is <5%. In the search for a novel therapy, continuous
	daily oral administration of imatinib mesylate was evaluated.Patients
	and methods Twenty-four subjects were enrolled in a two-stage study.
	Imatinib was administered for the first 4 weeks (cycle) at 170 mg/sqm
	b.i.d. If no major toxicity occurred, the dose was escalated to 300
	mg/sqm b.i.d. for a maximum of 12 cycles. Clinical response and toxicity
	were evaluated according to international criteria. Pharmacokinetics
	(PK) profiles and tyrosine hydroxylase (TH) mRNA expression were
	also determined in a subset of subjects.Results Five (21%) complete
	responses, with one subject still alive at 68 months, and 2 (8%)
	partial responses lasting up to 29 months were obtained. No grade
	4 toxicity was observed. At steady-state, PK exposure (69.7 Âµg h/ml)
	was similar to that of adults receiving 1000 mg/die. Responses appear
	to correlate with the absence or presence of metastasis in the bone
	marrow (BM) alone, with low TH expression levels at study entry and
	low imatinib exposure.Conclusions Imatinib mesylate was well-tolerated
	and effective in the subset of subjects with low BM infiltration
	as only site of metastasis.Study identifier EudraCT: 2005-005778-63.},
  comment   = {Same setting as Garaventa, doesn't specify full design:
	
	
	0 7 10 ? ? 24
	
	
	So same design as Garaventa too, with same missunderstanding of the
	toxicity terminology.},
  owner     = {meddwilb},
  timestamp = {2014.02.28},
}

@Article{Campana2013,
  author    = {Campana, LucaG. and Bianchi, Giuseppe and Mocellin, Simone and Valpione, Sara and Campanacci, Laura and Brunello, Antonella and Donati, Davide and Sieni, Elisabetta and Rossi, CarloR.},
  title     = {Electrochemotherapy Treatment of Locally Advanced and Metastatic Soft Tissue Sarcomas: Results of a Non-Comparative Phase {II} Study},
  journal   = {World Journal of Surgery},
  year      = {2013},
  language  = {English},
  pages     = {1-10},
  issn      = {0364-2313},
  doi       = {10.1007/s00268-013-2321-1},
  url       = {http://dx.doi.org/10.1007/s00268-013-2321-1},
  comment   = {Design used in paper gives
	
	
	[1] 0.05 0.30 0.20 0.40
	
	[1] 0.01 0.20 0.10
	
	
	[1] 0 6 14 5 12 34
	
	[1] "Expected size: 62833.2898711179"
	
	[1] "alpha_r: 0.00172511037607361"
	
	[1] "alpha_t: 0.00268082739546522"
	
	[1] "power: 0.271838614175655"
	
	
	SSS gives
	
	
	[1] 1 3 14 6 9 34
	
	[1] "Expected size: 14674.4237428466"
	
	[1] "alpha_r: 0.000956942190285223"
	
	[1] "alpha_t: 0.0841903867431484"
	
	[1] "power: 0.753453258870623"
	
	
	Local search gives
	
	
	[1] 1 4 19 5 9 38
	
	[1] "Expected size: 25.1428313797966"
	
	[1] "alpha_r: 0.00920387795977963"
	
	[1] "alpha_t: 0.157754241680255"
	
	[1] "power: 0.900127932832121"},
  owner     = {meddwilb},
  publisher = {Springer US},
  timestamp = {2014.02.28},
}

@ARTICLE{Campbell2000,
  author = {Michelle Campbell and Ray Fitzpatrick and Andrew Haines and Ann Louise
	Kinmonth and Peter Sandercock and David Spiegelhalter and Peter Tyrer},
  title = {Framework for design and evaluation of complex interventions to improve
	health},
  journal = {BMJ},
  year = {2000},
  volume = {321},
  pages = {694--696},
  doi = {10.1136/bmj.321.7262.694},
  owner = {meddwilb},
  timestamp = {2013.09.17}
}

@ARTICLE{Campbell2004,
  author = {Campbell, Marion and Grimshaw, Jeremy and Elbourne, Diana},
  title = {Intracluster correlation coefficients in cluster randomized trials:
	empirical insights into how should they be reported},
  journal = {BMC Medical Research Methodology},
  year = {2004},
  volume = {4},
  pages = {9},
  number = {1},
  abstract = {BACKGROUND:Increasingly, researchers are recognizing that there are
	many situations where the use of a cluster randomized trial may be
	more appropriate than an individually randomized trial. Similarly,
	the need for appropriate standards of reporting of cluster trials
	is more widely acknowledged.METHODS:In this paper, we describe the
	results of a survey to inform the appropriate reporting of the intracluster
	correlation coefficient (ICC) - the statistical measure of the clustering
	effect associated with a cluster randomized trial.RESULTS:We identified
	three dimensions that should be considered when reporting an ICC
	- a description of the dataset (including characteristics of the
	outcome and the intervention), information on how the ICC was calculated,
	and information on the precision of the ICC.CONCLUSIONS:This paper
	demonstrates the development of a framework for the reporting of
	ICCs. If adopted into routine practice, it has the potential to facilitate
	the interpretation of the cluster trial being reported and should
	help the development of new trials in the area.},
  doi = {10.1186/1471-2288-4-9},
  issn = {1471-2288},
  owner = {meddwilb},
  pubmedid = {15115554},
  timestamp = {2014.07.21},
  url = {http://www.biomedcentral.com/1471-2288/4/9}
}

@ARTICLE{Campbell2007,
  author = {Campbell, M. J. and Donner, A. and Klar, N.},
  title = {Developments in cluster randomized trials and Statistics in Medicine},
  journal = {Statistics in Medicine},
  year = {2007},
  volume = {26},
  pages = {2--19},
  number = {1},
  doi = {10.1002/sim.2731},
  issn = {1097-0258},
  keywords = {cluster randomized trial, matched pair design, generalized estimating
	equations, population averaged models, cluster specific models, Consort
	statement},
  owner = {meddwilb},
  publisher = {John Wiley \& Sons, Ltd.},
  timestamp = {2013.09.12},
  url = {http://dx.doi.org/10.1002/sim.2731}
}

@ARTICLE{Campbell2004a,
  author = {Campbell, Marion K and Elbourne, Diana R and Altman, Douglas G},
  title = {CONSORT statement: extension to cluster randomised trials},
  journal = {BMJ},
  year = {2004},
  volume = {328},
  pages = {702--708},
  number = {7441},
  doi = {10.1136/bmj.328.7441.702},
  isbn = {1468-5833},
  issn = {0959-8138},
  owner = {meddwilb},
  publisher = {BMJ Publishing Group Ltd},
  timestamp = {2014.08.11}
}

@ARTICLE{Campbell2012,
  author = {Marion K Campbell and Gilda Piaggio and Diana R Elbourne and Douglas
	G Altman},
  title = {Consort 2010 statement: extension to cluster randomised trials},
  journal = {BMJ},
  year = {2012},
  volume = {345},
  month = {9},
  doi = {10.1136/bmj.e5661},
  owner = {meddwilb},
  timestamp = {2013.10.24}
}

@Article{Candel2015,
  author =    {Candel, Math JJM and van Breukelen, Gerard JP},
  title =     {Sample size calculation for treatment effects in randomized trials with fixed cluster sizes and heterogeneous intraclass correlations and variances},
  journal =   {Statistical Methods in Medical Research},
  year =      {2015},
  volume =    {24},
  number =    {5},
  pages =     {557-573},
  abstract =  {When comparing two different kinds of group therapy or two individual treatments where patients within each arm are nested within care providers, clustering of observations may occur in both arms. The arms may differ in terms of (a) the intraclass correlation, (b) the outcome variance, (c) the cluster size, and (d) the number of clusters, and there may be some ideal group size or ideal caseload in case of care providers, fixing the cluster size. For this case, optimal cluster numbers are derived for a linear mixed model analysis of the treatment effect under cost constraints as well as under power constraints. To account for uncertain prior knowledge on relevant model parameters, also maximin sample sizes are given. Formulas for sample size calculation are derived, based on the standard normal as the asymptotic distribution of the test statistic. For small sample sizes, an extensive numerical evaluation shows that in a two-tailed test employing restricted maximum likelihood estimation, a safe correction for both 80% and 90% power, is to add three clusters to each arm for a 5% type I error rate and four clusters to each arm for a 1% type I error rate.},
  doi =       {10.1177/0962280214563100},
  eprint =    {http://smm.sagepub.com/content/24/5/557.full.pdf+html},
  owner =     {meddwilb},
  timestamp = {2016.02.15},
  url =       {http://smm.sagepub.com/content/24/5/557.abstract}
}

@Article{Cao2009,
  author    = {Jing Cao and J. Jack Lee and Susan Alber},
  title     = {Comparison of Bayesian sample size criteria: {ACC}, {ALC}, and {WOC}},
  journal   = {Journal of Statistical Planning and Inference},
  year      = {2009},
  volume    = {139},
  number    = {12},
  pages     = {4111 - 4122},
  issn      = {0378-3758},
  doi       = {http://dx.doi.org/10.1016/j.jspi.2009.05.041},
  url       = {http://www.sciencedirect.com/science/article/pii/S0378375809001670},
  abstract  = {A challenge for implementing performance-based Bayesian sample size
	determination is selecting which of several methods to use. We compare
	three Bayesian sample size criteria: the average coverage criterion
	(ACC) which controls the coverage rate of fixed length credible intervals
	over the predictive distribution of the data, the average length
	criterion (ALC) which controls the length of credible intervals with
	a fixed coverage rate, and the worst outcome criterion (WOC) which
	ensures the desired coverage rate and interval length over all (or
	a subset of) possible datasets. For most models, the \{WOC\} produces
	the largest sample size among the three criteria, and sample sizes
	obtained by the \{ACC\} and the \{ALC\} are not the same. For Bayesian
	sample size determination for normal means and differences between
	normal means, we investigate, for the first time, the direction and
	magnitude of differences between the \{ACC\} and \{ALC\} sample sizes.
	For fixed hyperparameter values, we show that the difference of the
	\{ACC\} and \{ALC\} sample size depends on the nominal coverage,
	and not on the nominal interval length. There exists a threshold
	value of the nominal coverage level such that below the threshold
	the \{ALC\} sample size is larger than the \{ACC\} sample size, and
	above the threshold the \{ACC\} sample size is larger. Furthermore,
	the \{ACC\} sample size is more sensitive to changes in the nominal
	coverage. We also show that for fixed hyperparameter values, there
	exists an asymptotic constant ratio between the \{WOC\} sample size
	and the \{ALC\} (ACC) sample size. Simulation studies are conducted
	to show that similar relationships among the ACC, ALC, and \{WOC\}
	may hold for estimating binomial proportions. We provide a heuristic
	argument that the results can be generalized to a larger class of
	models. },
  keywords  = {Average coverage criterion},
  owner     = {meddwilb},
  timestamp = {2015.05.13},
}

@Article{Cella2011,
  author    = {Cella, Matteo and Stahl, Daniel and Reme, Silje Endresen and Chalder, Trudie},
  title     = {Therapist effects in routine psychotherapy practice: An account from chronic fatigue syndrome},
  year      = {2011},
  volume    = {21},
  number    = {2},
  pages     = {168-178},
  note      = {PMID: 21271461},
  doi       = {10.1080/10503307.2010.535571},
  eprint    = { http://dx.doi.org/10.1080/10503307.2010.535571 },
  url       = { http://dx.doi.org/10.1080/10503307.2010.535571 
},
  abstract  = { Abstract The effect of therapists in psychotherapy is a much debated
	topic, with a number of studies showing therapist variance being
	large while other studies show little or no variability in outcomes
	due to therapists. The aim of this study was to investigate therapist
	effects in a well-defined sample of patients and therapists from
	an outpatient service which specializes in providing cognitive behaviour
	therapy (CBT) for patients with chronic fatigue syndrome (CFS). Therapy
	was provided in a highly specialized clinical setting for CFS and
	was delivered by qualified CBT therapists with at least 2 years experience
	with this client group. Three hundred and seventy-four patients with
	CFS and 12 cognitive behavioural psychotherapists took part. Therapist
	effects on the primary outcomes of fatigue and disability were investigated
	with multilevel random effects models and variance component analysis.
	Different models were computed and compared. Results showed a reduction
	in fatigue and disability scores after therapy. Variance explained
	by therapists, when demographic covariates were accounted for, was
	0% for fatigue and under 2% for disability. A number of important
	factors may have played a significant role in minimizing therapist
	effects in our study. These are: specialist setting, single centre,
	patients with the same primary diagnosis, therapists of the same
	orientation and training, shared environment and supervision. Future
	studies may stress the importance of these factors in the investigation
	of the therapist effect in psychotherapy. },
  groups    = {PACE},
  journal   = {Psychotherapy Research},
  owner     = {meddwilb},
  timestamp = {2015.01.26},
}

@ARTICLE{Cerga-Pashoja2010,
  author = {Cerga-Pashoja, Arlinda and Lowery, David and Bhattacharya, Rahul
	and Griffin, Mark and Iliffe, Steve and Lee, James and Leonard, Claire
	and Ricketts, Sue and Strother, Lyn and Waters, Fiona and Ritchie,
	CraigW and Warner, James},
  title = {Evaluation of exercise on individuals with dementia and their carers:
	a randomised controlled trial},
  journal = {Trials},
  year = {2010},
  volume = {11},
  number = {1},
  eid = {53},
  doi = {10.1186/1745-6215-11-53},
  language = {English},
  owner = {meddwilb},
  publisher = {BioMed Central},
  timestamp = {2014.07.16},
  url = {http://dx.doi.org/10.1186/1745-6215-11-53}
}

@ARTICLE{Chakraborty2009,
  author = {Chakraborty, Bibhas and Collins, Linda M. and Strecher, Victor J.
	and Murphy, Susan A.},
  title = {Developing multicomponent interventions using fractional factorial
	designs},
  journal = {Statistics in Medicine},
  year = {2009},
  volume = {28},
  pages = {2687--2708},
  number = {21},
  abstract = {Multicomponent interventions composed of behavioral, delivery, or
	implementation factors in addition to medications are becoming increasingly
	common in health sciences. A natural experimental approach to developing
	and refining such multicomponent interventions is to start with a
	large number of potential components and screen out the least active
	ones. Factorial designs can be used efficiently in this endeavor.
	We address common criticisms and misconceptions regarding the use
	of factorial designs in these screening studies. We also provide
	an operationalization of screening studies. As an example, we consider
	the use of a screening study in the development of a multicomponent
	smoking cessation intervention. Simulation results are provided to
	support the discussions. Copyright Â© 2009 John Wiley & Sons, Ltd.},
  doi = {10.1002/sim.3643},
  issn = {1097-0258},
  keywords = {multicomponent intervention, experimental design, fractional factorial
	design, screening, follow-up studies},
  owner = {meddwilb},
  publisher = {John Wiley \& Sons, Ltd.},
  timestamp = {2013.11.20},
  url = {http://dx.doi.org/10.1002/sim.3643}
}

@Article{Chalder2015,
  author    = {Chalder, Trudie and Goldsmith, Kimberley A and White, Peter D and Sharpe, Michael and Pickles, Andrew R},
  title     = {Rehabilitative therapies for chronic fatigue syndrome: a secondary mediation analysis of the PACE trial},
  year      = {2015},
  volume    = {2},
  number    = {2},
  pages     = {141--152},
  doi       = {10.1016/S2215-0366(14)00069-8},
  url       = {http://www.thelancet.com/journals/lanpsy/article/PIIS2215-0366(14)00069-8/abstract},
  abstract  = {BackgroundCognitive behaviour therapy (CBT) added to specialist medical
	care (SMC), or graded exercise therapy (GET) added to SMC, are more
	effective in reducing fatigue and improving physical function than
	both adaptive pacing therapy (APT) plus SMC and SMC alone for chronic
	fatigue syndrome. We investigate putative treatment mechanisms.
	
	BackgroundCognitive behaviour therapy (CBT) added to specialist medical
	care (SMC), or graded exercise therapy (GET) added to SMC, are more
	effective in reducing fatigue and improving physical function than
	both adaptive pacing therapy (APT) plus SMC and SMC alone for chronic
	fatigue syndrome. We investigate putative treatment mechanisms.},
  booktitle = {The Lancet Psychiatry},
  comment   = {doi: 10.1016/S2215-0366(14)00069-8},
  groups    = {PACE},
  journal   = {The Lancet Psychiatry},
  owner     = {meddwilb},
  publisher = {Elsevier},
  timestamp = {2015.01.28},
}

@ARTICLE{Chaloner1995,
  author = {Kathryn Chaloner and Isabella Verdinelli},
  title = {Bayesian Experimental Design: A Review},
  journal = {Statistical Science},
  year = {1995},
  volume = {10},
  pages = {273-304},
  number = {3},
  abstract = {This paper reviews the literature on Bayesian experimental design.
	A unified view of this topic is presented, based on a decision-theoretic
	approach. This framework casts criteria from the Bayesian literature
	of design as part of a single coherent approach. The decision-theoretic
	structure incorporates both linear and nonlinear design problems
	and it suggests possible new directions to the experimental design
	problem, motivated by the use of new utility functions. We show that,
	in some special cases of linear design problems, Bayesian solutions
	change in a sensible way when the prior distribution and the utility
	function are modified to allow for the specific structure of the
	experiment. The decision-theoretic approach also gives a mathematical
	justification for selecting the appropriate optimality criterion.},
  doi = {10.1214/ss/1177009939},
  owner = {meddwilb},
  timestamp = {2013.11.15}
}

@Article{Chevalier2014,
  author    = {Cl{\'{e}}ment Chevalier and Victor Picheny and David Ginsbourger},
  title     = {{KrigInv}: An efficient and user-friendly implementation of batch-sequential inversion strategies based on kriging},
  year      = {2014},
  volume    = {71},
  month     = {mar},
  pages     = {1021--1034},
  doi       = {10.1016/j.csda.2013.03.008},
  url       = {http://dx.doi.org/10.1016/j.csda.2013.03.008},
  groups    = {Optimisation},
  journal   = {Computational Statistics {\&} Data Analysis},
  owner     = {meddwilb},
  publisher = {Elsevier {BV}},
  timestamp = {2016.07.26},
}

@ARTICLE{Chow2002,
  author = {Chow, Shein-Chung and Shao, Jun and Wang, Hansheng},
  title = {A NOTE ON SAMPLE SIZE CALCULATION FOR MEAN COMPARISONS BASED ON NONCENTRAL
	t-STATISTICS},
  journal = {Journal of Biopharmaceutical Statistics},
  year = {2002},
  volume = {12},
  pages = {441-456},
  number = {4},
  note = {PMID: 12477068},
  abstract = { One-sample and two-sample t-tests are commonly used in analyzing
	data from clinical trials in comparing mean responses from two drug
	products. During the planning stage of a clinical study, a crucial
	step is the sample size calculation, i.e., the determination of the
	number of subjects (patients) needed to achieve a desired power (e.g.,
	80%) for detecting a clinically meaningful difference in the mean
	drug responses. Based on noncentral t-distributions, we derive some
	sample size calculation formulas for testing equality, testing therapeutic
	noninferiority/superiority, and testing therapeutic equivalence,
	under the popular one-sample design, two-sample parallel design,
	and two-sample crossover design. Useful tables are constructed and
	some examples are given for illustration. },
  doi = {10.1081/BIP-120016229},
  eprint = { http://dx.doi.org/10.1081/BIP-120016229 },
  owner = {meddwilb},
  timestamp = {2014.10.30},
  url = { http://dx.doi.org/10.1081/BIP-120016229 
}
}

@ARTICLE{Ciani2013,
  author = {Oriana Ciani and Marc Buyse and Ruth Garside and Toby Pavey and Ken
	Stein and Jonathan A C Sterne and Rod S Taylor},
  title = {Comparison of treatment effect sizes associated with surrogate and
	final patient relevant outcomes in randomised controlled trials:
	meta-epidemiological study},
  journal = {BMJ},
  year = {2013},
  volume = {346:f457},
  month = {1},
  doi = {10.1136/bmj.f457},
  owner = {meddwilb},
  timestamp = {2013.11.21}
}

@ARTICLE{Cochran1934,
  author = {Cochran,W. G.},
  title = {The distribution of quadratic forms in a normal system, with applications
	to the analysis of covariance},
  journal = {Mathematical Proceedings of the Cambridge Philosophical Society},
  year = {1934},
  volume = {30},
  pages = {178--191},
  month = {4},
  abstract = {ABSTRACT Many of the most frequently used applications of the theory
	of statistics, such for example as the methods of analysis of variance
	and covariance, the general test of multiple regression and the test
	of a regression coefficient, depend essentially on the joint distribution
	of several quadratic forms in a univariate normal system. The object
	of this paper is to prove the main-relevant results about this distribution.
	As an application of these results, the theory involved in the method
	of analysis of covariance will be investigated.},
  doi = {10.1017/S0305004100016595},
  issn = {1469-8064},
  issue = {02},
  numpages = {14},
  owner = {meddwilb},
  timestamp = {2014.03.20},
  url = {http://journals.cambridge.org/article_S0305004100016595}
}

@Article{Cocks2013,
  author    = {Kim Cocks and David J. Torgerson},
  title     = {Sample size calculations for pilot randomized trials: a confidence intervalÂ approach},
  year      = {2013},
  volume    = {66},
  number    = {2},
  pages     = {197 - 201},
  issn      = {0895-4356},
  doi       = {http://dx.doi.org/10.1016/j.jclinepi.2012.09.002},
  url       = {http://www.sciencedirect.com/science/article/pii/S0895435612002740},
  abstract  = {Objectives To describe a method using confidence intervals (CIs) to
	estimate the sample size for a pilot randomized trial. Study Design
	Using one-sided \{CIs\} and the estimated effect size that would
	be sought in a large trial, we calculated the sample size needed
	for pilot trials. Results Using an 80% one-sided CI, we estimated
	that a pilot trial should have at least 9% of the sample size of
	the main planned trial. Conclusion Using the estimated effect size
	difference for the main trial and using a one-sided CI, this allows
	us to calculate a sample size for a pilot trial, which will make
	its results more useful than at present. },
  groups    = {SSR},
  journal   = {Journal of Clinical Epidemiology},
  keywords  = {Sample size},
  owner     = {meddwilb},
  timestamp = {2014.08.14},
}

@BOOK{Cohen1988,
  title = {Statistical power analysis for the behavioral sciences},
  publisher = {Lawrence Erlbaum Associates, Inc.},
  year = {1988},
  author = {Jacob Cohen},
  edition = {2nd},
  owner = {meddwilb},
  timestamp = {2014.01.06}
}

@ARTICLE{Cohen1996,
  author = {Judith A. Cohen and Anthony P. Mannarino},
  title = {A Treatment Outcome Study for Sexually Abused Preschool Children:
	Initial Findings },
  journal = {Journal of the American Academy of Child \& Adolescent Psychiatry
	},
  year = {1996},
  volume = {35},
  pages = {42 - 50},
  number = {1},
  abstract = {\{ABSTRACTObjective\} Treatment outcome for sexually abused preschool-age
	children and their parents was assessed, comparing the effectiveness
	of a cognitive-behavioral intervention to nondirective supportive
	treatment. Method Sixty-seven sexually abused preschool children
	and their parents were randomly assigned to either (1) cognitive-behavioral
	therapy adapted for sexually abused preschool children (CBT-SAP)
	or (2) nondirective supportive therapy (NST). Treatment consisted
	of 12 individual sessions for both the child and parent, monitored
	for integrity with the therapeutic model through intensive training
	and supervision, use of treatment manuals, and rating of audio taped
	sessions. Parents completed the Child Behavior Checklist, the Child
	Sexual Behavior Inventory, and the Weekly Behavior Report to measure
	a variety of emotional and behavioral symptoms. Results Within-group
	comparison of pretreatment and posttreatment outcome measures demonstrated
	that while the \{NST\} group did not change significantly with regard
	to symptomatology, the CBT-SAP group had highly significant symptomatic
	improvement on most outcome measures. Repeated-measures analyses
	of variance demonstrated group Ã— time interactions on some variables
	as well. Clinical findings also supported the effectiveness of the
	CBT-SAP intervention over NST. Conclusions Findings provide strong
	preliminary evidence for the effectiveness of a specific cognitive-behavioral
	treatment model for sexually abused preschool children and their
	parents. },
  doi = {http://dx.doi.org/10.1097/00004583-199601000-00011},
  issn = {0890-8567},
  keywords = {child sexual abuse},
  owner = {meddwilb},
  timestamp = {2014.12.16},
  url = {http://www.sciencedirect.com/science/article/pii/S0890856709634031}
}

@ARTICLE{Collins2009,
  author = {Collins, Linda M and Chakraborty, Bibhas and Murphy, Susan A and
	Strecher, Victor},
  title = {Comparison of a phased experimental approach and a single randomized
	clinical trial for developing multicomponent behavioral interventions},
  journal = {Clinical Trials},
  year = {2009},
  volume = {6},
  pages = {5-15},
  number = {1},
  abstract = {Background Many interventions in today's health sciences are multicomponent,
	and often one or more of the components are behavioral. Two approaches
	to building behavioral interventions empirically can be identified.
	The more typically used approach, labeled here the classical approach,
	consists of constructing a likely best intervention a priori, and
	then evaluating the intervention in a standard randomized controlled
	trial (RCT). By contrast, the emergent phased experimental approach
	involves programmatic phases of empirical research and discovery
	aimed at identifying individual intervention component effects and
	the best combination of components and levels.Purpose The purpose
	of this article is to provide a head-to-head comparison between the
	classical and phased experimental approaches and thereby highlight
	the relative advantages and disadvantages of these approaches when
	they are used to select program components and levels so as to arrive
	at the most potent intervention.Methods A computer simulation was
	performed in which the classical and phased experimental approaches
	to intervention development were applied to the same randomly generated
	data.Results The phased experimental approach resulted in better
	mean intervention outcomes when the intervention effect size was
	medium or large, whereas the classical approach resulted in better
	mean intervention outcomes when the effect size was small. The phased
	experimental approach led to identification of the correct set of
	intervention components and levels at a higher rate than the classical
	approach across all conditions.Limitations Some potentially important
	factors were not varied in the simulation, for example the underlying
	structural model and the number of intervention components.Conclusions
	The phased experimental approach merits serious consideration, because
	it has the potential to enable intervention scientists to develop
	more efficacious behavioral interventions. Clinical Trials 2009;
	6: 5â€”15. http://ctj.sagepub.com},
  doi = {10.1177/1740774508100973},
  eprint = {http://ctj.sagepub.com/content/6/1/5.full.pdf+html},
  owner = {meddwilb},
  timestamp = {2013.11.20},
  url = {http://ctj.sagepub.com/content/6/1/5.abstract}
}

@ARTICLE{Collins2005,
  author = {Collins, Linda M. and Murphy, Susan A. and Nair, Vijay N. and Strecher,
	Victor J.},
  title = {A strategy for optimizing and evaluating behavioral interventions},
  journal = {Annals of Behavioral Medicine},
  year = {2005},
  volume = {30},
  pages = {65-73},
  number = {1},
  doi = {10.1207/s15324796abm3001\_8},
  issn = {0883-6612},
  language = {English},
  owner = {meddwilb},
  publisher = {Springer-Verlag},
  timestamp = {2013.10.11},
  url = {http://dx.doi.org/10.1207/s15324796abm3001\_8}
}

@ARTICLE{Collinson2014,
  author = {Collinson, Michelle and Owens, David and Blenkiron, Paul and Burton,
	Kayleigh and Graham, Liz and Hatcher, Simon and House, Allan and
	Martin, Katie and Pembroke, Louise and Protheroe, David and Tubeuf,
	Sandy and Farrin, Amanda},
  title = {MIDSHIPS: Multicentre Intervention Designed for Self-Harm using Interpersonal
	Problem-Solving: protocol for a randomised controlled feasibility
	study},
  journal = {Trials},
  year = {2014},
  volume = {15},
  pages = {163},
  number = {1},
  abstract = {BACKGROUND:Around 150,000 people each year attend hospitals in England
	due to self-harm, many of them more than once. Over 5,000 people
	die by suicide each year in the UK, a quarter of them having attended
	hospital in the previous year because of self-harm. Self-harm is
	a major identifiable risk factor for suicide. People receive variable
	care at hospital; many are not assessed for their psychological needs
	and little psychological therapy is offered. Despite its frequent
	occurrence, we have no clear research evidence about how to reduce
	the repetition of self-harm. Some people who have self-harmed show
	less active ways of solving problems, and brief problem-solving therapies
	are considered the most promising psychological treatments.METHODS/DESIGN:This
	is a pragmatic, individually randomised, controlled, feasibility
	study comparing interpersonal problem-solving therapy plus treatment-as-usual
	with treatment-as-usual alone, for adults attending a general hospital
	following self-harm. A total of 60 participants will be randomised
	equally between the treatment arms, which will be balanced with respect
	to the type of most recent self-harm event, number of previous self-harm
	events, gender and age. Feasibility objectives are as follows: a)
	To establish and field test procedures for implementing the problem-solving
	intervention; b) To determine the feasibility and best method of
	participant recruitment and follow up; c) To assess therapeutic delivery;
	d) To assess the feasibility of obtaining the definitive trial's
	primary and secondary outcomes; e) To assess the perceived burden
	and acceptability of obtaining the trial's self-reported outcome
	data; f) To inform the sample size calculation for the definitive
	trial.DISCUSSION:The results of this feasibility study will be used
	to determine the appropriateness of proceeding to a definitive trial
	and will allow us to design an achievable trial of interpersonal
	problem-solving therapy for adults who self-harm.TRIAL REGISTRATION:Current
	Controlled Trials (ISRCTN54036115)},
  doi = {10.1186/1745-6215-15-163},
  issn = {1745-6215},
  owner = {meddwilb},
  pubmedid = {24886683},
  timestamp = {2014.07.16},
  url = {http://www.trialsjournal.com/content/15/1/163}
}

@BOOK{Colquhoun1971,
  title = {Lectures on Biostatistics},
  publisher = {Oxford University Press},
  year = {1971},
  author = {David Colquhoun},
  file = {:P\:\\CTRU\\Projects\\Health_Sciences\\Methodology\\2013 NIHR Research Methods Fellowship\\Literature\\Books\\Colquhoun_Lectures_on_biostatistics.pdf:PDF},
  owner = {meddwilb},
  timestamp = {2014.08.05}
}

@Article{Conaway1995,
  author              = {Conaway, Mark R. and Petroni, Gina R.},
  title               = {Bivariate Sequential Designs for Phase {II} Trials},
  journal             = {Biometrics},
  year                = {1995},
  language            = {English},
  volume              = {51},
  number              = {2},
  pages               = {pp. 656-664},
  issn                = {0006341X},
  url                 = {http://www.jstor.org/stable/2532952},
  abstract            = {In this paper we propose methods for designing group sequential phase
	II trials with two dependent binary endpoints. The emphasis is on
	the derivation of stopping rules for phase II trials which require
	the enrollment of a small number of patients. The methods are based
	on enumerating the exact distribution for the binary endpoints. We
	illustrate the methods with a recent study which required the use
	of a group sequential design to monitor antitumor activity and toxicity.},
  comment             = {As in~\cite{Bryant1995}, two stage design for two binary outcomes, each of which we want to be good - so proceed to next stage if \emph{both} endpoints exceed critical values.

Operating characteristics are: a `local' type I error (proceeding past stage 2 under 00); a `global' type I error (the maximum of the type one error over the entire null hypothesis space, defined as less effective than the standard in either aspect); and a single type II error (not proceeding under 11).  They go on to show that the 'global' type I must occur at either 10 or 01.  Note that this constraint is then the same as having two type I errors, at 01 and 10, with the same nominal constraint.

Criteria suggested are minimal expected sample size under 00, or minimal expected sample size at either 01 or 10 (so minimising the maximum expected sample size over the null region).  But, emphasises that the choice of criteria is flexible and we can use whatever we like.},
  copyright           = {Copyright Â© 1995 International Biometric Society},
  groups              = {Multiple endpoints},
  jstor_articletype   = {research-article},
  jstor_formatteddate = {Jun., 1995},
  owner               = {meddwilb},
  publisher           = {International Biometric Society},
  timestamp           = {2014.12.04},
}

@ARTICLE{Conaway1996,
  author = {Conaway, Mark R. and Petroni, Gina R.},
  title = {Designs for Phase {II} Trials Allowing for a Trade-Off between Response
	and Toxicity},
  journal = {Biometrics},
  year = {1996},
  volume = {52},
  pages = {pp. 1375-1386},
  number = {4},
  abstract = {In this paper we propose methods for designing phase II trials that
	allow for a trade-off between treatment safety and antitumor activity,
	where safety and antitumor activity are measured as binary endpoints.
	The designs can be carried out either in a single stage or can be
	conducted in two stages, with an interim analysis to assess whether
	the treatment appears sufficiently safe and effective to warrant
	continuing. The emphasis is on the derivation of stopping rules for
	phase II trials that require the enrollment of a small number of
	patients and are based on enumerating the exact distribution of the
	proposed test statistic. We illustrate the methods with a recent
	study that required the use of a group sequential design to monitor
	antitumor activity and toxicity.},
  copyright = {Copyright Â© 1996 International Biometric Society},
  issn = {0006341X},
  jstor_articletype = {research-article},
  jstor_formatteddate = {Dec., 1996},
  language = {English},
  owner = {meddwilb},
  publisher = {International Biometric Society},
  timestamp = {2014.12.04},
  url = {http://www.jstor.org/stable/2532851}
}

@Article{Conti2010,
  author    = {Stefano Conti and Anthony O'Hagan},
  title     = {{Bayes}ian emulation of complex multi-output and dynamic computer models},
  year      = {2010},
  volume    = {140},
  number    = {3},
  month     = {mar},
  pages     = {640--651},
  doi       = {10.1016/j.jspi.2009.08.006},
  url       = {http://dx.doi.org/10.1016/j.jspi.2009.08.006},
  groups    = {Optimisation},
  journal   = {Journal of Statistical Planning and Inference},
  owner     = {meddwilb},
  publisher = {Elsevier {BV}},
  timestamp = {2016.07.26},
}

@ARTICLE{Cook2012,
  author = {Cook, Jonathan and Bruckner, Thomas and MacLennan, Graeme and Seiler,
	Christoph},
  title = {Clustering in surgical trials - database of intracluster correlations},
  journal = {Trials},
  year = {2012},
  volume = {13},
  pages = {2},
  number = {1},
  abstract = {BACKGROUND:Randomised trials evaluation of surgical interventions
	are often designed and analysed as if the outcome of individual patients
	is independent of the surgeon providing the intervention. There is
	reason to expect outcomes for patients treated by the same surgeon
	tend to be more similar than those under the care of another surgeon
	due to previous experience, individual practice, training, and infrastructure.
	Such a phenomenon is referred to as the clustering effect and potentially
	impacts on the design and analysis adopted and thereby the required
	sample size. The aim of this work was to inform trial design by quantifying
	clustering effects (at both centre and surgeon level) for various
	outcomes using a database of surgical trials.METHODS:Intracluster
	correlation coefficients (ICCs) were calculated for outcomes from
	a set of 10 multicentre surgical trials for a range of outcomes and
	different time points for clustering at both the centre and surgeon
	level.RESULTS:ICCs were calculated for 198 outcomes across the 10
	trials at both centre and surgeon cluster levels. The number of cases
	varied from 138 to 1370 across the trials. The median (range) average
	cluster size was 32 (9 to 51) and 6 (3 to 30) for centre and surgeon
	levels respectively. ICC estimates varied substantially between outcome
	type though uncertainty around individual ICC estimates was substantial,
	which was reflected in generally wide confidence intervals.CONCLUSIONS:This
	database of surgical trials provides trialists with valuable information
	on how to design surgical trials. Our data suggests clustering of
	outcome is more of an issue than has been previously acknowledged.
	We anticipate that over time the addition of ICCs from further surgical
	trial datasets to our database will further inform the design of
	surgical trials.},
  doi = {10.1186/1745-6215-13-2},
  issn = {1745-6215},
  owner = {meddwilb},
  pubmedid = {22217216},
  timestamp = {2015.05.22},
  url = {http://www.trialsjournal.com/content/13/1/2}
}

@ARTICLE{Cornfield1976,
  author = {J. Cornfield},
  title = {Recent methodological contributions to clinical trials},
  journal = {American Journal of Epidemiology},
  year = {1976},
  volume = {104},
  pages = {408-421},
  number = {4},
  owner = {meddwilb},
  timestamp = {2014.05.12}
}

@ARTICLE{Cornfield1978,
  author = {Cornfield, Jerome},
  title = {Randomization by group: a formal analysis},
  journal = {American Journal of Epidemiology},
  year = {1978},
  volume = {108},
  pages = {100-102},
  number = {2},
  eprint = {http://aje.oxfordjournals.org/content/108/2/100.full.pdf+html},
  owner = {meddwilb},
  timestamp = {2013.09.12},
  url = {http://aje.oxfordjournals.org/content/108/2/100.short}
}

@ARTICLE{Corrigan2002,
  author = {Corrigan, PJ and Basket, RM and Farrin, AJ and Mulley, GP and Heath,
	MR},
  title = {The development of a method for functional assessment of dentures},
  journal = {Gerodontology},
  year = {2002},
  volume = {19},
  pages = {41--45},
  number = {1},
  abstract = {Aims: To design and validate a method of assessing complete dentures
	from a functional standpoint.Subjects: A random sample of 40 complete
	denture wearers took part in the study.Setting: A university clinical
	department of prosthetic dentistry.Intervention: We undertook a pilot
	study to refine the protocol and criteria. All participants and their
	dentures were examined by two authors independently, with no prior
	knowledge of the patients'complaints.Design: We defined nine clinical
	factors of functional quality and applied criteria with binary scoring.
	We analysed the scores for these factors for inter-rater reliability.Results:
	The method proved simple to apply and took less than 5 minutes to
	complete. The inter-examiner agreement for all factors was 86% to
	100% giving Kappa scores of 0.64 to 1.00 (all Good or Very Good).Conclusions:
	This study successfully demonstrates that the technique, which we
	call the Functional Assessment of Dentures (FAD), can give good inter-examiner
	reliability. It can therefore be used separately as a routine diagnostic
	tool and to investigate the relationship between denture qualities
	and functional â€˜outcomeâ€™ such as difficulty eating or dietary
	selection.},
  doi = {10.1111/j.1741-2358.2002.00041.x},
  issn = {1741-2358},
  keywords = {complete denture assessment, quality index, elderly},
  owner = {meddwilb},
  publisher = {Blackwell Publishing Ltd},
  timestamp = {2014.03.25},
  url = {http://dx.doi.org/10.1111/j.1741-2358.2002.00041.x}
}

@ARTICLE{Cosimelli2010,
  author = {Cosimelli, M and Golfieri, R and Cagol, P P and Carpanese, L and
	Sciuto, R and Maini, C L and Mancini, R and Sperduti, I and Pizzi,
	G and Diodoro, M G and Perrone, M and Giampalma, E and Angelelli,
	B and Fiore, F and Lastoria, S and Bacchetti, S and Gasperini, D
	and Geatti, O and Izzo, F},
  title = {Multi-centre phase II clinical trial of yttrium-90 resin microspheres
	alone in unresectable, chemotherapy refractory colorectal liver metastases},
  journal = {Br J Cancer},
  year = {2010},
  volume = {103},
  pages = {324--331},
  number = {3},
  month = jul,
  issn = {0007-0920},
  owner = {meddwilb},
  publisher = {Cancer Research UK},
  timestamp = {2015.09.17},
  url = {http://dx.doi.org/10.1038/sj.bjc.6605770}
}

@ARTICLE{Cottrell2011,
  author = {Cottrell, David and Graham, Liz and Farrin, Amanda},
  title = {The complexities of maximising recruitment to complex intervention
	trials in child and adolescent mental health services},
  journal = {Trials},
  year = {2011},
  volume = {12},
  pages = {A122},
  number = {Suppl 1},
  doi = {10.1186/1745-6215-12-S1-A122},
  issn = {1745-6215},
  owner = {meddwilb},
  timestamp = {2014.03.25},
  url = {http://www.trialsjournal.com/content/12/S1/A122}
}

@BOOK{Cox1974,
  title = {Theoretical Statistics},
  publisher = {Chapman and Hall Ltd.},
  year = {1974},
  author = {D. R. Cox and D. V. Hinkley},
  edition = {1st},
  owner = {meddwilb},
  timestamp = {2014.04.02}
}

@ARTICLE{Craig2008,
  author = {Peter Craig and Paul Dieppe and Sally Macintyre and Susan Michie
	and Irwin Nazareth and Mark Petticrew},
  title = {Developing and evaluating complex interventions: the new Medical
	Research Council guidance},
  journal = {BMJ: British Medical Journal},
  year = {2008},
  volume = {337},
  month = {9},
  doi = {10.1136/bmj.a1655},
  owner = {meddwilb},
  timestamp = {2013.09.06}
}

@INPROCEEDINGS{Dawid2008,
  author = {Philip Dawid and Vanessa Didelez},
  title = {Identifying Optimal Sequential Decisions},
  booktitle = {Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial
	Intelligence (UAI 2008)},
  year = {2008},
  abstract = {We consider conditions that allow us to find an optimal strategy for
	sequential decisions from a given data situation. For the case where
	all interventions are unconditional (atomic), identifiability has
	been discussed by Pearl & Robins (1995). We argue here that an optimal
	strategy must be conditional, i.e. take the information available
	at each decision point into account. We show that the identification
	of an optimal sequential decision strategy is more restrictive, in
	the sense that conditional interventions might not always be identified
	when atomic interventions are. We further demonstrate that a simple
	graphical criterion for the identifiability of an optimal strategy
	can be given.},
  owner = {meddwilb},
  timestamp = {2014.02.19},
  url = {http://arxiv.org/abs/1206.3245}
}

@Article{DeSantis2007,
  author    = {De Santis, Fulvio},
  title     = {Using historical data for Bayesian sample size determination},
  year      = {2007},
  volume    = {170},
  number    = {1},
  pages     = {95--113},
  issn      = {1467-985X},
  doi       = {10.1111/j.1467-985X.2006.00438.x},
  url       = {http://dx.doi.org/10.1111/j.1467-985X.2006.00438.x},
  abstract  = {Summary.â€‚ We consider the sample size determination (SSD) problem,
	which is a basic yet extremely important aspect of experimental design.
	Specifically, we deal with the Bayesian approach to SSD, which gives
	researchers the possibility of taking into account pre-experimental
	information and uncertainty on unknown parameters. At the design
	stage, this fact offers the advantage of removing or mitigating typical
	drawbacks of classical methods, which might lead to serious miscalculation
	of the sample size. In this context, the leading idea is to choose
	the minimal sample size that guarantees a probabilistic control on
	the performance of quantities that are derived from the posterior
	distribution and used for inference on parameters of interest. We
	are concerned with the use of historical dataâ€”i.e. observations
	from previous similar studiesâ€”for SSD. We illustrate how the class
	of power priors can be fruitfully employed to deal with lack of homogeneity
	between historical data and observations of the upcoming experiment.
	This problem, in fact, determines the necessity of discounting prior
	information and of evaluating the effect of heterogeneity on the
	optimal sample size. Some of the most popular Bayesian SSD methods
	are reviewed and their use, in concert with power priors, is illustrated
	in several medical experimental contexts.},
  groups    = {SSR},
  journal   = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  keywords  = {Elicitation, Experimental design, Historical data, Power priors, Sample size, Statistical evidence},
  owner     = {meddwilb},
  publisher = {Blackwell Publishing Ltd},
  timestamp = {2014.10.27},
}

@Article{DelMastro2005,
  author    = {Del Mastro, L. and Perrone, F. and Repetto, L. and Manzione, L. and Zagonel, V. and Fratino, L. and Marenco, D. and Venturini, M. and Maggi, E. and Bighin, C. and Catzeddu, T. and Venturino, A. and Rosso, R.},
  title     = {Weekly paclitaxel as first-line chemotherapy in elderly advanced breast cancer patients: a phase {II} study of the Gruppo Italiano di Oncologia Geriatrica (GIOGer)},
  journal   = {Annals of Oncology},
  year      = {2005},
  volume    = {16},
  number    = {2},
  pages     = {253-258},
  doi       = {10.1093/annonc/mdi056},
  eprint    = {http://annonc.oxfordjournals.org/content/16/2/253.full.pdf+html},
  url       = {http://annonc.oxfordjournals.org/content/16/2/253.abstract},
  abstract  = {Background: First-line chemotherapy regimens suitable for elderly
	advanced breast cancer patients are still not defined.Patients and
	methods: Women with stage III or IV breast cancer aged â‰¥70 years
	were enrolled in a phase II study aimed to evaluate both activity
	and toxicity of weekly paclitaxel. Among 46 planned patients, at
	least 18 responses and not more than seven unacceptable toxic events
	are required for a favourable conclusion. Paclitaxel 80â€‰mg/m2 was
	administered weekly for 3 weeks every 28 days.Results: Unacceptable
	toxicity occurred in seven out of 46 patients evaluated for toxicity
	[15.2%; exact 95% confidence interval (CI) 7.6% to 28.2%] and was
	represented by one case of febrile neutropenia, one case of severe
	allergic reaction and five cases of cardiac toxicity. Among 41 patients
	evaluated for response, a complete response occurred in two (4.9%)
	patients and a partial response in 20 (48.8%), with an overall response
	rate of 53.7% (exact 95% CI 38.7% to 67.9%). The median progression-free
	survival was 9.7 months (95% CI 8.5â€“18.7) and median survival was
	35.8 months (95% CI 19â€“not defined).Conclusions: Weekly paclitaxel
	is highly active in elderly advanced breast cancer patients. Data
	on cardiovascular complications, however, indicate the need for a
	careful monitoring of cardiac function before and during chemotherapy.},
  comment   = {design used, d <- c(7,17,22,18,38,46), same as SSS
	
	Gives
	
	[1] 0.30 0.50 0.75 0.95
	
	[1] 0.1 0.1 0.1
	
	
	[1] 7 17 22 18 38 46
	
	[1] "Expected size: 2991.8406105673"
	
	[1] "alpha_r: 0.0594552671186883"
	
	[1] "alpha_t: 0.0632290283417144"
	
	[1] "power: 0.870380176370362"
	
	
	Local search finds
	
	[1] 0 10 16 15 32 39
	
	[1] "Expected size: 283.723183508727"
	
	[1] "alpha_r: 0.0941346661000272"
	
	[1] "alpha_t: 0.0999946193975158"
	
	[1] "power: 0.897551985262453"},
  owner     = {meddwilb},
  timestamp = {2014.03.03},
}

@ARTICLE{Demetrashvili2014,
  author = {Demetrashvili, Nino and Wit, Ernst C and van den Heuvel, Edwin R},
  title = {Confidence intervals for intraclass correlation coefficients in variance
	components models},
  journal = {Statistical Methods in Medical Research},
  year = {2014},
  abstract = {Confidence intervals for intraclass correlation coefficients in agreement
	studies with continuous outcomes are model-specific and no generic
	approach exists. This paper provides two generic approaches for intraclass
	correlation coefficients of the form . The first approach uses Satterthwaiteâ€™s
	approximation and an F-distribution. The second approach uses the
	first and second moments of the intraclass correlation coefficient
	estimate in combination with a Beta distribution. Both approaches
	are based on the restricted maximum likelihood estimates for the
	variance components involved. Simulation studies are conducted to
	examine the coverage probabilities of the confidence intervals for
	agreement studies with a mix of small sample sizes. Two different
	three-way variance components models and balanced and unbalanced
	one-way random effects models are investigated. The proposed approaches
	are compared with other approaches developed for these specific models.
	The approach based on the F-distribution provides acceptable coverage
	probabilities, but the approach based on the Beta distribution results
	in accurate coverages for most settings in both balanced and unbalanced
	designs. A real agreement study is provided to illustrate the approaches.},
  doi = {10.1177/0962280214522787},
  eprint = {http://smm.sagepub.com/content/early/2014/02/14/0962280214522787.full.pdf+html},
  owner = {meddwilb},
  timestamp = {2016.01.14},
  url = {http://smm.sagepub.com/content/early/2014/02/14/0962280214522787.abstract}
}

@BOOK{Desu1990,
  title = {Sample size methodology},
  publisher = {Academic Press, Inc.},
  year = {1990},
  author = {M. M. Desu and D. Raghavarao},
  owner = {meddwilb},
  timestamp = {2014.01.06}
}

@ARTICLE{Dhani2009,
  author = {Dhani, Neesha and Tu, Dongsheng and Sargent, Daniel J. and Seymour,
	Lesley and Moore, Malcolm J.},
  title = {Alternate Endpoints for Screening Phase {II} Studies},
  journal = {Clinical Cancer Research},
  year = {2009},
  volume = {15},
  pages = {1873-1882},
  number = {6},
  abstract = {Phase II trials are screening trials that seek to identify agents
	with sufficient activity to continue development and those for which
	further evaluation should be halted. Although definitive phase III
	trials use progression-free or overall survival to confirm clinical
	benefit, earlier endpoints are preferable for phase II trials. Traditionally,
	tumor shrinkage of a predetermined degree (response) has been used
	as a surrogate of eventual survival benefit based on the observation
	that high response rates (RR), and particularly complete responses,
	in the phase II setting resulted in survival benefit in subsequent
	phase III trials. Recently, some molecularly targeted agents have
	shown survival and clinical benefit despite very modest RRs in early
	clinical trials. These observations provide a major conundrum, with
	concerns of inappropriate termination of development for active agents
	with low RRs being balanced by concerns of inactive agents being
	taken to late-phase development with resultant increases in the failure
	rate of phase III trials. Numerous alternate or complementary endpoints
	have been explored, incorporating multinomial endpoints (including
	progression and response), progression-free survival, biomarkers,
	and, more recently, evaluation of tumor size as a continuous variable.
	In this review, we discuss the current status of phase II endpoints
	and present retrospective analyses of two international gastrointestinal
	cancer studies showing the potential utility of one novel approach.
	Alternate endpoints, although promising, require additional evaluation
	and prospective validation before their use as a primary endpoint
	for phase II trials.},
  doi = {10.1158/1078-0432.CCR-08-2034},
  eprint = {http://clincancerres.aacrjournals.org/content/15/6/1873.full.pdf+html},
  owner = {meddwilb},
  timestamp = {2014.04.09},
  url = {http://clincancerres.aacrjournals.org/content/15/6/1873.abstract}
}

@Article{Ding2007,
  author    = {Meichun Ding and Gary L. Rosner and Peter Müller},
  title     = {{Bayes}ian Optimal Design for Phase {II} Screening Trials},
  year      = {2007},
  volume    = {64},
  number    = {3},
  month     = {dec},
  pages     = {886--894},
  doi       = {10.1111/j.1541-0420.2007.00951.x},
  url       = {http://dx.doi.org/10.1111/j.1541-0420.2007.00951.x},
  groups    = {Bayesian, Decision theory},
  journal   = {Biometrics},
  owner     = {meddwilb},
  publisher = {Wiley-Blackwell},
  timestamp = {2016.07.08},
}

@ARTICLE{Djulbegovic2008,
  author = {Benjamin Djulbegovic and Ambuj Kumar and Heloisa P. Soares and Iztok
	Hozo and Gerold Bepler and Mike Clarke and Charles L. Bennett},
  title = {Treatment success in cancer: New cancer treatment successes identified
	in phase 3 randomized controlled trials conducted by the national
	cancer institute-sponsored cooperative oncology groups, 1955 to 2006},
  journal = {Archives of Internal Medicine},
  year = {2008},
  volume = {168},
  pages = {632--642},
  number = {6},
  month = mar,
  abstract = {BackgroundÂ  The evaluation of research output, such as estimation
	of the proportion of treatment successes, is of ethical, scientific,
	and public importance but has rarely been evaluated systematically.
	We assessed how often experimental cancer treatments that undergo
	testing in randomized clinical trials (RCTs) result in discovery
	of successful new interventions.MethodsÂ  We extracted data from
	all completed (published and unpublished) phase 3 RCTs conducted
	by the National Cancer Institute cooperative groups since their inception
	in 1955. Therapeutic successes were determined by (1) assessing the
	proportion of statistically significant trials favoring new or standard
	treatments, (2) determining the proportion of the trials in which
	new treatments were considered superior to standard treatments according
	to the original researchers, and (3) quantitatively synthesizing
	data for main clinical outcomes (overall and event-free survival).ResultsÂ 
	Data from 624 trials (781 randomized comparisons) involving 216Â 451
	patients were analyzed. In all, 30% of trials had statistically significant
	results, of which new interventions were superior to established
	treatments in 80% of trials. The original researchers judged that
	the risk-benefit profile favored new treatments in 41% of comparisons
	(316 of 766). Hazard ratios for overall and event-free survival,
	available for 614 comparisons, were 0.95 (99% confidence interval
	[CI], 0.93-0.98) and 0.90 (99% CI, 0.87- 0.93), respectively, slightly
	favoring new treatments. Breakthrough interventions were discovered
	in 15% of trials.ConclusionsÂ  Approximately 25% to 50% of new cancer
	treatments that reach the stage of assessment in RCTs will prove
	successful. The pattern of successes has become more stable over
	time. The results are consistent with the hypothesis that the ethical
	principle of equipoise defines limits of discoverability in clinical
	research and ultimately drives therapeutic advances in clinical medicine.},
  comment = {10.1001/archinte.168.6.632},
  doi = {10.1001/archinte.168.6.632},
  issn = {0003-9926},
  owner = {meddwilb},
  timestamp = {2014.11.04},
  url = {http://dx.doi.org/10.1001/archinte.168.6.632}
}

@ARTICLE{Dmitrienko2013,
  author = {Dmitrienko, Alex and D'Agostino, Ralph},
  title = {Traditional multiplicity adjustment methods in clinical trials},
  journal = {Statistics in Medicine},
  year = {2013},
  volume = {32},
  pages = {5172--5218},
  number = {29},
  abstract = {This tutorial discusses important statistical problems arising in
	clinical trials with multiple clinical objectives based on different
	clinical variables, evaluation of several doses or regiments of a
	new treatment, analysis of multiple patient subgroups, etc. Simultaneous
	assessment of several objectives in a single trial gives rise to
	multiplicity. If unaddressed, problems of multiplicity can undermine
	integrity of statistical inferences. The tutorial reviews key concepts
	in multiple hypothesis testing and introduces main classes of methods
	for addressing multiplicity in a clinical trial setting. General
	guidelines for the development of relevant and efficient multiple
	testing procedures are presented on the basis of application-specific
	clinical and statistical information. Case studies with common multiplicity
	problems are used to motivate and illustrate the statistical methods
	presented in the tutorial, and software implementation of the multiplicity
	adjustment methods is discussed. Copyright Â© 2013 John Wiley & Sons,
	Ltd.},
  doi = {10.1002/sim.5990},
  issn = {1097-0258},
  keywords = {clinical trials, multiplicity problems, multiplicity adjustments,
	type I error rate, multiple testing procedures},
  owner = {meddwilb},
  timestamp = {2014.04.23},
  url = {http://dx.doi.org/10.1002/sim.5990}
}

@ARTICLE{Dmitrienko2013a,
  author = {Dmitrienko, Alex and D'Agostino, Ralph B. and Huque, Mohammad F.},
  title = {Key multiplicity issues in clinical drug development},
  journal = {Statistics in Medicine},
  year = {2013},
  volume = {32},
  pages = {1079--1111},
  number = {7},
  abstract = {Much progress has been made over the past decade with the development
	of novel methods for addressing increasingly more complex multiplicity
	problems arising in confirmatory Phase III clinical trials. This
	includes traditional problems with a single source of multiplicity,
	for example, analysis of multiple endpoints or doseâ€“placebo contrasts.
	In addition, more advanced problems with several sources of multiplicity
	have attracted attention in clinical drug development. These problems
	include two or more families of objectives such as multiple endpoints
	evaluated at multiple dose levels or in multiple patient populations.
	This paper provides a review of concepts that play a central role
	in defining and solving multiplicity problems (error rate definitions)
	and introduces main classes of multiple testing procedures widely
	used in clinical trials (nonparametric, semiparametric, and parametric
	procedures). The paper also presents recent advances in multiplicity
	research, including gatekeeping procedures for clinical trials with
	multiple sets of objectives. The concepts and methods introduced
	in the paper are illustrated using several case studies on the basis
	of real clinical trials. Software implementation of commonly used
	multiple testing and gatekeeping procedures is discussed. Copyright
	Â© 2012 John Wiley & Sons, Ltd.},
  doi = {10.1002/sim.5642},
  issn = {1097-0258},
  keywords = {clinical trials, multiple comparisons, type I error rate control,
	mltiple testing procedures, gatekeeping procedures},
  owner = {meddwilb},
  publisher = {John Wiley \& Sons, Ltd},
  timestamp = {2014.04.23},
  url = {http://dx.doi.org/10.1002/sim.5642}
}

@ARTICLE{Donner1986,
  author = {Donner, Allan},
  title = {A Review of Inference Procedures for the Intraclass Correlation Coefficient
	in the One-Way Random Effects Model},
  journal = {International Statistical Review / Revue Internationale de Statistique},
  year = {1986},
  volume = {54},
  pages = {pp. 67-82},
  number = {1},
  abstract = {Recent theory and methodology for inferences concerning the intraclass
	correlation coefficient are reviewed, under the assumption of an
	underlying random effects model. Topics discussed include point and
	interval estimation, significance-testing for nonzero values of the
	intraclass correlation, and inference procedures in multiple samples.
	/// On prÃ©sente une revue de la thÃ©orie et de la mÃ©thodologie
	rÃ©centes de l'infÃ©rence en ce qui concerne le coefficient de corrÃ©lation
	intraclasse, en se basant sur l'hypothÃ¨se d'un modÃ¨le d'effets
	alÃ©atoires. Les sujets discutÃ©s comprennent l'estimation ponctuelle
	et par intervalles, des tests significatifs pour des valeurs non-zÃ©ro
	de la corrÃ©lation intraclasse et des procÃ©dures d'infÃ©rence pour
	des Ã©chantillons multiples.},
  copyright = {Copyright Â© 1986 International Statistical Institute (ISI)},
  issn = {03067734},
  jstor_articletype = {research-article},
  jstor_formatteddate = {Apr., 1986},
  language = {English},
  owner = {meddwilb},
  publisher = {International Statistical Institute (ISI)},
  timestamp = {2015.07.15},
  url = {http://www.jstor.org/stable/1403259}
}

@Article{Donner1996,
  author =    {Allan Donner and Neil Klar},
  title =     {Statistical considerations in the design and analysis of community intervention trials },
  journal =   {Journal of Clinical Epidemiology },
  year =      {1996},
  volume =    {49},
  number =    {4},
  pages =     {435 - 439},
  abstract =  {Community intervention trials are often characterized by the allocation of intact social units to different intervention groups. The assessment of adequate sample size for such trials must take into account the statistical dependencies among responses observed within an allocated unit. However, the small numbers of units typically involved in such trials imply that many methods of analysis that have been proposed for analyzing correlated data, particularly in the case of a dichotomous outcome variable, are not applicable to such designs. In this article we investigate this issue and determine the minimum number of units required per group, for the case of both a dichotomous and a continuous outcome variable, needed to provide adequate statistical power for detecting various levels of treatment effect. The use of significance testing as a method of detecting intracluster correlation is also investigated, and, in general, discouraged. },
  doi =       {http://dx.doi.org/10.1016/0895-4356(95)00511-0},
  issn =      {0895-4356},
  keywords =  {Cluster randomization},
  owner =     {meddwilb},
  timestamp = {2016.02.23},
  url =       {http://www.sciencedirect.com/science/article/pii/0895435695005110}
}

@BOOK{Donner2000,
  title = {Design and Analysis of Cluster Randomization Trials in Health Research},
  publisher = {London Arnold Publishers},
  year = {2000},
  author = {Donner, Allan and Klar, Neil},
  owner = {meddwilb},
  timestamp = {2013.10.24}
}

@ARTICLE{Drummond2009,
  author = {Drummond, Colin and Coulton, Simon and James, Darren and Godfrey,
	Christine and Parrott, Steve and Baxter, John and Ford, David and
	Lervy, Bruce and Rollnick, Stephen and Russell, Ian and Peters, Timothy},
  title = {Effectiveness and cost-effectiveness of a stepped care intervention
	for alcohol use disorders in primary care: pilot study},
  journal = {The British Journal of Psychiatry},
  year = {2009},
  volume = {195},
  pages = {448-456},
  number = {5},
  abstract = {BackgroundScreening for alcohol use disorders identifies a wide range
	of needs, varying from hazardous and harmful drinking to alcohol
	dependence. Stepped care offers a potentially resource-efficient
	way of meeting these needs, but requires evaluation in a randomised
	controlled trial.AimsTo evaluate the feasibility, effectiveness and
	cost-effectiveness of opportunistic screening and a stepped care
	intervention in primary care.MethodA total of 1794 male primary care
	attendees at six practices in South Wales were screened using the
	Alcohol Use Disorders Identification Test (AUDIT). Of these, 112
	participants who scored 8 or more on the AUDIT and who consented
	to enter the study were randomised to receive either 5 minutes of
	minimal intervention delivered by a practice nurse (control group)
	or stepped care intervention consisting of three successive steps
	(intervention group): a single session of behaviour change counselling
	delivered by a practice nurse; four 50-minute sessions of motivational
	enhancement therapy delivered by a trained alcohol counsellor; and
	referral to a community alcohol treatment agency.ResultsBoth groups
	reduced alcohol consumption 6 months after randomisation with a greater,
	although not significant, improvement for the stepped care intervention.
	Motivation to change was greater following the stepped care intervention.
	The stepped care intervention resulted in greater cost savings compared
	with the minimal intervention.ConclusionsStepped care was feasible
	to implement in the primary care setting and resulted in greater
	cost savings compared with minimal intervention.},
  doi = {10.1192/bjp.bp.108.056697},
  eprint = {http://bjp.rcpsych.org/content/195/5/448.full.pdf+html},
  owner = {meddwilb},
  timestamp = {2013.09.18},
  url = {http://bjp.rcpsych.org/content/195/5/448.abstract}
}

@BOOK{Eisenhauer2006,
  title = {Phase {I} Cancer Clinical Trials},
  publisher = {Oxford University Press},
  year = {2006},
  author = {Elizabeth A. Eisenhauer and Chris Twelves and Marc Buyse},
  owner = {meddwilb},
  timestamp = {2013.11.19}
}

@ARTICLE{Eisenhauer2009,
  author = {E.A. Eisenhauer and P. Therasse and J. Bogaerts and L.H. Schwartz
	and D. Sargent and R. Ford and J. Dancey and S. Arbuck and S. Gwyther
	and M. Mooney and L. Rubinstein and L. Shankar and L. Dodd and R.
	Kaplan and D. Lacombe and J. Verweij},
  title = {New response evaluation criteria in solid tumours: Revised \{RECIST\}
	guideline (version 1.1) },
  journal = {European Journal of Cancer },
  year = {2009},
  volume = {45},
  pages = {228 - 247},
  number = {2},
  note = {Response assessment in solid tumours (RECIST): Version 1.1 and supporting
	papers },
  abstract = {Background Assessment of the change in tumour burden is an important
	feature of the clinical evaluation of cancer therapeutics: both tumour
	shrinkage (objective response) and disease progression are useful
	endpoints in clinical trials. Since \{RECIST\} was published in 2000,
	many investigators, cooperative groups, industry and government authorities
	have adopted these criteria in the assessment of treatment outcomes.
	However, a number of questions and issues have arisen which have
	led to the development of a revised \{RECIST\} guideline (version
	1.1). Evidence for changes, summarised in separate papers in this
	special issue, has come from assessment of a large data warehouse
	(&gt;6500 patients), simulation studies and literature reviews. Highlights
	of revised \{RECIST\} 1.1 Major changes include: Number of lesions
	to be assessed: based on evidence from numerous trial databases merged
	into a data warehouse for analysis purposes, the number of lesions
	required to assess tumour burden for response determination has been
	reduced from a maximum of 10 to a maximum of five total (and from
	five to two per organ, maximum). Assessment of pathological lymph
	nodes is now incorporated: nodes with a short axis of â©¾15&#xa0;mm
	are considered measurable and assessable as target lesions. The short
	axis measurement should be included in the sum of lesions in calculation
	of tumour response. Nodes that shrink to &lt;10&#xa0;mm short axis
	are considered normal. Confirmation of response is required for trials
	with response primary endpoint but is no longer required in randomised
	studies since the control arm serves as appropriate means of interpretation
	of data. Disease progression is clarified in several aspects: in
	addition to the previous definition of progression in target disease
	of 20% increase in sum, a 5&#xa0;mm absolute increase is now required
	as well to guard against over calling \{PD\} when the total sum is
	very small. Furthermore, there is guidance offered on what constitutes
	â€˜unequivocal progressionâ€™ of non-measurable/non-target disease,
	a source of confusion in the original \{RECIST\} guideline. Finally,
	a section on detection of new lesions, including the interpretation
	of FDG-PET scan assessment is included. Imaging guidance: the revised
	\{RECIST\} includes a new imaging appendix with updated recommendations
	on the optimal anatomical assessment of lesions. Future work A key
	question considered by the \{RECIST\} Working Group in developing
	\{RECIST\} 1.1 was whether it was appropriate to move from anatomic
	unidimensional assessment of tumour burden to either volumetric anatomical
	assessment or to functional assessment with \{PET\} or MRI. It was
	concluded that, at present, there is not sufficient standardisation
	or evidence to abandon anatomical assessment of tumour burden. The
	only exception to this is in the use of FDG-PET imaging as an adjunct
	to determination of progression. As is detailed in the final paper
	in this special issue, the use of these promising newer approaches
	requires appropriate clinical validation studies. },
  doi = {http://dx.doi.org/10.1016/j.ejca.2008.10.026},
  issn = {0959-8049},
  keywords = {Response criteria},
  owner = {meddwilb},
  timestamp = {2014.07.21},
  url = {http://www.sciencedirect.com/science/article/pii/S0959804908008733}
}

@ARTICLE{Eldridge2005,
  author = {Eldridge, Sandra and Spencer, Anne and Cryer, Colin and Parsons,
	Suzanne and Underwood, Martin and Feder, Gene},
  title = {Why modelling a complex intervention is an important precursor to
	trial design: lessons from studying an intervention to reduce falls-related
	injuries in older people},
  journal = {Journal of Health Services Research \& Policy},
  year = {2005},
  volume = {10},
  pages = {133-142},
  number = {3},
  abstract = {Objectives: To develop a cost-effectiveness model of a complex intervention
	from pilot study data in order to inform the viability and design
	of a subsequent falls prevention trial. Methods: We used two models;
	the first estimated the probability of falling over a 12-month period
	based on a probability tree; the second used Markov simulation to
	assess the impact of the programme over time. Results: The first
	model indicated that our intervention would reduce the proportion
	falling by only 2.8% over a 12-month period. The major reason for
	this small effect was that less than a quarter of older people at
	risk of falling were assessed using our screening tool. Even if policy-makers
	were willing to spend Â£30,000 per quality-adjusted life-year gained,
	there is only a 40% chance that the intervention would be cost-effective.
	Sensitivity analyses showed that the only scenarios that produced
	a substantial increase in the effect of the intervention were those
	in which all older people are assessed. Conclusions: The model-building
	approach described in this paper is vital when designing complex
	trials and where a trial is not possible. Information from the modelling
	can be used to re-design the intervention. The effectiveness of our
	proposed intervention appears very small due to its inability to
	reach those at risk of falling. It is most likely not to be cost-effective.
	If inability to reach the target group is a weakness common to other
	similar interventions, this suggests an area for further research.},
  doi = {10.1258/1355819054338942},
  eprint = {http://hsr.sagepub.com/content/10/3/133.full.pdf+html},
  owner = {meddwilb},
  timestamp = {2013.10.10},
  url = {http://hsr.sagepub.com/content/10/3/133.abstract}
}

@ARTICLE{Eldridge2006,
  author = {Eldridge, Sandra M and Ashby, Deborah and Kerry, Sally},
  title = {Sample size for cluster randomized trials: effect of coefficient
	of variation of cluster size and analysis method},
  journal = {International Journal of Epidemiology},
  year = {2006},
  volume = {35},
  pages = {1292-1300},
  number = {5},
  abstract = {Background Cluster randomized trials are increasingly popular. In
	many of these trials, cluster sizes are unequal. This can affect
	trial power, but standard sample size formulae for these trials ignore
	this. Previous studies addressing this issue have mostly focused
	on continuous outcomes or methods that are sometimes difficult to
	use in practice.Methods We show how a simple formula can be used
	to judge the possible effect of unequal cluster sizes for various
	types of analyses and both continuous and binary outcomes. We explore
	the practical estimation of the coefficient of variation of cluster
	size required in this formula and demonstrate the formula's performance
	for a hypothetical but typical trial randomizing UK general practices.Results
	The simple formula provides a good estimate of sample size requirements
	for trials analysed using cluster-level analyses weighting by cluster
	size and a conservative estimate for other types of analyses. For
	trials randomizing UK general practices the coefficient of variation
	of cluster size depends on variation in practice list size, variation
	in incidence or prevalence of the medical condition under examination,
	and practice and patient recruitment strategies, and for many trials
	is expected to be âˆ¼0.65. Individual-level analyses can be noticeably
	more efficient than some cluster-level analyses in this context.Conclusions
	When the coefficient of variation is <0.23, the effect of adjustment
	for variable cluster size on sample size is negligible. Most trials
	randomizing UK general practices and many other cluster randomized
	trials should account for variable cluster size in their sample size
	calculations.},
  doi = {10.1093/ije/dyl129},
  eprint = {http://ije.oxfordjournals.org/content/35/5/1292.full.pdf+html},
  owner = {meddwilb},
  timestamp = {2013.09.09},
  url = {http://ije.oxfordjournals.org/content/35/5/1292.abstract}
}

@Article{Eldridge2015,
  author    = {Eldridge, Sandra M and Costelloe, Ceire E and Kahan, Brennan C and Lancaster, Gillian A and Kerry, Sally M},
  title     = {How big should the pilot study for my cluster randomised trial be?},
  year      = {2015},
  doi       = {10.1177/0962280215588242},
  eprint    = {http://smm.sagepub.com/content/early/2015/06/12/0962280215588242.full.pdf+html},
  url       = {http://smm.sagepub.com/content/early/2015/06/12/0962280215588242.abstract},
  abstract  = {There is currently a lot of interest in pilot studies conducted in
	preparation for randomised controlled trials. This paper focuses
	on sample size requirements for external pilot studies for cluster
	randomised trials. We consider how large an external pilot study
	needs to be to assess key parameters for input to the main trial
	sample size calculation when the primary outcome is continuous, and
	to estimate rates, for example recruitment rates, with reasonable
	precision. We used simulation to provide the distribution of the
	expected number of clusters for the main trial under different assumptions
	about the natural cluster size, intra-cluster correlation, eventual
	cluster size in the main trial, and various decisions made at the
	piloting stage. We chose intra-cluster correlation values and pilot
	study size to reflect those commonly reported in the literature.
	Our results show that estimates of sample size required for the main
	trial are likely to be biased downwards and very imprecise unless
	the pilot study includes large numbers of clusters and individual
	participants. We conclude that pilot studies will usually be too
	small to estimate parameters required for estimating a sample size
	for a main cluster randomised trial (e.g. the intra-cluster correlation
	coefficient) with sufficient precision and too small to provide reliable
	estimates of rates for process measures such as recruitment or follow-up
	rates.},
  groups    = {Pilot/feasibility},
  journal   = {Statistical Methods in Medical Research},
  owner     = {meddwilb},
  timestamp = {2015.11.19},
}

@ARTICLE{Ergina2009,
  author = {Patrick L Ergina and Jonathan A Cook and Jane M Blazeby and Isabelle
	Boutron and Pierre-Alain Clavien and Barnaby C Reeves and Christoph
	M Seiler},
  title = {Challenges in evaluating surgical innovation },
  journal = {The Lancet },
  year = {2009},
  volume = {374},
  pages = {1097 - 1104},
  number = {9695},
  abstract = {Summary Research on surgical interventions is associated with several
	methodological and practical challenges of which few, if any, apply
	only to surgery. However, surgical evaluation is especially demanding
	because many of these challenges coincide. In this report, the second
	of three on surgical innovation and evaluation, we discuss obstacles
	related to the study design of randomised controlled trials and non-randomised
	studies assessing surgical interventions. We also describe the issues
	related to the nature of surgical proceduresâ€”for example, their
	complexity, surgeon-related factors, and the range of outcomes. Although
	difficult, surgical evaluation is achievable and necessary. Solutions
	tailored to surgical research and a framework for generating evidence
	on which to base surgical practice are essential. },
  doi = {http://dx.doi.org/10.1016/S0140-6736(09)61086-2},
  issn = {0140-6736},
  owner = {meddwilb},
  timestamp = {2014.07.07},
  url = {http://www.sciencedirect.com/science/article/pii/S0140673609610862}
}

@Article{Everitt2013,
  author    = {Everitt, Hazel and Moss-Morris, Rona and Sibelli, Alice and Tapp, Laura and Coleman, Nicholas and Yardley, Lucy and Smith, Peter and Little, Paul},
  title     = {Management of irritable bowel syndrome in primary care: the results of an exploratory randomised controlled trial of mebeverine, methylcellulose, placebo and a self-management website},
  journal   = {BMC Gastroenterology},
  year      = {2013},
  volume    = {13},
  number    = {1},
  pages     = {68},
  issn      = {1471-230X},
  doi       = {10.1186/1471-230X-13-68},
  url       = {http://www.biomedcentral.com/1471-230X/13/68},
  abstract  = {BACKGROUND:Many patients with IBS suffer on-going symptoms. The evidence
	base is poor for IBS drugs but they are widely prescribed and advised
	in Guidelines. Cognitive Behavioural Therapy (CBT) can be helpful,
	but availability is poor in the NHS. We developed a web-based CBT
	self-management programme (Regul8) in partnership with patients and
	trialled it and common IBS medications in an exploratory factorial
	RCT to test trial procedures and provide information for a larger
	trial.METHODS:Patients, 16 to 60years, with IBS symptoms fulfilling
	Rome III criteria were recruited via GP practices and randomised
	to over-encapsulated mebeverine, methylcellulose or placebo for 6weeks
	and to 1 of 3 website conditions: Regul8 with a nurse telephone session
	and email support, Regul8 with minimal email support, or no website.RESULTS:135
	patients recruited from 26 GP practices. Mean IBS SSS score 241.9
	(sd 87.7), IBS-QOL 64 (sd 20) at baseline. 91% follow-up at 12weeks.
	Mean IBS SSS decreased by 35 points from baseline to 12weeks. There
	was no significant difference in IBS SSS or IBS-QOL score between
	medication or website groups at 12weeks, or in medication groups
	at 6weeks, or IBS-QOL in website groups at 6weeks. However, IBS SSS
	at 6weeks was lower in the No website group than the website groups
	(IBS SSS no website =162.8 (95% CI 137.4-188.3), website 197.0 (172.4
	- 221.7), Website + telephone support 208.0 (183.1-233.0) p=0.037).Enablement
	and Subjects Global Assessment of relief (SGA) were significantly
	improved in the Regul8 groups compared to the non-website group at
	12weeks (Enablement =0 in 56.8% of No website group, 18.4% website,
	10.5% Website + support, p=0.001) (SGA; 32.4% responders in No website
	group, 45.7% website group, 63.2% website + support group, p=0.035).CONCLUSIONS:This
	exploratory study demonstrates feasibility and high follow-up rates
	and provides information for a larger trial. Primary outcomes (IBS
	SS and IBS QOL) did not reach significance at 6 or 12weeks, apart
	from IBS SSS being lower in the no-website group at 6weeks - this
	disappeared by 12weeks. Improved Enablement suggests patients with
	access to the Regul8 website felt better able to cope with their
	symptoms than the non-website group. Improved SGA score in the Regul8
	groups may indicate some overall improvement not captured on other
	measures.TRIAL REGISTRATION:ClinicalTrials.gov Identifier (NCT number):
	NCT00934973.},
  comment   = {"Data screening showed that the assumptions of normality were met
	for the distributions of all the variables" including IBS-SSS (possibly
	as is, possibly change too)},
  owner     = {meddwilb},
  pubmedid  = {23602047},
  timestamp = {2014.06.12},
}

@ARTICLE{Faes2010,
  author = {Faes, Miriam C. and Reelick, Miriam F. and Esselink, Rianne A. and
	Rikkert, Marcel G. Olde},
  title = {Developing and Evaluating Complex Healthcare Interventions in Geriatrics:
	The Use of the Medical Research Council Framework Exemplified on
	a Complex Fall Prevention Intervention},
  journal = {Journal of the American Geriatrics Society},
  year = {2010},
  volume = {58},
  pages = {2212--2221},
  number = {11},
  doi = {10.1111/j.1532-5415.2010.03108.x},
  issn = {1532-5415},
  keywords = {complex interventions, frail older persons, development, evaluation,
	MRC framework},
  owner = {meddwilb},
  publisher = {Blackwell Publishing Inc},
  timestamp = {2013.09.17},
  url = {http://dx.doi.org/10.1111/j.1532-5415.2010.03108.x}
}

@ARTICLE{Farquhar2010,
  author = {Farquhar,Morag and Higginson,Irene J. and Fagan,Petrea and Booth,Sara},
  title = {Results of a pilot investigation into a complex intervention for
	breathlessness in advanced chronic obstructive pulmonary disease
	(COPD): Brief report},
  journal = {Palliative \& Supportive Care},
  year = {2010},
  volume = {8},
  pages = {143--149},
  month = {6},
  doi = {10.1017/S1478951509990897},
  issn = {1478-9523},
  issue = {02},
  numpages = {7},
  owner = {meddwilb},
  timestamp = {2013.09.18}
}

@ARTICLE{Farrin2011,
  author = {Farrin, Amanda and Collinson, Michelle},
  title = {Designing and analysing pragmatic clinical trials of complex interventions
	in mental health: challenges and solutions},
  journal = {Trials},
  year = {2011},
  volume = {12},
  pages = {A149},
  number = {Suppl 1},
  doi = {10.1186/1745-6215-12-S1-A149},
  issn = {1745-6215},
  owner = {meddwilb},
  timestamp = {2014.03.25},
  url = {http://www.trialsjournal.com/content/12/S1/A149}
}

@ARTICLE{Farrin2005,
  author = {Farrin, Amanda and Russell, Ian and Torgerson, David and Underwood,
	Martin},
  title = {Differential recruitment in a cluster randomized trial in primary
	care: the experience of the UK Back pain, Exercise, Active management
	and Manipulation (UK BEAM) feasibility study},
  journal = {Clinical Trials},
  year = {2005},
  volume = {2},
  pages = {119-124},
  number = {2},
  abstract = {Background Cluster randomized trials, which randomize groups of patients
	rather than individuals, are commonly used to evaluate healthcare
	interventions such as training programmes targeted at health professionals.
	This article reports the dangers of randomizing entire primary care
	practices when participants cannot be identified before randomization,
	as shown by a UK national trial. Method The UK BEAM trial, a national
	cluster randomized 322 factorial trial, was designed to evaluate
	three treatments for back pain in primary care: â€œactive managementâ€;
	randomized by practice; and spinal manipulation and exercise classes,
	both randomized by individual. Results Two hundred and thirty-one
	participants were recruited in the feasibility study, 165 (141% of
	expected recruitment) from active (management) practices but only
	66 (54% of expected recruitment) from traditional (management) practices.
	The participants in active practices were significantly different
	from those in traditional practices, notably in suffering from milder
	back pain. Conclusions The feasibility study highlighted the dangers
	of randomizing clusters when individuals cannot be identified beforehand.
	Different numbers and types of participants were recruited in the
	two types of cluster. This differential recruitment led us to change
	the main trial design by abandoning practice level randomization.
	Instead all practices were trained in active management to maximize
	recruitment. Ideally cluster randomized trials should identify patients
	beforehand, to minimize the chance of selection bias. If this is
	not possible, patient recruitment should be independent in both intervention
	and control clusters. Pilot studies are especially important for
	cluster randomized trials, to identify unforeseen problems.},
  doi = {10.1191/1740774505cn073oa},
  eprint = {http://ctj.sagepub.com/content/2/2/119.full.pdf+html},
  owner = {meddwilb},
  timestamp = {2014.03.25},
  url = {http://ctj.sagepub.com/content/2/2/119.abstract}
}

@Article{Farrow2006,
  author    = {Malcolm Farrow and Michael Goldstein},
  title     = {Trade-off sensitive experimental design: a multicriterion, decision theoretic, Bayes linear approach},
  year      = {2006},
  volume    = {136},
  number    = {2},
  pages     = {498 - 526},
  issn      = {0378-3758},
  doi       = {http://dx.doi.org/10.1016/j.jspi.2004.07.008},
  url       = {http://www.sciencedirect.com/science/article/pii/S0378375804003179},
  abstract  = {We show how mutually utility independent hierarchies, which weigh
	the various costs of an experiment against benefits expressed through
	a mixed Bayes linear utility representing the potential gains in
	knowledge from the experiment, provide a flexible and intuitive methodology
	for experimental design which remains tractable even for complex
	multivariate problems. A key feature of the approach is that we allow
	imprecision in the trade-offs between the various costs and benefits.
	We identify the Pareto optimal designs under the imprecise specification
	and suggest a criterion for selecting between such designs. The approach
	is illustrated with respect to an experiment related to the oral
	glucose tolerance test. },
  groups    = {Decision theory},
  journal   = {Journal of Statistical Planning and Inference},
  keywords  = {Imprecise utility},
  owner     = {meddwilb},
  timestamp = {2015.05.13},
}

@Article{Feng2001,
  author =    {Ziding Feng and Paula Diehr and Arthur Peterson and Dale McLerran},
  title =     {Selected Statistical Issues in Group Randomized Trials},
  journal =   {Annual Review of Public Health},
  year =      {2001},
  volume =    {22},
  number =    {1},
  pages =     {167-187},
  note =      {PMID: 11274517},
  abstract =  { â–ª Abstractâ€‚Group randomized trials (GRTs) in public health research typically use a small number of randomized groups with a relatively large number of participants per group. Two fundamental features characterize GRTs: a positive correlation of outcomes within a group, and the small number of groups. Appropriate consideration of these fundamental features is essential for design and analysis. This paper presents the fundamental features of GRTs and the importance of considering these features in design and analysis. It also reviews and contrasts the main analytic methods proposed for GRTs, emphasizing the assumptions required to make these methods valid and efficient. Also discussed are various design issues, along with guidelines for choosing among them. A real data example illustrates these issues and methods. },
  doi =       {10.1146/annurev.publhealth.22.1.167},
  eprint =    { http://dx.doi.org/10.1146/annurev.publhealth.22.1.167 },
  owner =     {meddwilb},
  timestamp = {2016.07.05},
  url =       { 
        http://dx.doi.org/10.1146/annurev.publhealth.22.1.167
    
}
}

@ARTICLE{Fisher1939,
  author = {R. A. Fisher},
  title = {Student},
  journal = {Annals of Eugenics},
  year = {1939},
  volume = {9},
  pages = {1-9},
  owner = {meddwilb},
  timestamp = {2014.04.23}
}

@Article{Fisher1955,
  author    = {Ronald Fisher},
  title     = {Statistical Methods and Scientific Induction},
  journal   = {Journal of the Royal Statistical Society. Series B (Methodological)},
  year      = {1955},
  volume    = {17},
  number    = {1},
  pages     = {69-78},
  issn      = {00359246},
  url       = {http://www.jstor.org/stable/2983785},
  abstract  = {The attempt to reinterpret the common tests of significance used in scientific research as though they constituted some kind of acceptance procedure and led to "decisions" in Wald's sense, originated in several misapprehensions and has led, apparently, to several more. The three phrases examined here, with a view to elucidating the fallacies they embody, are: (i) "Repeated sampling from the same population", (ii) Errors of the "second kind", (iii) "Inductive behaviour". Mathematicians without personal contact with the Natural Sciences have often been misled by such phrases. The errors to which they lead are not always only numerical.},
  publisher = {[Royal Statistical Society, Wiley]},
}

@ARTICLE{Fleming1982,
  author = {Fleming, Thomas R.},
  title = {One-Sample Multiple Testing Procedure for Phase {II} Clinical Trials},
  journal = {Biometrics},
  year = {1982},
  volume = {38},
  pages = {143-151},
  number = {1},
  abstract = {Commonly, the central objective of Phase II clinical trials is the
	assessment of the antitumor 'therapeutic efficacy' of a specific
	treatment regimen. It is of interest to formulate test procedures
	which can be employed in these trials to decide whether or not this
	therapeutic efficacy warrants further investigation. For ethical
	reasons, these procedures should allow for early termination if initial
	results are extreme. In this paper, a one-sample multiple testing
	procedure is proposed which employs the standard single-stage test
	procedure at the last test, and which both allows for early termination
	and essentially preserves the size, power and simplicity of the single-stage
	procedure.},
  copyright = {Copyright Â© 1982 International Biometric Society},
  issn = {0006341X},
  jstor_articletype = {research-article},
  jstor_formatteddate = {Mar., 1982},
  language = {English},
  owner = {meddwilb},
  publisher = {International Biometric Society},
  timestamp = {2013.09.26},
  url = {http://www.jstor.org/stable/2530297}
}

@ARTICLE{Ford2010,
  author = {Ford, A. C. and Moayyedi, P.},
  title = {Meta-analysis: factors affecting placebo response rate in the irritable
	bowel syndrome},
  journal = {Alimentary Pharmacology \& Therapeutics},
  year = {2010},
  volume = {32},
  pages = {144--158},
  number = {2},
  abstract = {Aliment Pharmacol Ther 2010; 32: 144â€“158SummaryBackgroundâ€‚ Irritable
	bowel syndrome (IBS) is a functional disorder of the gastrointestinal
	tract with a significant placebo response.Aimâ€‚ To conduct a systematic
	review and meta-analysis examining the magnitude of placebo response
	rate in treatment trials for IBS.Methodsâ€‚ MEDLINE, EMBASE and the
	Cochrane central register of controlled trials were searched to identify
	randomized controlled trials (RCTs) comparing pharmacological therapies
	with placebo in adult IBS patients. Studies reported either global
	assessment of IBS symptom cure or improvement or abdominal pain cure
	or improvement. Data were extracted as intention-to-treat analyses
	with drop-outs assumed to be treatment failures and pooled using
	a random-effects model. Proportion of placebo patients experiencing
	symptom improvement or resolution was reported with a 95% confidence
	interval (CI). Effect of trial characteristics on magnitude of placebo
	response was examined.Resultsâ€‚ In all, 73 RCTs were eligible, including
	8364 patients with IBS allocated to placebo. Pooled placebo response
	rate across all RCTs was 37.5% (95% CI 34.4â€“40.6%). Rates were
	higher in European RCTs, RCTs that used physician-reported outcomes
	and RCTs using shorter duration of therapy.Conclusionsâ€‚ Placebo
	response rates across RCTs of pharmacological therapies in IBS were
	high. Future research should identify patient characteristics predicting
	placebo response.},
  doi = {10.1111/j.1365-2036.2010.04328.x},
  issn = {1365-2036},
  owner = {meddwilb},
  publisher = {Blackwell Publishing Ltd},
  timestamp = {2014.06.16},
  url = {http://dx.doi.org/10.1111/j.1365-2036.2010.04328.x}
}

@ARTICLE{Ford2009,
  author = {Ford, A C and Talley, N J and Schoenfeld, P S and Quigley, E M M
	and Moayyedi, P},
  title = {Efficacy of antidepressants and psychological therapies in irritable
	bowel syndrome: systematic review and meta-analysis},
  journal = {Gut},
  year = {2009},
  volume = {58},
  pages = {367-378},
  number = {3},
  abstract = {Objective: Irritable bowel syndrome (IBS) is a chronic functional
	gastrointestinal disorder. Evidence for treatment of the condition
	with antidepressants and psychological therapies is conflicting.Design:
	Systematic review and meta-analysis of randomised controlled trials
	(RCTs). MEDLINE, EMBASE and the Cochrane Controlled Trials Register
	were searched (up to May 2008).Setting: RCTs based in primary, secondary
	and tertiary care.Patients: Adults with IBS.Interventions: Antidepressants
	versus placebo, and psychological therapies versus control therapy
	or “usual management”.Main outcome measures: Dichotomous symptom
	data were pooled to obtain a relative risk (RR) of remaining symptomatic
	after therapy, with a 95% confidence interval (CI). The number needed
	to treat (NNT) was calculated from the reciprocal of the risk difference.Results:
	The search strategy identified 571 citations. Thirty-two RCTs were
	eligible for inclusion: 19 compared psychological therapies with
	control therapy or “usual management”, 12 compared antidepressants
	with placebo, and one compared both psychological therapy and antidepressants
	with placebo. Study quality was generally good for antidepressant
	but poor for psychological therapy trials. The RR of IBS symptoms
	persisting with antidepressants versus placebo was 0.66 (95% CI,
	0.57 to 0.78), with similar treatment effects for both tricyclic
	antidepressants and selective serotonin reuptake inhibitors. The
	RR of symptoms persisting with psychological therapies was 0.67 (95%
	CI, 0.57 to 0.79). The NNT was 4 for both interventions.Conclusions:
	Antidepressants are effective in the treatment of IBS. There is less
	high-quality evidence for routine use of psychological therapies
	in IBS, but available data suggest these may be of comparable efficacy.},
  doi = {10.1136/gut.2008.163162},
  eprint = {http://gut.bmj.com/content/58/3/367.full.pdf+html},
  owner = {meddwilb},
  timestamp = {2014.06.03},
  url = {http://gut.bmj.com/content/58/3/367.abstract}
}

@Article{Friede2006,
  author    = {Tim Friede and Meinhard Kieser},
  title     = {Sample Size Recalculation in Internal Pilot Study Designs: A Review},
  year      = {2006},
  volume    = {48},
  number    = {4},
  month     = {aug},
  pages     = {537--555},
  doi       = {10.1002/bimj.200510238},
  url       = {http://dx.doi.org/10.1002/bimj.200510238},
  groups    = {Pilot/feasibility, SSR},
  journal   = {Biom. J.},
  owner     = {meddwilb},
  publisher = {Wiley-Blackwell},
  timestamp = {2016.07.13},
}

@Article{Garaventa2003,
  author    = {Garaventa, Alberto and Luksch, Roberto and Biasotti, Simona and Severi, Gianluca and Pizzitola, Maria Rosa and Viscardi, Elisabetta and Prete, Arcangelo and Mastrangelo, Stefano and Podda, Marta and Haupt, Riccardo and De Bernardi, Bruno},
  title     = {A phase {II} study of topotecan with vincristine and doxorubicin in children with recurrent/refractory neuroblastoma},
  journal   = {Cancer},
  year      = {2003},
  volume    = {98},
  number    = {11},
  pages     = {2488--2494},
  issn      = {1097-0142},
  doi       = {10.1002/cncr.11797},
  url       = {http://dx.doi.org/10.1002/cncr.11797},
  abstract  = {BACKGROUNDA Phase II trial in children with advanced neuroblastoma
	was carried out in five Italian institutions to evaluate the antitumor
	activity and tolerability of topotecan followed by vincristine and
	doxorubicin.METHODSChildren older than age 1 year with Stage III
	or Stage IV neuroblastoma, all of whom had been treated previously
	with chemotherapy and were diagnosed with either refractory or recurrent
	disease, were treated with topotecan at an intravenous dose of 1.5
	mg/m2 (the dose was 0.75 mg/m2 for patients who were treated within
	1 year of previous megatherapy) per day for 5 days followed by 48-hour
	intravenous infusions of 2 mg/m2 vincristine and 45 mg/m2 doxorubicin.
	Cycles of therapy were repeated every 3 weeks.RESULTSTwenty-five
	patients (2 with Stage III disease and 23 with Stage IV disease;
	19 with refractory disease and 6 with recurrent disease) were treated
	with a total of 115 cycles. Four patients had complete responses,
	12 patients had partial responses, 4 patients had minor responses
	or stable disease, and 5 patients had tumor progression. The overall
	response rate (including complete and partial responses) was 64%
	(95% confidence interval, 43â€“82%). Fifteen patients were alive
	at the time of the current report and were progression free at 4â€“16
	months (median, 9 months) after the first course of this treatment.
	Toxicity generally was limited to the hematopoietic system. Dose-limiting
	toxicity was observed in only 1 patient (Grade 4 liver toxicity).
	There were no deaths due to infectious or toxic causes.CONCLUSIONSThe
	topotecan-vincristine-doxorubicin combination was active and well
	tolerated in previously treated patients with advanced neuroblastoma.
	Cancer 2003. Â© 2003 American Cancer Society.},
  comment   = {Design described in paper gives
	
	
	[1] 0.05 0.25 0.10 0.40
	
	[1] 0.1 0.1 0.1
	
	
	[1] 0 7 10 2 17 24
	
	[1] "Expected size: 89983.0876210938"
	
	[1] "alpha_r: 2.87354000552099e-05"
	
	[1] "alpha_t: 3.4865878688745e-14"
	
	[1] "power: 0.000269814458091044"0.05 0.25 0.10 0.40
	
	
	*Seems the authors have missinterpreted the notation used in the B&D
	design. Asking for no more than 6 toxicities when H0 is 90%? In the
	event, only one toxicity was observed, suggesting their belief begarding
	likely toxicity rates was some way off.
	
	
	SSS gives
	
	
	[1] 1 2 12 3 5 23
	
	[1] "Expected size: 19963.8568357249"
	
	[1] "alpha_r: 0.0176487926669942"
	
	[1] "alpha_t: 0.0135188453938604"
	
	[1] "power: 0.700493364829644"
	
	
	Local search gives
	
	
	[1] 0 1 12 2 4 23
	
	[1] "Expected size: 16.9569859675434"
	
	[1] "alpha_r: 0.0938157739665672"
	
	[1] "alpha_t: 0.0610128271044585"
	
	[1] "power: 0.903436295769069"},
  keywords  = {Phase II, neuroblastoma, topotecan, vincristine, doxorubicin},
  owner     = {meddwilb},
  publisher = {Wiley Subscription Services, Inc., A Wiley Company},
  timestamp = {2014.02.28},
}

@ARTICLE{Garratt2001,
  author = {Garratt, Andrew and Klaber Moffett, Jennifer and Farrin, Amanda J.},
  title = {Responsiveness of Generic and Specific Measures of Health Outcome
	in Low Back Pain},
  journal = {Spine},
  year = {2001},
  volume = {26},
  pages = {71-77},
  number = {1},
  doi = {10.1097/00007632-200101010-00014},
  owner = {meddwilb},
  timestamp = {2014.03.25}
}

@ARTICLE{Gega2012,
  author = {Gega, Lina and Swift, Louise and Barton, Garry and Todd, Gillian
	and Reeve, Nesta and Bird, Kelly and Holland, Richard and Howe, Amanda
	and Wilson, Jon and Molle, Jo},
  title = {Computerised therapy for depression with clinician vs. assistant
	and brief vs. extended phone support: study protocol for a randomised
	controlled trial},
  journal = {Trials},
  year = {2012},
  volume = {13},
  pages = {151},
  number = {1},
  abstract = {BACKGROUND:Computerised cognitive behaviour therapy (cCBT) involves
	standardised, automated, interactive self-help programmes delivered
	via a computer. Randomised controlled trials (RCTs) and observational
	studies have shown than cCBT reduces depressive symptoms as much
	as face-to-face therapy and more than waiting lists or treatment
	as usual. cCBT's efficacy and acceptability may be influenced by
	the "human" support offered as an adjunct to it, which can vary in
	duration and can be offered by people with different levels of training
	and expertise.METHODS/DESIGN:This is a two-by-two factorial RCT investigating
	the effectiveness, cost-effectiveness and acceptability of cCBT supplemented
	with 12 weekly phone support sessions are either brief (5-10 min)
	or extended (20-30 min) and are offered by either an expert clinician
	or an assistant with no clinical training. Adults with non-suicidal
	depression in primary care can self-refer into the study by completing
	and posting to the research team a standardised questionnaire. Following
	an assessment interview, eligible referrals have access to an 8-session
	cCBT programme called Beating the Blues and are randomised to one
	of four types of support: brief-assistant, extended-assistant, brief-clinician
	or extended-clinician.A sample size of 35 per group (total 140) is
	sufficient to detect a moderate effect size with 90% power on our
	primary outcome measure (Work and Social Adjustment Scale); assuming
	a 30% attrition rate, 200 patients will be randomised. Secondary
	outcome measures include the Beck Depression and Anxiety Inventories
	and the PHQ-9 and GAD-7. Data on clinical outcomes, treatment usage
	and patient experiences are collected in three ways: by post via
	self-report questionnaires at week 0 (randomisation) and at weeks
	12 and 24 post-randomisation; electronically by the cCBT system every
	time patients log-in; by phone during assessments, support sessions
	and exit interviews.DISCUSSION:The study's factorial design increases
	its efficiency by allowing the concurrent investigation of two types
	of adjunct support for cCBT with a single sample of participants.
	Difficulties in recruitment, uptake and retention of participants
	are anticipated because of the nature of the targeted clinical problem
	(depression impairs motivation) and of the studied interventions
	(lack of face-to-face contact because referrals, assessments, interventions
	and data collection are completed by phone, computer or post).TRIAL
	REGISTRATION:Current Controlled Trials ISRCTN98677176},
  doi = {10.1186/1745-6215-13-151},
  issn = {1745-6215},
  owner = {meddwilb},
  pubmedid = {22925596},
  timestamp = {2014.07.16},
  url = {http://www.trialsjournal.com/content/13/1/151}
}

@ARTICLE{Gehan1961,
  author = {Edmund A. Gehan},
  title = {The determination of the number of patients required in a preliminary
	and a follow-up trial of a new chemotherapeutic agent },
  journal = {Journal of Chronic Diseases },
  year = {1961},
  volume = {13},
  pages = {346 - 353},
  number = {4},
  doi = {http://dx.doi.org/10.1016/0021-9681(61)90060-1},
  issn = {0021-9681},
  owner = {meddwilb},
  timestamp = {2013.09.16},
  url = {http://www.sciencedirect.com/science/article/pii/0021968161900601}
}

@Manual{Ginsbourger2013,
  author    = {D. Ginsbourger and V. Picheny and O. Roustant and with contributions by C. Chevalier and T. Wagner},
  title     = {DiceOptim: Kriging-based optimization for computer experiments},
  year      = {2013},
  note      = {R package version 1.4},
  url       = {http://CRAN.R-project.org/package=DiceOptim},
  groups    = {Optimisation},
  owner     = {meddwilb},
  timestamp = {2014.02.17},
}

@ARTICLE{Gittins2000,
  author = {Gittins, John and Pezeshk, Hamid},
  title = {How Large Should a Clinical Trial Be?},
  journal = {Journal of the Royal Statistical Society: Series D (The Statistician)},
  year = {2000},
  volume = {49},
  pages = {177--187},
  number = {2},
  abstract = {The determination of the optimal sample size of a clinical trial is
	considered when the number of subsequent users of a new treatment
	is a function of both the statistical significance of the difference
	and of the magnitude of the apparent difference between the performance
	of the new treatment and that of the treatment in current use. An
	objective function is proposed consisting of the total benefit from
	the resulting change in the number of patients using the new treatment
	minus the cost of the trial. From this the optimal sample size may
	be calculated. The model has features which allow for the following
	contingencies: a cost difference between the two treatments; a pay-off
	function defined either from the public health or from a drug company
	standpoint; the performance of the existing treatment is either known
	or unknown; varying degrees of severity of the condition to be treated;
	a set-up cost for conducting the trial.},
  doi = {10.1111/1467-9884.00228},
  issn = {1467-9884},
  keywords = {Bayesian approach, Clinical trials, Maximum expected net benefit,
	Sample size determination, Subsequent users of therapy},
  owner = {meddwilb},
  publisher = {Blackwell Publishers Ltd},
  timestamp = {2015.05.13},
  url = {http://dx.doi.org/10.1111/1467-9884.00228}
}

@Article{Gittins2000a,
  author    = {Gittins, John and Pezeshk, Hamid},
  title     = {A Behavioral Bayes Method for Determining the Size of a Clinical Trial},
  year      = {2000},
  volume    = {34},
  number    = {2},
  pages     = {355-363},
  doi       = {10.1177/009286150003400204},
  eprint    = {http://dij.sagepub.com/content/34/2/355.full.pdf+html},
  url       = {http://dij.sagepub.com/content/34/2/355.abstract},
  abstract  = {In this paper we introduce a fully Bayesian approach to sample size determination in clinical trials. In contrast to the usual Bayesian decision theoretic methodology, which assumes a single decision maker, our approach recognizes the existence of three decision makers, namely: the pharmaceutical company conducting the trial, which decides on its size; the regulator, whose approval is necessary for the drug to be licensed for sale; and the public at large, who determine ultimate usage. Moreover, we model the subsequent usage by plausible assumptions for actual behavior, rather than assuming that it represents decisions which are in some sense optimal.The results, not surprisingly, show that the optimal sample size depends strongly on the expected benefit from a conclusively favorable outcome, and on the strength of the evidence required by the regulator.},
  groups    = {Bayesian, Decision theory},
  journal   = {Drug Information Journal},
  owner     = {meddwilb},
  timestamp = {2016.07.06},
}

@Article{Gittins2002,
  author    = {J. C. Gittins and H. Pezeshk},
  title     = {A decision theoretic approach to sample size determination in clinical trials},
  year      = {2002},
  volume    = {12},
  number    = {4},
  pages     = {535-551},
  note      = {PMID: 12477073},
  doi       = {10.1081/BIP-120016234},
  eprint    = { http://dx.doi.org/10.1081/BIP-120016234 },
  url       = { http://dx.doi.org/10.1081/BIP-120016234 
},
  abstract  = { In this paper, we discuss a Behavioral Bayes approach to the determination
	of sample size in phase III clinical trials for which the data are
	assumed to come from a normal distribution for which the mean and
	variance are both unknown. Software is described which minimizes
	the expected net cost as a function of the sample size, thereby establishing
	the optimal sample size. This methodology extends previous work by
	the assumption of unknown variance. Numerical examples show that
	the more general model can have a large effect on the optimal sample
	size, compared with a procedure which uses the known variance model
	with an estimate of the variance. },
  groups    = {SSR},
  journal   = {Journal of Biopharmaceutical Statistics},
  owner     = {meddwilb},
  timestamp = {2015.05.13},
}

@BOOK{Goldstein2003,
  title = {Multilevel Statistical Models},
  publisher = {Arnold},
  year = {2003},
  author = {Harvey Goldstein},
  edition = {3rd},
  owner = {meddwilb},
  timestamp = {2013.11.22}
}

@Article{Goldstein2002,
  author    = {Goldstein, Harvey and Browne, William and Rasbash, Jon},
  title     = {Multilevel modelling of medical data},
  year      = {2002},
  volume    = {21},
  number    = {21},
  pages     = {3291--3315},
  issn      = {1097-0258},
  doi       = {10.1002/sim.1264},
  url       = {http://dx.doi.org/10.1002/sim.1264},
  abstract  = {This tutorial presents an overview of multilevel or hierarchical data
	modelling and its applications in medicine. A description of the
	basic model for nested data is given and it is shown how this can
	be extended to fit flexible models for repeated measures data and
	more complex structures involving cross-classifications and multiple
	membership patterns within the software package MLwiN. A variety
	of response types are covered and both frequentist and Bayesian estimation
	methods are described. Copyright Â© 2002 John Wiley & Sons, Ltd.},
  groups    = {PACE},
  journal   = {Statistics in Medicine},
  keywords  = {complex data structures, mixed model, multilevel model, random effects model, repeated measures},
  owner     = {meddwilb},
  publisher = {John Wiley \& Sons, Ltd.},
  timestamp = {2015.01.26},
}

@ARTICLE{Goodman1998,
  author = {Goodman, Steven N.},
  title = {Multiple Comparisons, Explained},
  journal = {American Journal of Epidemiology},
  year = {1998},
  volume = {147},
  pages = {807-812},
  number = {9},
  eprint = {http://aje.oxfordjournals.org/content/147/9/807.full.pdf+html},
  owner = {meddwilb},
  timestamp = {2014.04.24},
  url = {http://aje.oxfordjournals.org/content/147/9/807.short}
}

@ARTICLE{Goodman1999,
  author = {Goodman, Steven N.},
  title = {Toward Evidence-Based Medical Statistics. 1: The P Value Fallacy},
  journal = {Annals of Internal Medicine},
  year = {1999},
  volume = {130},
  pages = {995-1004},
  number = {12},
  doi = {10.7326/0003-4819-130-12-199906150-00008},
  file = {:P\:\\CTRU\\Projects\\Health_Sciences\\Methodology\\2013 NIHR Research Methods Fellowship\\Literature\\Papers\\Goodman1999.pdf:PDF},
  owner = {meddwilb},
  timestamp = {2013.10.17},
  url = {http://dx.doi.org/10.7326/0003-4819-130-12-199906150-00008}
}

@ARTICLE{Goodman1999a,
  author = {Goodman, Steven N.},
  title = {Toward Evidence-Based Medical Statistics. 2: The Bayes Factor},
  journal = {Annals of Internal Medicine},
  year = {1999},
  volume = {130},
  pages = {1005-1013},
  number = {12},
  doi = {10.7326/0003-4819-130-12-199906150-00019},
  file = {:P\:\\CTRU\\Projects\\Health_Sciences\\Methodology\\2013 NIHR Research Methods Fellowship\\Literature\\Papers\\Goodman1999a.pdf:PDF},
  owner = {meddwilb},
  timestamp = {2013.10.17},
  url = {+ http://dx.doi.org/10.7326/0003-4819-130-12-199906150-00019}
}

@Article{Green2014,
  author    = {Helen Green and Michael Barkham and Stephen Kellett and David Saxon},
  title     = {Therapist effects and {IAPT} Psychological Wellbeing Practitioners (PWPs): A multilevel modelling and mixed methods analysis},
  year      = {2014},
  volume    = {63},
  number    = {0},
  pages     = {43 - 54},
  issn      = {0005-7967},
  doi       = {http://dx.doi.org/10.1016/j.brat.2014.08.009},
  url       = {http://www.sciencedirect.com/science/article/pii/S0005796714001387},
  abstract  = {Abstract The aim of this research was (a) to determine the extent
	of therapist effects in Psychological Wellbeing Practitioners (PWPs)
	delivering guided self-help in \{IAPT\} services and (b) to identify
	factors that defined effective \{PWP\} clinical practice. Using patient
	(NÂ =Â 1122) anxiety and depression outcomes (PHQ-9 and GAD-7), the
	effectiveness of NÂ =Â 21 \{PWPs\} across 6 service sites was examined
	using multi-level modelling. \{PWPs\} and their clinical supervisors
	were also interviewed and completed measures of ego strength, intuition
	and resilience. Therapist effects accounted for around 9 per cent
	of the variance in patient outcomes. One \{PWP\} had significantly
	better than average outcomes on both PHQ-9 and GAD-7 while 3 \{PWPs\}
	were significantly below average on the PHQ-9 and 2 were below average
	on the GAD-7. Computed \{PWP\} ranks identified quartile clusters
	of the most (NÂ =Â 5) and least (NÂ =Â 5) effective PWPs. More effective
	\{PWPs\} generated higher rates of reliable and clinically significant
	change and displayed greater resilience, organisational abilities,
	knowledge and confidence. Study weaknesses are identified and methodological
	considerations for future studies examining therapist effects in
	low intensity cognitive behaviour therapy are provided. },
  groups    = {Barkham},
  journal   = {Behaviour Research and Therapy},
  keywords  = {Therapist effects},
  owner     = {meddwilb},
  timestamp = {2015.01.23},
}

@ARTICLE{Greenland1991,
  author = {Greenland, Sander and Robins, James M.},
  title = {Empirical-Bayes Adjustments for Multiple Comparisons Are Sometimes
	Useful},
  journal = {Epidemiology},
  year = {1991},
  volume = {2},
  pages = {pp. 244-251},
  number = {4},
  abstract = {Rothman recommends against adjustments for multiple comparisons. Implicit
	in his recommendation, however, is an assumption that the sole objective
	of the data analysis is to report and scientifically interpret the
	data. We concur with his recommendation when this assumption is correct
	and one is willing to abandon frequentist interpretations of the
	summary statistics. Nevertheless, there are situations in which an
	additional or even primary goal of analysis is to reach a set of
	decisions based on the data. In such situations, Bayes and empirical-Bayes
	adjustments can provide a better basis for the decisions than conventional
	procedures.},
  copyright = {Copyright Â© 1991 Lippincott Williams & Wilkins},
  issn = {10443983},
  jstor_articletype = {research-article},
  jstor_formatteddate = {Jul., 1991},
  language = {English},
  owner = {meddwilb},
  publisher = {Lippincott Williams \& Wilkins},
  timestamp = {2014.05.12},
  url = {http://www.jstor.org/stable/20065674}
}

@ARTICLE{Grouin2007,
  author = {Grouin, Jean-Marie and Coste, Maylis and Bunouf, Pierre and Lecoutre,
	Bruno},
  title = {Bayesian sample size determination in non-sequential clinical trials:
	Statistical aspects and some regulatory considerations},
  journal = {Statistics in Medicine},
  year = {2007},
  volume = {26},
  pages = {4914--4924},
  number = {27},
  abstract = {The most common Bayesian methods for sample size determination (SSD)
	are reviewed in the non-sequential context of a confirmatory phase
	III trial in drug development. After recalling the regulatory viewpoint
	on SSD, we discuss the relevance of the various priors applied to
	the planning of clinical trials. We then investigate whether these
	Bayesian methods could compete with the usual frequentist approach
	to SSD and be considered as acceptable from a regulatory viewpoint.
	Copyright Â© 2007 John Wiley & Sons, Ltd.},
  doi = {10.1002/sim.2958},
  issn = {1097-0258},
  keywords = {sample size determination, Bayesian methods, regulatory viewpoint},
  owner = {meddwilb},
  publisher = {John Wiley \& Sons, Ltd.},
  timestamp = {2015.05.13},
  url = {http://dx.doi.org/10.1002/sim.2958}
}

@Article{Guittet2005,
  author =    {Guittet, Lydia and Giraudeau, Bruno and Ravaud, Philippe},
  title =     {A priori postulated and real power in cluster randomized trials: mind the gap},
  journal =   {BMC Medical Research Methodology},
  year =      {2005},
  volume =    {5},
  number =    {1},
  pages =     {1--7},
  abstract =  {Cluster randomization design is increasingly used for the evaluation of health-care, screening or educational interventions. The intraclass correlation coefficient (ICC) defines the clustering effect and be specified during planning. The aim of this work is to study the influence of the ICC on power in cluster randomized trials.},
  doi =       {10.1186/1471-2288-5-25},
  issn =      {1471-2288},
  owner =     {meddwilb},
  timestamp = {2016.07.05},
  url =       {http://dx.doi.org/10.1186/1471-2288-5-25}
}

@Article{Halpern2001,
  author    = {Halpern, Jerry and Brown, Byron Wm. and Hornberger, John},
  title     = {The sample size for a clinical trial: A Bayesian decision theoretic approach},
  year      = {2001},
  volume    = {20},
  number    = {6},
  pages     = {841--858},
  issn      = {1097-0258},
  doi       = {10.1002/sim.703},
  url       = {http://dx.doi.org/10.1002/sim.703},
  abstract  = {Using decision theory, what is an appropriate sample size for a clinical trial, with a binary endpoint? We present a program, suitable for actual planning, which, with some extensions, implements Canner's solution to this question. Examples with a discussion are given. Implications of a Bayesian approach are discussed. Bayesian and Neymanâ€“Pearson approaches are compared. Copyright Â© 2001 John Wiley & Sons, Ltd.},
  groups    = {Bayesian, Decision theory},
  journal   = {Statistics in Medicine},
  owner     = {meddwilb},
  publisher = {John Wiley \& Sons, Ltd.},
  timestamp = {2016.07.06},
}

@ARTICLE{Hampson2014,
  author = {Hampson, Lisa V. and Whitehead, John and Eleftheriou, Despina and
	Brogan, Paul},
  title = {Bayesian methods for the design and interpretation of clinical trials
	in very rare diseases},
  journal = {Statistics in Medicine},
  year = {2014},
  volume = {33},
  pages = {4186--4201},
  number = {24},
  abstract = {This paper considers the design and interpretation of clinical trials
	comparing treatments for conditions so rare that worldwide recruitment
	efforts are likely to yield total sample sizes of 50 or fewer, even
	when patients are recruited over several years. For such studies,
	the sample size needed to meet a conventional frequentist power requirement
	is clearly infeasible. Rather, the expectation of any such trial
	has to be limited to the generation of an improved understanding
	of treatment options. We propose a Bayesian approach for the conduct
	of rare-disease trials comparing an experimental treatment with a
	control where patient responses are classified as a success or failure.
	A systematic elicitation from clinicians of their beliefs concerning
	treatment efficacy is used to establish Bayesian priors for unknown
	model parameters. The process of determining the prior is described,
	including the possibility of formally considering results from related
	trials. As sample sizes are small, it is possible to compute all
	possible posterior distributions of the two success rates. A number
	of allocation ratios between the two treatment groups can be considered
	with a view to maximising the prior probability that the trial concludes
	recommending the new treatment when in fact it is non-inferior to
	control. Consideration of the extent to which opinion can be changed,
	even by data from the best feasible design, can help to determine
	whether such a trial is worthwhile. Â©â€‰2014 The Authors. Statistics
	in Medicine published by John Wiley & Sons, Ltd.},
  doi = {10.1002/sim.6225},
  issn = {1097-0258},
  keywords = {allocation ratio, Bayesian model, expert opinion, prior elicitation,
	prior power, rare diseases},
  owner = {meddwilb},
  timestamp = {2014.11.05},
  url = {http://dx.doi.org/10.1002/sim.6225}
}

@ARTICLE{HARRELL1996,
  author = {HARRELL, FRANK E. and LEE, KERRY L. and MARK, DANIEL B.},
  title = {MULTIVARIABLE PROGNOSTIC MODELS: ISSUES IN DEVELOPING MODELS, EVALUATING
	ASSUMPTIONS AND ADEQUACY, AND MEASURING AND REDUCING ERRORS},
  journal = {Statistics in Medicine},
  year = {1996},
  volume = {15},
  pages = {361--387},
  number = {4},
  abstract = {Multivariable regression models are powerful tools that are used frequently
	in studies of clinical outcomes. These models can use a mixture of
	categorical and continuous variables and can handle partially observed
	(censored) responses. However, uncritical application of modelling
	techniques can result in models that poorly fit the dataset at hand,
	or, even more likely, inaccurately predict outcomes on new subjects.
	One must know how to measure qualities of a model's fit in order
	to avoid poorly fitted or overfitted models. Measurement of predictive
	accuracy can be difficult for survival time data in the presence
	of censoring. We discuss an easily interpretable index of predictive
	discrimination as well as methods for assessing calibration of predicted
	survival probabilities. Both types of predictive accuracy should
	be unbiasedly validated using bootstrapping or cross-validation,
	before using predictions in a new data series. We discuss some of
	the hazards of poorly fitted and overfitted regression models and
	present one modelling strategy that avoids many of the problems discussed.
	The methods described are applicable to all regression models, but
	are particularly needed for binary, ordinal, and time-to-event outcomes.
	Methods are illustrated with a survival analysis in prostate cancer
	using Cox regression.},
  doi = {10.1002/(SICI)1097-0258(19960229)15:4<361::AID-SIM168>3.0.CO;2-4},
  issn = {1097-0258},
  owner = {meddwilb},
  publisher = {Wiley Subscription Services, Inc., A Wiley Company},
  timestamp = {2015.03.25},
  url = {http://dx.doi.org/10.1002/(SICI)1097-0258(19960229)15:4<361::AID-SIM168>3.0.CO;2-4}
}

@ARTICLE{Hedges2007,
  author = {Hedges, Larry V.},
  title = {Correcting a Significance Test for Clustering},
  journal = {Journal of Educational and Behavioral Statistics},
  year = {2007},
  volume = {32},
  pages = {151-179},
  number = {2},
  abstract = {A common mistake in analysis of cluster randomized trials is to ignore
	the effect of clustering and analyze the data as if each treatment
	group were a simple random sample. This typically leads to an overstatement
	of the precision of results and anticonservative conclusions about
	precision and statistical significance of treatment effects. This
	article gives a simple correction to the t statistic that would be
	computed if clustering were (incorrectly) ignored. The correction
	is a multiplicative factor depending on the total sample size, the
	cluster size, and the intraclass correlation Ï. The corrected t statistic
	has Studentâ€™s t distribution with reduced degrees of freedom. The
	corrected statistic reduces to the t statistic computed by ignoring
	clustering when Ï = 0. It reduces to the t statistic computed using
	cluster means when Ï = 1. If 0 < Ï < 1, it lies between these two,
	and the degrees of freedom are in between those corresponding to
	these two extremes.},
  doi = {10.3102/1076998606298040},
  eprint = {http://jeb.sagepub.com/content/32/2/151.full.pdf+html},
  owner = {meddwilb},
  timestamp = {2014.04.14},
  url = {http://jeb.sagepub.com/content/32/2/151.abstract}
}

@Article{Hee2016,
  author    = {Hee, Siew Wan and Hamborg, Thomas and Day, Simon and Madan, Jason and Miller, Frank and Posch, Martin and Zohar, Sarah and Stallard, Nigel},
  title     = {Decision-theoretic designs for small trials and pilot studies: A review},
  year      = {2016},
  volume    = {25},
  number    = {3},
  pages     = {1022-1038},
  doi       = {10.1177/0962280215588245},
  eprint    = {http://smm.sagepub.com/content/25/3/1022.full.pdf+html},
  url       = {http://smm.sagepub.com/content/25/3/1022.abstract},
  abstract  = {Pilot studies and other small clinical trials are often conducted but serve a variety of purposes and there is little consensus on their design. One paradigm that has been suggested for the design of such studies is Bayesian decision theory. In this article, we review the literature with the aim of summarizing current methodological developments in this area. We find that decision-theoretic methods have been applied to the design of small clinical trials in a number of areas. We divide our discussion of published methods into those for trials conducted in a single stage, those for multi-stage trials in which decisions are made through the course of the trial at a number of interim analyses, and those that attempt to design a series of clinical trials or a drug development programme. In all three cases, a number of methods have been proposed, depending on the decision maker’s perspective being considered and the details of utility functions that are used to construct the optimal design.},
  groups    = {Bayesian, Decision theory, Pilot/feasibility},
  journal   = {Statistical Methods in Medical Research},
  owner     = {meddwilb},
  timestamp = {2016.07.06},
}

@ARTICLE{Hee2012,
  author = {Hee, Siew Wan and Stallard, Nigel},
  title = {Designing a series of decision-theoretic phase {II} trials in a small
	population},
  journal = {Statistics in Medicine},
  year = {2012},
  volume = {31},
  pages = {4337--4351},
  number = {30},
  abstract = {This paper introduces a decision-theoretic design for a series of
	phase II trials. Instead of designing phase II trials individually,
	we proposed a development plan that consists of a series of phase
	II trials and one phase III trial such that the long-term expected
	utility on the whole is optimized. The phase II trials are conducted
	sequentially, and patients are recruited sequentially to each phase
	II trial. At each interim stage, a decision is made to continue recruiting
	patients to the current trial, to stop and recommend the treatment
	proceeds to a phase III trial, to stop and initiate a new phase II
	trial or to stop and cease the development plan. The methodology
	uses a hybrid approach in which it is assumed that the data from
	the final phase III trial will be analysed using a classical frequentist
	hypothesis test. The expected power of this test based on some specified
	prior distribution for the effect of the experimental treatment is
	then used in a utility function, which is used to obtain the optimal
	design for the whole series of trials. Copyright Â© 2012 John Wiley
	& Sons, Ltd.},
  doi = {10.1002/sim.5573},
  issn = {1097-0258},
  keywords = {clinical trial design, hybrid design, frequentist, Bayesian decision
	theory, backward induction},
  owner = {meddwilb},
  publisher = {John Wiley \& Sons, Ltd},
  timestamp = {2013.11.29},
  url = {http://dx.doi.org/10.1002/sim.5573}
}

@ARTICLE{Hemming2011,
  author = {Hemming, Karla and Girling, Alan and Sitch, Alice and Marsh, Jennifer
	and Lilford, Richard},
  title = {Sample size calculations for cluster randomised controlled trials
	with a fixed number of clusters},
  journal = {BMC Medical Research Methodology},
  year = {2011},
  volume = {11},
  pages = {102},
  number = {1},
  abstract = {BACKGROUND:Cluster randomised controlled trials (CRCTs) are frequently
	used in health service evaluation. Assuming an average cluster size,
	required sample sizes are readily computed for both binary and continuous
	outcomes, by estimating a design effect or inflation factor. However,
	where the number of clusters are fixed in advance, but where it is
	possible to increase the number of individuals within each cluster,
	as is frequently the case in health service evaluation, sample size
	formulae have been less well studied.METHODS:We systematically outline
	sample size formulae (including required number of randomisation
	units, detectable difference and power) for CRCTs with a fixed number
	of clusters, to provide a concise summary for both binary and continuous
	outcomes. Extensions to the case of unequal cluster sizes are provided.RESULTS:For
	trials with a fixed number of equal sized clusters (k), the trial
	will be feasible provided the number of clusters is greater than
	the product of the number of individuals required under individual
	randomisation (nI) and the estimated intra-cluster correlation (rho).
	So, a simple rule is that the number of clusters (k) will be sufficient
	provided:Where this is not the case, investigators can determine
	the maximum available power to detect the pre-specified difference,
	or the minimum detectable difference under the pre-specified value
	for power.CONCLUSIONS:Designing a CRCT with a fixed number of clusters
	might mean that the study will not be feasible, leading to the notion
	of a minimum detectable difference (or a maximum achievable power),
	irrespective of how many individuals are included within each cluster.},
  doi = {10.1186/1471-2288-11-102},
  issn = {1471-2288},
  owner = {meddwilb},
  pubmedid = {21718530},
  timestamp = {2014.07.15},
  url = {http://www.biomedcentral.com/1471-2288/11/102}
}

@Article{Hobbs2011,
  author    = {Hobbs, Brian P. and Carlin, Bradley P. and Mandrekar, Sumithra J. and Sargent, Daniel J.},
  title     = {Hierarchical Commensurate and Power Prior Models for Adaptive Incorporation of Historical Information in Clinical Trials},
  year      = {2011},
  volume    = {67},
  number    = {3},
  pages     = {1047--1056},
  issn      = {1541-0420},
  doi       = {10.1111/j.1541-0420.2011.01564.x},
  url       = {http://dx.doi.org/10.1111/j.1541-0420.2011.01564.x},
  abstract  = {Summary Bayesian clinical trial designs offer the possibility of a
	substantially reduced sample size, increased statistical power, and
	reductions in cost and ethical hazard. However when prior and current
	information conflict, Bayesian methods can lead to higher than expected
	type I error, as well as the possibility of a costlier and lengthier
	trial. This motivates an investigation of the feasibility of hierarchical
	Bayesian methods for incorporating historical data that are adaptively
	robust to prior information that reveals itself to be inconsistent
	with the accumulating experimental data. In this article, we present
	several models that allow for the commensurability of the information
	in the historical and current data to determine how much historical
	information is used. A primary tool is elaborating the traditional
	power prior approach based upon a measure of commensurability for
	Gaussian data. We compare the frequentist performance of several
	methods using simulations, and close with an example of a colon cancer
	trial that illustrates a linear models extension of our adaptive
	borrowing approach. Our proposed methods produce more precise estimates
	of the model parameters, in particular, conferring statistical significance
	to the observed reduction in tumor size for the experimental regimen
	as compared to the control regimen.},
  groups    = {SSR},
  journal   = {Biometrics},
  keywords  = {Adaptive designs, Bayesian, Clinical trials, Colorectal cancer, Power priors},
  owner     = {meddwilb},
  publisher = {Blackwell Publishing Inc},
  timestamp = {2014.06.02},
}

@Article{Hobbs2013,
  author    = {Hobbs, Brian P and Carlin, Bradley P and Sargent, Daniel J},
  title     = {Adaptive adjustment of the randomization ratio using historical control data},
  year      = {2013},
  volume    = {10},
  number    = {3},
  pages     = {430-440},
  doi       = {10.1177/1740774513483934},
  eprint    = {http://ctj.sagepub.com/content/10/3/430.full.pdf+html},
  url       = {http://ctj.sagepub.com/content/10/3/430.abstract},
  abstract  = {Background Prospective trial design often occurs in the presence of
	â€˜acceptableâ€™ historical control data. Typically, these data are
	only utilized for treatment comparison in a posteriori retrospective
	analysis to estimate population-averaged effects in a random-effects
	meta-analysis.Purpose We propose and investigate an adaptive trial
	design in the context of an actual randomized controlled colorectal
	cancer trial. This trial, originally reported by Goldberg et al.,
	succeeded a similar trial reported by Saltz et al., and used a control
	therapy identical to that tested (and found beneficial) in the Saltz
	trial.Methods The proposed trial implements an adaptive randomization
	procedure for allocating patients aimed at balancing total information
	(concurrent and historical) among the study arms. This is accomplished
	by assigning more patients to receive the novel therapy in the absence
	of strong evidence for heterogeneity among the concurrent and historical
	controls. Allocation probabilities adapt as a function of the effective
	historical sample size (EHSS), characterizing relative informativeness
	defined in the context of a piecewise exponential model for evaluating
	time to disease progression. Commensurate priors are utilized to
	assess historical and concurrent heterogeneity at interim analyses
	and to borrow strength from the historical data in the final analysis.
	The adaptive trialâ€™s frequentist properties are simulated using
	the actual patient-level historical control data from the Saltz trial
	and the actual enrollment dates for patients enrolled into the Goldberg
	trial.Results Assessing concurrent and historical heterogeneity at
	interim analyses and balancing total information with the adaptive
	randomization procedure lead to trials that on average assign more
	new patients to the novel treatment when the historical controls
	are unbiased or slightly biased compared to the concurrent controls.
	Large magnitudes of bias lead to approximately equal allocation of
	patients among the treatment arms. Using the proposed commensurate
	prior model to borrow strength from the historical data, after balancing
	total information with the adaptive randomization procedure, provides
	admissible estimators of the novel treatment effect with desirable
	bias-variance trade-offs.Limitations Adaptive randomization methods
	in general are sensitive to population drift and more suitable for
	trials that initiate with gradual enrollment. Balancing information
	among study arms in time-to-event analyses is difficult in the presence
	of informative right-censoring.Conclusions The proposed design could
	prove important in trials that follow recent evaluations of a control
	therapy. Efficient use of the historical controls is especially important
	in contexts where reliance on preexisting information is unavoidable
	because the control therapy is exceptionally hazardous, expensive,
	or the disease is rare.},
  groups    = {SSR},
  journal   = {Clinical Trials},
  owner     = {meddwilb},
  timestamp = {2014.06.05},
}

@Article{Hoenig2001,
  author =    {John M Hoenig and Dennis M Heisey},
  title =     {The Abuse of Power},
  journal =   {The American Statistician},
  year =      {2001},
  volume =    {55},
  number =    {1},
  pages =     {19-24},
  doi =       {10.1198/000313001300339897},
  eprint =    { http://dx.doi.org/10.1198/000313001300339897 },
  owner =     {meddwilb},
  timestamp = {2016.02.02},
  url =       { http://dx.doi.org/10.1198/000313001300339897 
}
}

@ARTICLE{Holford2010,
  author = {Holford, N and Ma, S C and Ploeger, B A},
  title = {Clinical Trial Simulation: A Review},
  journal = {Clin Pharmacol Ther},
  year = {2010},
  volume = {88},
  pages = {166--182},
  number = {2},
  month = aug,
  issn = {0009-9236},
  owner = {meddwilb},
  publisher = {American Society of Clinical Pharmacology and Therapeutics},
  timestamp = {2014.08.11},
  url = {http://dx.doi.org/10.1038/clpt.2010.114}
}

@ARTICLE{Holloway2011,
  author = {Holloway, Ivana and Farrin, Amanda},
  title = {Sample size in cluster randomised trials with unequal clusters},
  journal = {Trials},
  year = {2011},
  volume = {12},
  pages = {A25},
  number = {Suppl 1},
  doi = {10.1186/1745-6215-12-S1-A25},
  issn = {1745-6215},
  owner = {meddwilb},
  timestamp = {2014.03.25},
  url = {http://www.trialsjournal.com/content/12/S1/A25}
}

@Article{Hooper2009,
  author    = {Richard Hooper},
  title     = {The Bayesian interpretation of a P-value depends only weakly on statistical power in realistic situations},
  year      = {2009},
  volume    = {62},
  number    = {12},
  pages     = {1242 - 1247},
  issn      = {0895-4356},
  doi       = {http://dx.doi.org/10.1016/j.jclinepi.2009.02.004},
  url       = {http://www.sciencedirect.com/science/article/pii/S0895435609000596},
  abstract  = {Objective It is often repeated that a low P-value provides more persuasive
	evidence for a genuine effect if the power of the test is high. However,
	this is based on an argument which ignores the precise P-value in
	favor of simply observing whether P is less than some cut-off, and
	which oversimplifies the possible effect sizes. In a non-Bayesian
	framework, there are good reasons to think that power does not affect
	the evidence of a given P-value. Here I illustrate the relationship
	between pre-study power and the Bayesian interpretation of a P-value
	in realistic situations. Study Design and Setting A Bayesian calculation,
	using a conventional prior distribution for the effect size and a
	normal approximation to the sampling distribution of the sample estimate,
	where the datum is the precise P-value. Results Over the range of
	pre-study powers typical in published research, the Bayesian interpretation
	of a given P-value varies little with power. Conclusion A Bayesian
	analysis with reasonable assumptions produces results remarkably
	in line with a more simple, non-Bayesian intuitionâ€”that the evidence
	against the null hypothesis provided by a precise P-value should
	not depend on power. },
  journal   = {Journal of Clinical Epidemiology},
  keywords  = {Bayes theorem},
  owner     = {meddwilb},
  timestamp = {2013.11.26},
}

@Article{Hooper2013,
  author    = {Richard Hooper},
  title     = {Versatile sample-size calculation using simulation},
  year      = {2013},
  volume    = {13},
  number    = {1},
  pages     = {21-38},
  url       = {http://www.stata-journal.com/article.html?article=st0282},
  abstract  = {I present a new Stata command, simsam, that uses simulation to determine
	the sample size required to achieve a given statistical power for
	any hypothesis test under any probability model that can be programmed
	in Stata. simsam returns the smallest sample size (or smallest multiple
	of 5, 10, or some other user-specified increment) so that the estimated
	power exceeds the target. The user controls the precision of the
	power estimate, and power is reported with a confidence interval.
	The sample size returned is reliable to the extent that if simsam
	is repeated, it will, nearly every time, give a sample size no more
	than one increment away.},
  file      = {:P\:\\CTRU\\Projects\\Health_Sciences\\Methodology\\2013 NIHR Research Methods Fellowship\\Literature\\Papers\\Hooper2013.pdf:PDF},
  groups    = {multi-D / sim SS},
  journal   = {The STATA Journal},
  owner     = {meddwilb},
  timestamp = {2013.12.20},
}

@ELECTRONIC{OKD2013,
  author = {Allan House and Ramzi Ajjan and Louise Bryant and Amanda Farrin and
	Elizabeth Graham and Claire Hulme and Gary Latchford and Dinesh Nagi
	and Damian Riley and Alison Stansfield},
  title = {Managing with Learning Disability and Diabetes},
  organization = {NIHR HTA},
  note = {Accesed 6th October 2014},
  url = {http://www.nets.nihr.ac.uk/projects/hta/1010203},
  owner = {meddwilb},
  timestamp = {2014.10.06}
}

@BOOK{Hox2002,
  title = {Multilevel Analysis: Techniques and Applications},
  publisher = {Lawrence Erlbaum Associates, Inc.},
  year = {2002},
  author = {Joop Hox},
  owner = {meddwilb},
  timestamp = {2014.12.16}
}

@ARTICLE{Hsieh1988,
  author = {Hsieh, F. Y.},
  title = {Sample size formulae for intervention studies with the cluster as
	unit of randomization},
  journal = {Statist. Med.},
  year = {1988},
  volume = {7},
  pages = {1195--1201},
  number = {11},
  abstract = {This paper presents sample size formulae for both continuous and dichotomous
	endpoints obtained from intervention studies that use the cluster
	as the unit of randomization. The formulae provide the required number
	of clusters or the required number of individuals per cluster when
	the other number is given. The proposed formulae derive from Student's
	t-test with use of cluster summary measures and a variance that consists
	of within and between cluster components. Power contours are provided
	to help in the design of intervention studies that use cluster randomization.
	Sample size formulae for designs with and without stratification
	of clusters appear separately.},
  issn = {1097-0258},
  keywords = {Sample size, Group randomization, Cluster randomization, Community
	intervention, Variance components},
  owner = {meddwilb},
  publisher = {Wiley Subscription Services, Inc., A Wiley Company},
  timestamp = {2014.11.17},
  url = {http://dx.doi.org/10.1002/sim.4780071113}
}

@ARTICLE{Hu2008,
  author = {Hu, Jiaqiao and Su, Zheng},
  title = {Efficient Error Determination in Sequential Clinical Trial Design},
  journal = {Journal of Computational and Graphical Statistics},
  year = {2008},
  volume = {17},
  pages = {pp. 925-948},
  number = {4},
  abstract = {Many clinical trials involve a sequential stopping rule to specify
	the conditions under which a study might be terminated earlier before
	its planned completion. The most important issue in the design stage
	is to determine common operating characteristics such as Type I and
	Type II error rates, for which crude Monte Carlo simulation methods
	are widely adopted. However, it is well known that crude Monte Carlo
	may lead to large variabilities in resultant estimates and excessive
	waste of computational resources. In this article, we propose an
	efficient importance sampling approach for determining Type I and
	Type II error rates in both fully sequential and group sequential
	clinical trial designs with either immediate responses or survival
	endpoints. The approach is insensitive to the underlying statistics
	of interest, and can be easily built into a general algorithm to
	evaluate error rates, determine sample sizes, test statistical hypotheses,
	and construct confidence intervals. Our simulation results on a hypotensive
	agent trial and a modified Beta-Blocker Heart Attack Trial indicate
	that the proposed approach is not only superior to the crude Monte
	Carlo method, but may also provide many-fold savings in simulation
	cost.},
  copyright = {Copyright Â© 2008 American Statistical Association, Institute of Mathematical
	Statistics and Interface Foundation of America},
  issn = {10618600},
  jstor_articletype = {research-article},
  jstor_formatteddate = {December 2008},
  language = {English},
  owner = {meddwilb},
  publisher = {American Statistical Association, Institute of Mathematical Statistics,
	and Interface Foundation of America},
  timestamp = {2014.07.21},
  url = {http://www.jstor.org/stable/25651236}
}

@ARTICLE{Ionan2014,
  author = {Ionan, Alexei C. and Polley, Mei-Yin C. and McShane, Lisa M. and
	Dobbin, Kevin K.},
  title = {Comparison of confidence interval methods for an intra-class correlation
	coefficient (ICC)},
  journal = {BMC Medical Research Methodology},
  year = {2014},
  volume = {14},
  pages = {1--11},
  number = {1},
  abstract = {The intraclass correlation coefficient (ICC) is widely used in biomedical
	research to assess the reproducibility of measurements between raters,
	labs, technicians, or devices. For example, in an inter-rater reliability
	study, a high ICC value means that noise variability (between-raters
	and within-raters) is small relative to variability from patient
	to patient. A confidence interval or Bayesian credible interval for
	the ICC is a commonly reported summary. Such intervals can be constructed
	employing either frequentist or Bayesian methodologies.},
  doi = {10.1186/1471-2288-14-121},
  issn = {1471-2288},
  owner = {meddwilb},
  timestamp = {2016.01.14},
  url = {http://dx.doi.org/10.1186/1471-2288-14-121}
}

@ARTICLE{Janega2004,
  author = {Jessica B. Janega and David M. Murray and Sherri P. Varnell and Jonathan
	L. Blitstein and Amanda S. Birnbaum and Leslie A. Lytle},
  title = {Assessing the most powerful analysis method for school-based intervention
	studies with alcohol, tobacco, and other drug outcomes },
  journal = {Addictive Behaviors },
  year = {2004},
  volume = {29},
  pages = {595 - 606},
  number = {3},
  doi = {http://dx.doi.org/10.1016/j.addbeh.2004.01.002},
  issn = {0306-4603},
  keywords = {Intraclass correlation},
  owner = {meddwilb},
  timestamp = {2013.09.10},
  url = {http://www.sciencedirect.com/science/article/pii/S030646030400005X}
}

@ARTICLE{Jepson2006,
  author = {Ruth Jepson and Fiona Harris and Steve MacGillivray and Nora Kearney
	and Neneh Rowa-Dewar},
  title = {A review of the effectiveness of interventions, approaches and models
	at individual, community and population level that are aimed at changing
	health outcomes through changing knowledge attitudes and behaviour},
  journal = {NICE guidelines},
  year = {2006},
  owner = {meddwilb},
  timestamp = {2015.05.13},
  url = {http://www.nice.org.uk/guidance/ph6/evidence/behaviour-change-review-1-effectiveness-review2}
}

@Article{Jones2001,
  author    = {Jones, Donald R.},
  title     = {A Taxonomy of Global Optimization Methods Based on Response Surfaces},
  year      = {2001},
  language  = {English},
  volume    = {21},
  number    = {4},
  pages     = {345-383},
  issn      = {0925-5001},
  doi       = {10.1023/A:1012771025575},
  url       = {http://link.springer.com/article/10.1023/A:1012771025575},
  groups    = {Optimisation},
  journal   = {Journal of Global Optimization},
  keywords  = {global optimization; response surface; kriging; splines},
  owner     = {meddwilb},
  publisher = {Kluwer Academic Publishers},
  timestamp = {2013.10.10},
}

@Article{Jones1998,
  author    = {Donald R. Jones and Matthias Schonlau and William J. Welch},
  title     = {Efficient Global Optimization of Expensive Black-Box Functions},
  year      = {1998},
  volume    = {13},
  number    = {4},
  pages     = {455--492},
  doi       = {10.1023/a:1008306431147},
  url       = {http://dx.doi.org/10.1023/A:1008306431147},
  groups    = {Optimisation},
  journal   = {Journal of Global Optimization},
  owner     = {meddwilb},
  publisher = {Springer Science $\mathplus$ Business Media},
  timestamp = {2016.08.01},
}

@ARTICLE{Joseph1997,
  author = {Joseph, Lawrence and Belisle, Patrick},
  title = {Bayesian sample size determination for normal means and differences
	between normal means},
  journal = {Journal of the Royal Statistical Society: Series D (The Statistician)},
  year = {1997},
  volume = {46},
  pages = {209--226},
  number = {2},
  abstract = {Several criteria for Bayesian sample size determination based on lengths
	and coverages of posterior credible intervals have recently appeared
	in the literature. Some but not all of these have been applied to
	estimating sample sizes for normal distributions. In this paper,
	these criteria are applied to find sample sizes for single normal
	means and differences between two normal means, both when the variances
	are known and when they are unknown. Fully Bayesian approaches as
	well as mixed Bayesian-- likelihood approaches are considered. In
	the case of the difference between two normal means, situations with
	equal and unequal variances of the two distributions are considered.
	In addition to the rule that assumes equally sized groups, optimal
	solutions are determined which allow the sizes drawn from the two
	populations to differ while minimizing their sum. Exact closed form
	solutions are available for many of the situations, whereas numerical
	algorithms are described for others.},
  doi = {10.1111/1467-9884.00077},
  issn = {1467-9884},
  keywords = {Bayesian approach, Credible interval, Normal distribution, Optimal
	design, Predictive distribution, Sample size},
  owner = {meddwilb},
  publisher = {Blackwell Publishers Ltd},
  timestamp = {2014.10.21},
  url = {http://dx.doi.org/10.1111/1467-9884.00077}
}

@Article{Joseph1997b,
  author    = {Joseph, Lawrence and Du Berger, Roxane and Belisle, Patrick},
  title     = {Bayesian and mixed Bayesian/likelihood criteria for sample size determination},
  year      = {1997},
  volume    = {16},
  number    = {7},
  pages     = {769--781},
  issn      = {1097-0258},
  doi       = {10.1002/(SICI)1097-0258(19970415)16:7<769::AID-SIM495>3.0.CO;2-V},
  url       = {http://dx.doi.org/10.1002/(SICI)1097-0258(19970415)16:7<769::AID-SIM495>3.0.CO;2-V},
  abstract  = {Sample size estimation is a major component of the design of virtually every experiment in medicine. Prudent use of the available prior information is a crucial element of experimental planning. Most sample size formulae in current use employ this information only in the form of point estimates, even though it is usually more accurately expressed as a distribution over a range of values. In this paper, we review several Bayesian and mixed Bayesian/likelihood approaches to sample size calculations based on lengths and coverages of posterior credible intervals. We apply these approaches to the design of an experiment to estimate the difference between two binomial proportions, and we compare results to those derived from standard formulae. Consideration of several criteria can contribute to selection of a final sample size. Â© 1997 by John Wiley & Sons, Ltd.},
  groups    = {Bayesian, Decision theory},
  journal   = {Statistics in Medicine},
  owner     = {meddwilb},
  publisher = {John Wiley \& Sons, Ltd.},
  timestamp = {2016.07.08},
}

@Article{Joseph1997a,
  author    = {Joseph, Lawrence and Wolfson, David B.},
  title     = {Interval-based versus decision theoretic criteria for the choice of sample size},
  year      = {1997},
  volume    = {46},
  number    = {2},
  pages     = {145--149},
  issn      = {1467-9884},
  doi       = {10.1111/1467-9884.00070},
  url       = {http://dx.doi.org/10.1111/1467-9884.00070},
  abstract  = {Several different criteria for Bayesian sample size determination
	have recently been proposed. Bayesian approaches are natural, since
	at the planning stage of an experiment one is forced to consider
	prior notions about unknown parameter values that may affect the
	choice of a final sample size. For this, all the methods consider
	a prior distribution over the unknown parameters. Differences between
	the methods have been driven by the type of inferences that will
	be made, e.g. hypothesis testing or interval estimation, the latter
	based on posterior means and variances or highest posterior density
	regions. A more fundamental question, however, is whether to introduce
	formally a loss or utility function to aid in choosing the sample
	size. In this paper, we discuss the advantages and disadvantages
	of taking a fully decision theoretic approach versus one of the simpler
	approaches, which only implicitly consider utilities in balancing
	increased precision against the increased costs associated with larger
	sample sizes. Throughout, we emphasize the practical aspects of sample
	size estimation, raising issues that would face the consumer of statistics
	in selecting a sample size in a given experiment.},
  groups    = {Decision theory},
  journal   = {Journal of the Royal Statistical Society: Series D (The Statistician)},
  keywords  = {Bayesian design, Decision theory, Highest posterior density, Sample size determination},
  owner     = {meddwilb},
  publisher = {Blackwell Publishers Ltd},
  timestamp = {2015.11.30},
}

@Article{Julious2005,
  author    = {Julious, Steven A.},
  title     = {Sample size of 12 per group rule of thumb for a pilot study},
  journal   = {Pharmaceutical Statistics},
  year      = {2005},
  volume    = {4},
  number    = {4},
  pages     = {287--291},
  issn      = {1539-1612},
  doi       = {10.1002/pst.185},
  url       = {http://dx.doi.org/10.1002/pst.185},
  abstract  = {When designing a clinical trial an appropriate justification for the
	sample size should be provided in the protocol. However, there are
	a number of settings when undertaking a pilot trial when there is
	no prior information to base a sample size on. For such pilot studies
	the recommendation is a sample size of 12 per group. The justifications
	for this sample size are based on rationale about feasibility; precision
	about the mean and variance; and regulatory considerations. The context
	of the justifications are that future studies will use the information
	from the pilot in their design. Copyright Â© 2005 John Wiley & Sons,
	Ltd.},
  groups    = {Pilot/feasibility},
  keywords  = {sample size, pilot study},
  owner     = {meddwilb},
  publisher = {John Wiley \& Sons, Ltd.},
  timestamp = {2013.11.13},
}

@ARTICLE{Jung2008,
  author = {Jung, Sin-Ho},
  title = {Randomized phase {II} trials with a prospective control},
  journal = {Statistics in Medicine},
  year = {2008},
  volume = {27},
  pages = {568--583},
  number = {4},
  abstract = {We consider phase II trials randomizing patients between a prospective
	control and an experimental therapy. Proposed are two-stage designs
	allowing for early termination of the study when the experimental
	arm does not show promising efficacy at the interim analysis. By
	using exact binomial distributions, the design characteristics, such
	as type I error and power, are exactly calculated. Given response
	probabilities for two arms, we define minimax and optimal designs
	that satisfy a prespecified restriction on type I and II error probabilities.
	These designs are randomized phase II trial analogs of Simon's designs
	that were proposed for single-arm phase II trials. The methods for
	two-arm trials are easily extended to multi-arm trials with one control
	and K experimental arms. Some phase II trials are taken as real examples.
	Copyright 2007 John Wiley & Sons, Ltd.},
  doi = {10.1002/sim.2961},
  issn = {1097-0258},
  keywords = {balanced allocation, futility, minimax design, multi-arm trial, optimal
	design, two-stage design},
  owner = {meddwilb},
  publisher = {John Wiley \& Sons, Ltd.},
  timestamp = {2013.12.10},
  url = {http://dx.doi.org/10.1002/sim.2961}
}

@ARTICLE{Jung2001,
  author = {Sin-Ho Jung and Mark Carey and Kyung Mann Kim},
  title = {Graphical Search for Two-Stage Designs for Phase {II} Clinical Trials},
  journal = {Controlled Clinical Trials },
  year = {2001},
  volume = {22},
  pages = {367 - 372},
  number = {4},
  doi = {http://dx.doi.org/10.1016/S0197-2456(01)00142-8},
  issn = {0197-2456},
  keywords = {Minimax design},
  owner = {meddwilb},
  timestamp = {2014.03.10},
  url = {http://www.sciencedirect.com/science/article/pii/S0197245601001428}
}

@ARTICLE{Kahan2012,
  author = {Kahan, Brennan C. and Morris, Tim P.},
  title = {Improper analysis of trials randomised using stratified blocks or
	minimisation},
  journal = {Statistics in Medicine},
  year = {2012},
  volume = {31},
  pages = {328--340},
  number = {4},
  abstract = {Many clinical trials restrict randomisation using stratified blocks
	or minimisation to balance prognostic factors across treatment groups.
	It is widely acknowledged in the statistical literature that the
	subsequent analysis should reflect the design of the study, and any
	stratification or minimisation variables should be adjusted for in
	the analysis. However, a review of recent general medical literature
	showed only 14 of 41 eligible studies reported adjusting their primary
	analysis for stratification or minimisation variables. We show that
	balancing treatment groups using stratification leads to correlation
	between the treatment groups. If this correlation is ignored and
	an unadjusted analysis is performed, standard errors for the treatment
	effect will be biased upwards, resulting in 95% confidence intervals
	that are too wide, type I error rates that are too low and a reduction
	in power. Conversely, an adjusted analysis will give valid inference.
	We explore the extent of this issue using simulation for continuous,
	binary and time-to-event outcomes where treatment is allocated using
	stratified block randomisation or minimisation. Copyright Â© 2011
	John Wiley & Sons, Ltd.},
  doi = {10.1002/sim.4431},
  issn = {1097-0258},
  keywords = {clinical trials, covariates, adjustment, minimisation, stratification,
	unadjusted analysis},
  owner = {meddwilb},
  timestamp = {2014.02.13},
  url = {http://dx.doi.org/10.1002/sim.4431}
}

@ARTICLE{Kennedy2001,
  author = {Kennedy, Marc C. and O'Hagan, Anthony},
  title = {Bayesian calibration of computer models},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  year = {2001},
  volume = {63},
  pages = {425--464},
  number = {3},
  abstract = {We consider prediction and uncertainty analysis for systems which
	are approximated using complex mathematical models. Such models,
	implemented as computer codes, are often generic in the sense that
	by a suitable choice of some of the model's input parameters the
	code can be used to predict the behaviour of the system in a variety
	of specific applications. However, in any specific application the
	values of necessary parameters may be unknown. In this case, physical
	observations of the system in the specific context are used to learn
	about the unknown parameters. The process of fitting the model to
	the observed data by adjusting the parameters is known as calibration.
	Calibration is typically effected by ad hoc fitting, and after calibration
	the model is used, with the fitted input values, to predict the future
	behaviour of the system. We present a Bayesian calibration technique
	which improves on this traditional approach in two respects. First,
	the predictions allow for all sources of uncertainty, including the
	remaining uncertainty over the fitted parameters. Second, they attempt
	to correct for any inadequacy of the model which is revealed by a
	discrepancy between the observed data and the model predictions from
	even the best-fitting parameter values. The method is illustrated
	by using data from a nuclear radiation release at Tomsk, and from
	a more complex simulated nuclear accident exercise.},
  doi = {10.1111/1467-9868.00294},
  issn = {1467-9868},
  keywords = {Calibration, Computer experiments, Deterministic models, Gaussian
	process, Interpolation, Model inadequacy, Sensitivity analysis, Uncertainty
	analysis},
  owner = {meddwilb},
  publisher = {Blackwell Publishers Ltd.},
  timestamp = {2014.02.17},
  url = {http://dx.doi.org/10.1111/1467-9868.00294}
}

@ARTICLE{Kennedy2005,
  author = {Tom Kennedy and Roger Jones and Simon Darnley and Paul Seed and Simon
	Wessely and Trudie Chalder},
  title = {Cognitive behaviour therapy in addition to antispasmodic treatment
	for irritable bowel syndrome in primary care: randomised controlled
	trial},
  journal = {BMJ},
  year = {2005},
  volume = {331},
  pages = {435},
  number = {7514},
  month = {8},
  doi = {10.1136/bmj.38545.505764.06},
  owner = {meddwilb},
  timestamp = {2014.06.20},
  url = {http://www.bmj.com/content/331/7514/435}
}

@ARTICLE{Kenward1997,
  author = {Kenward, Michael G. and Roger, James H.},
  title = {Small Sample Inference for Fixed Effects from Restricted Maximum
	Likelihood},
  journal = {Biometrics},
  year = {1997},
  volume = {53},
  pages = {pp. 983-997},
  number = {3},
  abstract = {Restricted maximum likelihood (REML) is now well established as a
	method for estimating the parameters of the general Gaussian linear
	model with a structured covariance matrix, in particular for mixed
	linear models. Conventionally, estimates of precision and inference
	for fixed effects are based on their asymptotic distribution, which
	is known to be inadequate for some small-sample problems. In this
	paper, we present a scaled Wald statistic, together with an F approximation
	to its sampling distribution, that is shown to perform well in a
	range of small sample settings. The statistic uses an adjusted estimator
	of the covariance matrix that has reduced small sample bias. This
	approach has the advantage that it reproduces both the statistics
	and F distributions in those settings where the latter is exact,
	namely for Hotelling T2 type statistics and for analysis of variance
	F-ratios. The performance of the modified statistics is assessed
	through simulation studies of four different REML analyses and the
	methods are illustrated using three examples.},
  copyright = {Copyright Â© 1997 International Biometric Society},
  issn = {0006341X},
  jstor_articletype = {research-article},
  jstor_formatteddate = {Sep., 1997},
  language = {English},
  owner = {meddwilb},
  publisher = {International Biometric Society},
  timestamp = {2014.04.07},
  url = {http://www.jstor.org/stable/2533558}
}

@ARTICLE{Keogh2007,
  author = {Keogh, Karen and White, Patricia and Smith, Susan and McGilloway,
	Sinead and O'Dowd, Tom and Gibney, James},
  title = {Changing illness perceptions in patients with poorly controlled type
	2 diabetes, a randomised controlled trial of a family-based intervention:
	protocol and pilot study},
  journal = {BMC Family Practice},
  year = {2007},
  volume = {8},
  pages = {36},
  number = {1},
  abstract = {BACKGROUND:This paper presents the pilot study and protocol for a
	randomised controlled trial to test the effectiveness of a psychological,
	family-based intervention to improve outcomes in those with poorly
	controlled type 2 diabetes. The intervention has been designed to
	change the illness perceptions of patients with poorly controlled
	type 2 diabetes, and their family members. It is a complex psychological
	intervention, developed from the Self-Regulatory Model of Illness
	Behaviour. The important influence the family context can have in
	psychological interventions and diabetes management is also recognised,
	by the inclusion of patients' family members.METHODS/DESIGN:We aim
	to recruit 122 patients with persistently poorly controlled diabetes.
	Patients are deemed to have persistent poor control when at least
	two out of their last three HbA1c readings are 8.0% or over. Patients
	nominate a family member to participate with them, and this patient/family
	member dyad is randomly allocated to either the intervention or control
	group. Participants in the control group receive their usual care.
	Participants in the intervention group participate, with their family
	members, in three intervention sessions. Sessions one and two are
	delivered in the participant's home by a health psychologist. Session
	one takes place approximately one week after session two, with the
	third session, a follow-up telephone call, one week later. The intervention
	is based upon clarifying the illness perceptions of both the patient
	and the family member, examining how they influence self-management
	behaviours, improving the degree of similarity of patient and family
	member perceptions in a positive direction and developing personalized
	action plans to improve diabetes management.DISCUSSION:This study
	is the first of its kind to incorporate the evidence from illness
	perceptions research into developing and applying an intervention
	for people with poorly controlled diabetes and their families. This
	study also acknowledges the important role of family members in effective
	diabetes care.TRIAL REGISTRATION:ISRCTN62219234},
  doi = {10.1186/1471-2296-8-36},
  issn = {1471-2296},
  owner = {meddwilb},
  pubmedid = {17597523},
  timestamp = {2013.09.18},
  url = {http://www.biomedcentral.com/1471-2296/8/36}
}

@ARTICLE{Keogh2011,
  author = {Karen M. Keogh and Susan M. Smith and Patricia White and Sinead McGilloway
	and Alan Kelly and James Gibney and Tom O'Dowd},
  title = {Psychological Family Intervention for Poorly Controlled Type 2 Diabetes},
  journal = {The American Journal of Managed Care},
  year = {2011},
  volume = {17},
  pages = {105-113},
  number = {2},
  abstract = {Objective: To evaluate the effectiveness of a psychological, family-based
	intervention to improve diabetes-related outcomes in patients with
	poorly controlled type 2 diabetes.
	
	
	Methods: This study was a randomized controlled trial of a psychological
	family-based intervention targeted at individuals with poorly controlled
	type 2 diabetes. Recruitment and follow-up occurred at specialist
	diabetes clinics. Patients were randomly allocated to an intervention
	group (n = 60) or a control group (n = 61). Poor control was defined
	as at least 2 of the patient's last 3 glycated hemoglobin (A1C) readings
	at >8.0%. The intervention consisted of 2 sessions delivered by a
	health psychologist to the patient and a family member in the patient's
	home, with a third session involving a 15-minute follow-up telephone
	call.
	
	
	Results: At 6-month follow-up, the intervention group reported significantly
	lower mean A1C levels than the control group (8.4% [SD = 0.99%] vs
	8.8% [SD = 1.36%]; P = .04). The intervention was most effective
	in those with the poorest control at baseline (A1C >9.5%) (intervention
	8.7% [SD = 1.16%, n = 15] vs control 9.9% [SD = 1.31%, n = 15]; P
	= .01). The intervention group also reported statistically significant
	improvements in beliefs about diabetes, psychological well-being,
	diet, exercise, and family support.
	
	
	Conclusions: After participating in a family-based intervention targeting
	negative and/or inaccurate illness perceptions, patients with poorly
	controlled type 2 diabetes showed improvements in A1C levels and
	other outcomes. Our results suggest that adding a psychological,
	family-based component to usual diabetes care may help improve diabetes
	management.},
  owner = {meddwilb},
  timestamp = {2013.09.18}
}

@Article{Kerry2001,
  author    = {Kerry, Sally M. and Bland, J. Martin},
  title     = {Unequal cluster sizes for trials in English and Welsh general practice: implications for sample size calculations},
  journal   = {Statistics in Medicine},
  year      = {2001},
  volume    = {20},
  number    = {3},
  pages     = {377--390},
  issn      = {1097-0258},
  doi       = {10.1002/1097-0258(20010215)20:3<377::AID-SIM799>3.0.CO;2-N},
  url       = {http://dx.doi.org/10.1002/1097-0258(20010215)20:3<377::AID-SIM799>3.0.CO;2-N},
  groups    = {Complex Interventions, Multilevel},
  owner     = {meddwilb},
  publisher = {John Wiley \& Sons, Ltd.},
  timestamp = {2013.09.10},
}

@ARTICLE{Khan2012,
  author = {I Khan and S-J Sarker and A Hackshaw},
  title = {Smaller sample sizes for phase {II} trials based on exact tests with
	actual error rates by trading-off their nominal levels of significance
	and power},
  journal = {British Journal of Cancer},
  year = {2012},
  volume = {107},
  pages = {1801-1809},
  abstract = {background: Sample sizes for single-stage phase II clinical trials
	in the literature are often based on exact (binomial) tests with
	levels of significance (alpha (?) <5% and power >80%). This is because
	there is not always a sample size where ? and power are exactly equal
	to 5% and 80%, respectively. Consequently, the opportunity to trade-off
	small amounts of ? and power for savings in sample sizes may be lost.
	
	methods: Sample-size tables are presented for single-stage phase II
	trials based on exact tests with actual levels of significance and
	power. Trade-off in small amounts of ? and power allows the researcher
	to select from several possible designs with potentially smaller
	sample sizes compared with existing approaches. We provide SAS macro
	coding and an R function, which for a given treatment difference,
	allow researchers to examine all possible sample sizes for specified
	differences are provided.
	
	results: In a single-arm study with P0 (standard treatment)=10% and
	P1 (new treatment)=20%, and specified ?=5% and power=80%, the A’Hern
	approach yields n=78 (exact ?=4.53%, power=80.81%). However, by relaxing
	? to 5.67% and power to 77.7%, a sample size of 65 can be used (a
	saving of 13 patients).
	
	interpretation: The approach we describe is especially useful for
	trials in rare disorders, or for proof-of-concept studies, where
	it is important to minimise the trial duration and financial costs,
	particularly in single-arm cancer trials commonly associated with
	expensive treatment options.},
  doi = {10.1038/bjc.2012.444},
  owner = {meddwilb},
  timestamp = {2013.12.11}
}

@Article{Kieser1996,
  author    = {Meinhard Kieser and Gernot Wassmer},
  title     = {On the Use of the Upper Confidence Limit for the Variance from a Pilot Sample for Sample Size Determination},
  year      = {1996},
  volume    = {38},
  number    = {8},
  pages     = {941--949},
  doi       = {10.1002/bimj.4710380806},
  url       = {http://dx.doi.org/10.1002/bimj.4710380806},
  groups    = {Pilot/feasibility},
  journal   = {Biom. J.},
  owner     = {meddwilb},
  publisher = {Wiley-Blackwell},
  timestamp = {2016.08.12},
}

@Article{Kikuchi2009,
  author    = {Kikuchi, Takashi and Gittins, John},
  title     = {A behavioral Bayes method to determine the sample size of a clinical trial considering efficacy and safety},
  year      = {2009},
  volume    = {28},
  number    = {18},
  pages     = {2293--2306},
  issn      = {1097-0258},
  url       = {http://dx.doi.org/10.1002/sim.3630},
  abstract  = {It is necessary for the calculation of sample size to achieve the best balance between the cost of a clinical trial and the possible benefits from a new treatment. Gittins and Pezeshk developed an innovative (behavioral Bayes) approach, which assumes that the number of users is an increasing function of the difference in performance between the new treatment and the standard treatment. The better a new treatment, the more the number of patients who want to switch to it. The optimal sample size is calculated in this framework. This BeBay approach takes account of three decision-makers, a pharmaceutical company, the health authority and medical advisers. Kikuchi, Pezeshk and Gittins generalized this approach by introducing a logistic benefit function, and by extending to the more usual unpaired case, and with unknown variance. The expected net benefit in this model is based on the efficacy of the new drug but does not take account of the incidence of adverse reactions. The present paper extends the model to include the costs of treating adverse reactions and focuses on societal cost-effectiveness as the criterion for determining sample size. The main application is likely to be to phase III clinical trials, for which the primary outcome is to compare the costs and benefits of a new drug with a standard drug in relation to national health-care. Copyright Â© 2009 John Wiley & Sons, Ltd.},
  groups    = {Bayesian, Decision theory},
  journal   = {Statist. Med.},
  keywords  = {sample size, cost-benefit, clinical trials, efficacy, safety},
  owner     = {meddwilb},
  publisher = {John Wiley \& Sons, Ltd.},
  timestamp = {2016.07.06},
}

@ARTICLE{King2009,
  author = {M King and L Jones and O McCarthy and M Rogers and A Richardson and
	R Williams and A Tookman and I Nazareth},
  title = {Development and pilot evaluation of a complex intervention to improve
	experienced continuity of care in patients with cancer},
  journal = {British Journal of Cancer},
  year = {2009},
  volume = {100},
  pages = {274 - 280},
  doi = {10.1038/sj.bjc.6604836},
  owner = {meddwilb},
  timestamp = {2013.09.18}
}

@ARTICLE{Klar1997,
  author = {Klar, Neil and Donner, Allan},
  title = {The merits of matching in community intervention trials: a cautionary
	tale},
  journal = {Statistics in Medicine},
  year = {1997},
  volume = {16},
  pages = {1753--1764},
  number = {15},
  doi = {10.1002/(SICI)1097-0258(19970815)16:15<1753::AID-SIM597>3.0.CO;2-E},
  issn = {1097-0258},
  owner = {meddwilb},
  publisher = {John Wiley \& Sons, Ltd.},
  timestamp = {2013.09.10},
  url = {http://dx.doi.org/10.1002/(SICI)1097-0258(19970815)16:15<1753::AID-SIM597>3.0.CO;2-E}
}

@ARTICLE{Klar2001,
  author = {Klar, Neil and Donner, Allan},
  title = {Current and future challenges in the design and analysis of cluster
	randomization trials},
  journal = {Statistics in Medicine},
  year = {2001},
  volume = {20},
  pages = {3729--3740},
  number = {24},
  abstract = {Randomized trials in which the unit of randomization is a community,
	worksite, school or family are becoming widely used in the evaluation
	of life-style interventions for the prevention of disease. The increasing
	interest in adopting a cluster randomization design is being matched
	by rapid methodological developments. In this paper we describe several
	of these developments. Brief mention is also made of issues related
	to economic analysis and to the planning and conduct of meta-analyses
	for cluster randomization trials. Recommendations for reporting are
	also discussed. Copyright Â© 2001 John Wiley & Sons, Ltd.},
  doi = {10.1002/sim.1115},
  issn = {1097-0258},
  owner = {meddwilb},
  publisher = {John Wiley \& Sons, Ltd.},
  timestamp = {2014.10.22},
  url = {http://dx.doi.org/10.1002/sim.1115}
}

@ARTICLE{Kleijnen2010,
  author = {Jack P.C. Kleijnen and Wim van Beers and Inneke van Nieuwenhuyse},
  title = {Constrained optimization in expensive simulation: Novel approach
	},
  journal = {European Journal of Operational Research },
  year = {2010},
  volume = {202},
  pages = {164 - 174},
  number = {1},
  abstract = {This article presents a novel heuristic for constrained optimization
	of computationally expensive random simulation models. One output
	is selected as objective to be minimized, while other outputs must
	satisfy given threshold values. Moreover, the simulation inputs must
	be integer and satisfy linear or nonlinear constraints. The heuristic
	combines (i) sequentialized experimental designs to specify the simulation
	input combinations, (ii) Kriging (or Gaussian process or spatial
	correlation modeling) to analyze the global simulation input/output
	data resulting from these designs, and (iii) integer nonlinear programming
	to estimate the optimal solution from the Kriging metamodels. The
	heuristic is applied to an ( s , S ) inventory system and a call-center
	simulation, and compared with the popular commercial heuristic OptQuest
	embedded in the Arena versions 11 and 12. In these two applications
	the novel heuristic outperforms OptQuest in terms of number of simulated
	input combinations and quality of the estimated optimum. },
  doi = {http://dx.doi.org/10.1016/j.ejor.2009.05.002},
  issn = {0377-2217},
  keywords = {Simulation},
  owner = {meddwilb},
  timestamp = {2014.01.30},
  url = {http://www.sciencedirect.com/science/article/pii/S0377221709003130}
}

@Article{Koyama2008,
  author    = {Koyama, Tatsuki and Chen, Heidi},
  title     = {Proper inference from Simon's two-stage designs},
  year      = {2008},
  volume    = {27},
  number    = {16},
  pages     = {3145--3154},
  issn      = {1097-0258},
  doi       = {10.1002/sim.3123},
  url       = {http://dx.doi.org/10.1002/sim.3123},
  abstract  = {Simon's two-stage designs are very popular for phase II clinical trials.
	A literature review revealed that the inference procedures used with
	Simon's designs almost always ignore the actual sampling plan used.
	Reported P-values, point estimates and confidence intervals for the
	response rate are not usually adjusted for the design's adaptiveness.
	In addition, we found that the actual sample size for the second
	stage is often different from that planned. We present here a method
	for inferences using both the planned and the actual sample sizes.
	The conventional and the preferred inference procedures usually yield
	similar P-values and confidence intervals for the response rate.
	The conventional inference, however, may contradict the result of
	the corresponding hypothesis testing. Copyright Â© 2007 John Wiley
	& Sons, Ltd.},
  groups    = {Phase II},
  journal   = {Statistics in Medicine},
  keywords  = {clinical trials, adaptive design, conditional power, sample size modification, P-values},
  owner     = {meddwilb},
  publisher = {John Wiley \& Sons, Ltd.},
  timestamp = {2014.02.28},
}

@ARTICLE{Krull1999,
  author = {Krull, Jennifer L. and Mackinnon, David P.},
  title = {Multilevel Mediation Modeling in Group-Based Intervention Studies},
  journal = {Evaluation Review},
  year = {1999},
  volume = {23},
  pages = {418-444},
  number = {4},
  abstract = { This article proposes and evaluates a method to test for mediation
	in multilevel data sets formed when an intervention administered
	to intact groups is designed to produce change in individual mediator
	and outcome variables. Simulated data of this form were used to compare
	ordinary least squares (OLS) and two multilevel estimators of the
	mediated effect. OLS and multilevel standard error approximations
	were also evaluated and recommendations given for optimal estimator
	choice. These methods were applied to data from an existing substance
	use intervention to show the impact multilevel mediation modeling
	can have on the conclusions drawn from real-world evaluation studies.
	},
  doi = {10.1177/0193841X9902300404},
  eprint = {http://erx.sagepub.com/content/23/4/418.full.pdf+html},
  owner = {meddwilb},
  timestamp = {2013.09.11},
  url = {http://erx.sagepub.com/content/23/4/418.abstract}
}

@ARTICLE{Ladabaum2010,
  author = {Ladabaum, Uri and Sharabidze, Annie and Levin, Theodore R. and Zhao,
	Wei K. and Chung, Elaine and Bacchetti, Peter and Jin, Chengshi and
	Grimes, Barbara and Pepin, Craig J.},
  title = {Citalopram Provides Little or No Benefit in Nondepressed Patients
	With Irritable Bowel Syndrome},
  journal = {Clinical Gastroenterology and Hepatology},
  year = {2010},
  volume = {8},
  pages = {42--48.e1},
  number = {1},
  abstract = {Background & AimsData on the benefit of selective serotonin reuptake
	inhibitors (SSRIs) in irritable bowel syndrome (IBS) are conflicting.
	The longitudinal relationship between clinical symptoms and sensitivity
	to barostat-mediated rectal distension in IBS remains unclear. We
	assessed the benefit of citalopram and explored the relationships
	between symptoms, quality of life (QOL), and rectal sensitivity to
	barostat distension in non-depressed IBS patients.
	
	Background & AimsData on the benefit of selective serotonin reuptake
	inhibitors (SSRIs) in irritable bowel syndrome (IBS) are conflicting.
	The longitudinal relationship between clinical symptoms and sensitivity
	to barostat-mediated rectal distension in IBS remains unclear. We
	assessed the benefit of citalopram and explored the relationships
	between symptoms, quality of life (QOL), and rectal sensitivity to
	barostat distension in non-depressed IBS patients.},
  booktitle = {Clinical Gastroenterology and Hepatology},
  comment = {doi: 10.1016/j.cgh.2009.09.008},
  doi = {10.1016/j.cgh.2009.09.008},
  owner = {meddwilb},
  publisher = {Elsevier},
  timestamp = {2014.06.11},
  url = {http://www.cghjournal.org/article/S1542-3565(09)00891-X/abstract}
}

@Article{Lake2002,
  author    = {Lake, Stephen and Kammann, Erin and Klar, Neil and Betensky, Rebecca},
  title     = {Sample size re-estimation in cluster randomization trials},
  year      = {2002},
  volume    = {21},
  number    = {10},
  pages     = {1337--1350},
  issn      = {1097-0258},
  doi       = {10.1002/sim.1121},
  url       = {http://dx.doi.org/10.1002/sim.1121},
  groups    = {SSR},
  journal   = {Statistics in Medicine},
  owner     = {meddwilb},
  publisher = {John Wiley \& Sons, Ltd.},
  timestamp = {2013.09.10},
}

@Article{Lancaster2004,
  author    = {Lancaster, Gillian A. and Dodd, Susanna and Williamson, Paula R.},
  title     = {Design and analysis of pilot studies: recommendations for good practice},
  year      = {2004},
  volume    = {10},
  number    = {2},
  pages     = {307--312},
  issn      = {1365-2753},
  doi       = {10.1111/j..2002.384.doc.x},
  url       = {http://dx.doi.org/10.1111/j..2002.384.doc.x},
  groups    = {Pilot/feasibility},
  journal   = {Journal of Evaluation in Clinical Practice},
  keywords  = {feasibility, methodology, pilot, randomized controlled trial, scientific rigour },
  owner     = {meddwilb},
  publisher = {Blackwell Science Ltd},
  timestamp = {2013.10.09},
}

@ARTICLE{Lancaster2010,
  author = {Lancaster, GA and Campbell, MJ and Eldridge, S. and Farrin, A. and
	Marchant, M. and Muller, S. and Perera, R. and Peters, TJ and Prevost,
	AT and Rait, G.},
  title = {Trials in primary care: statistical issues in the design, conduct
	and evaluation of complex interventions},
  journal = {Statistical Methods in Medical Research},
  year = {2010},
  volume = {19},
  pages = {349-377},
  number = {4},
  abstract = {Trials carried out in primary care typically involve complex interventions
	that require considerable planning if they are to be implemented
	successfully. The role of the statistician in promoting both robust
	study design and appropriate statistical analysis is an important
	contribution to a multi-disciplinary primary care research group.
	Issues in the design of complex interventions have been addressed
	in the Medical Research Councilâ€™s new guidance document and over
	the past 7 years by the Royal Statistical Societyâ€™s Primary Health
	Care Study Group. With the aim of raising the profile of statistics
	and building research capability in this area, particularly with
	respect to methodological issues, the study group meetings have covered
	a wide range of topics that have been of interest to statisticians
	and non-statisticians alike. The aim of this article is to provide
	an overview of the statistical issues that have arisen over the years
	related to the design and evaluation of trials in primary care, to
	provide useful examples and references for further study and ultimately
	to promote good practice in the conduct of complex interventions
	carried out in primary care and other health care settings. Throughout
	we have given particular emphasis to statistical issues related to
	the design of cluster randomised trials.},
  doi = {10.1177/0962280209359883},
  eprint = {http://smm.sagepub.com/content/19/4/349.full.pdf+html},
  owner = {meddwilb},
  timestamp = {2013.09.17},
  url = {http://smm.sagepub.com/content/19/4/349.abstract}
}

@Article{Landau2013,
  author    = {Landau, Sabine and Stahl, Daniel},
  title     = {Sample size and power calculations for medical studies by simulation when closed form expressions are not available},
  year      = {2013},
  volume    = {22},
  number    = {3},
  pages     = {324-345},
  doi       = {10.1177/0962280212439578},
  eprint    = {http://smm.sagepub.com/content/22/3/324.full.pdf+html},
  url       = {http://smm.sagepub.com/content/22/3/324.abstract},
  abstract  = {This paper shows how Monte Carlo simulation can be used for sample
	size, power or precision calculations when planning medical research
	studies. Standard study designs can lead to the use of analysis methods
	for which power formulae do not exist. This may be because complex
	modelling techniques with optimal statistical properties are used
	but power formulae have not yet been derived or because analysis
	models are employed that divert from the population model due to
	lack of availability of more appropriate analysis tools. Our presentation
	concentrates on the conceptual steps involved in carrying out power
	or precision calculations by simulation. We demonstrate these steps
	in three examples concerned with (i) drop out in longitudinal studies,
	(ii) measurement error in observational studies and (iii) causal
	effect estimation in randomised controlled trials with non-compliance.
	We conclude that the Monte Carlo simulation approach is an important
	general tool in the methodological arsenal for assessing power and
	precision.},
  groups    = {multi-D / sim SS},
  journal   = {Statistical Methods in Medical Research},
  owner     = {meddwilb},
  timestamp = {2014.07.25},
}

@Article{Lee2014,
  author    = {Lee, Ellen and Whitehead, Amy and Jacques, Richard and Julious, Steven},
  title     = {The statistical interpretation of pilot trials: should significance thresholds be reconsidered?},
  year      = {2014},
  volume    = {14},
  number    = {1},
  pages     = {41},
  issn      = {1471-2288},
  doi       = {10.1186/1471-2288-14-41},
  url       = {http://www.biomedcentral.com/1471-2288/14/41},
  abstract  = {BACKGROUND:In an evaluation of a new health technology, a pilot trial
	may be undertaken prior to a trial that makes a definitive assessment
	of benefit. The objective of pilot studies is to provide sufficient
	evidence that a larger definitive trial can be undertaken and, at
	times, to provide a preliminary assessment of benefit.METHODS:We
	describe significance thresholds, confidence intervals and surrogate
	markers in the context of pilot studies and how Bayesian methods
	can be used in pilot trials. We use a worked example to illustrate
	the issues raised.RESULTS:We show how significance levels other than
	the traditional 5% should be considered to provide preliminary evidence
	for efficacy and how estimation and confidence intervals should be
	the focus to provide an estimated range of possible treatment effects.
	We also illustrate how Bayesian methods could also assist in the
	early assessment of a health technology.CONCLUSIONS:We recommend
	that in pilot trials the focus should be on descriptive statistics
	and estimation, using confidence intervals, rather than formal hypothesis
	testing and that confidence intervals other than 95% confidence intervals,
	such as 85% or 75%, be used for the estimation. The confidence interval
	should then be interpreted with regards to the minimum clinically
	important difference. We also recommend that Bayesian methods be
	used to assist in the interpretation of pilot trials. Surrogate endpoints
	can also be used in pilot trials but they must reliably predict the
	overall effect on the clinical outcome.},
  groups    = {Pilot/feasibility},
  journal   = {BMC Medical Research Methodology},
  owner     = {meddwilb},
  pubmedid  = {24650044},
  timestamp = {2014.12.04},
}

@ARTICLE{Lee2005a,
  author = {Lee, J. Jack and Feng, Lei},
  title = {Randomized Phase {II} Designs in Cancer Clinical Trials: Current
	Status and Future Directions},
  journal = {Journal of Clinical Oncology},
  year = {2005},
  volume = {23},
  pages = {4450-4457},
  number = {19},
  abstract = {Purpose Randomized phase II (RPh2) designs are popular in cancer clinical
	trials because of the smaller sample size requirements when multiple
	treatments are being evaluated. We reviewed the use of RPh2 designs
	and give comments on future directions.Design The trial design, statistical
	properties, conduct, data analysis, results, and reporting were examined
	in RPh2 trials reported from 1986 to 2002.Results A statistical design
	was reported in only 46% of the 266 cancer trials, and approximately
	half of those provided inadequate information. Most studies applied
	randomization to achieve patient comparability, while embedding a
	one-sample phase II design within each treatment arm. Seventy-five
	percent of the trialsâ€™ accruals were within Â± 10% of their targets.
	The average accrual rate was 3.3 patients per month. Planned interim
	analyses were reported in 27% of the trials, and 56% of the trials
	were stopped early; 69%, 13%, 13%, and 4% of the trial discontinuations
	were because of lack of efficacy, efficacy, toxicity, and slow accrual,
	respectively. Thirty-nine trials (14%) recommended or started phase
	III evaluations, with four positive reports in six phase III studies
	identified.Conclusion There is a trend of increasing use of RPh2
	designs in cancer research. Continued improvement in study design,
	conduct, analysis, and reporting is required to enhance the quality
	of RPh2 designs. The accrual rate and success rate of the trials
	remain low, and therefore, futility stopping rules to terminate ineffective
	treatment arm(s) should be implemented more frequently. More innovative,
	flexible RPh2 designs are needed to facilitate the development of
	effective cancer treatments.},
  doi = {10.1200/JCO.2005.03.197},
  eprint = {http://jco.ascopubs.org/content/23/19/4450.full.pdf+html},
  owner = {meddwilb},
  timestamp = {2014.08.07},
  url = {http://jco.ascopubs.org/content/23/19/4450.abstract}
}

@ARTICLE{Lee2005,
  author = {Lee, Katherine J and Thompson, Simon G},
  title = {The use of random effects models to allow for clustering in individually
	randomized trials},
  journal = {Clinical Trials},
  year = {2005},
  volume = {2},
  pages = {163-173},
  number = {2},
  abstract = { Background We describe different forms of clustering that may occur
	in individually randomized trials, where the observed outcomes for
	different individuals cannot be regarded as independent. We propose
	random effects models to allow for such clustering, across a range
	of contexts and trial designs, and investigate their effect on estimation
	and interpretation of the treatment effect. Methods We apply our
	proposed models to two individually randomized trials with potential
	for clustering, a trial of teleconsultation in hospital referral
	(the main outcome being offer of a further hospital appointment)
	and a trial of exercise therapy delivered by physiotherapists for
	low back pain (the outcome being a back pain score). Extensions to
	the methods include the possibility of explaining heterogeneity between
	clusters using cluster level characteristics and the potential dilution
	of cluster effects due to noncompliance. Results In the teleconsultation
	trial, the odds ratio was significant (1.52, 95% CI 1.27 to 1.82)
	when clustering was ignored, but smaller and nonsignificant (1.36,
	95% CI 0.85 to 2.13) when clustering by hospital consultant was taken
	into account. The 95% range of estimated treatment effects across
	consultants was from 0.21 to 8.76. This variability was only partially
	explained by the specialty of the consultant. In the back pain trial,
	although there was an overall benefit of exercise (change of 20.51
	points on the back pain score) and little evidence of clustering,
	the estimated treatment effects for different physiotherapists ranged
	from 21.26 to +0.26 points. Conclusions Clustering is an important
	issue in many individually randomized trials. Ignoring it can lead
	to underestimates of the uncertainty and too extreme P-values. Even
	when there is little apparent heterogeneity across clusters, it can
	still have a large impact on the estimation and interpretation of
	the treatment effect.},
  doi = {10.1191/1740774505cn082oa},
  eprint = {http://ctj.sagepub.com/content/2/2/163.full.pdf+html},
  owner = {meddwilb},
  timestamp = {2014.07.21},
  url = {http://ctj.sagepub.com/content/2/2/163.abstract}
}

@ARTICLE{Lee2000,
  author = {Lee, Sandra J. and Zelen, Marvin},
  title = {Clinical Trials and Sample Size Considerations: Another Perspective},
  journal = {Statistical Science},
  year = {2000},
  volume = {15},
  pages = {pp. 95-103},
  number = {2},
  abstract = {We propose a Bayesian formulation of the sample size problem for planning
	clinical trials. The frequentist paradigm for calculating sample
	sizes for clinical trials is to prespecify the type I and II error
	probabilities. These error probabilities are conditional on the true
	hypotheses. Instead we propose prespecifying posterior probabilities
	which are conditional on the outcome of the trial. Our method is
	easy to implement and has intuitive interpretations. We illustrate
	an application of our method to the planning of cancer clinical trials
	for the Eastern Cooperative Oncology Group (ECOG).},
  owner = {meddwilb},
  timestamp = {2014.01.14},
  url = {http://www.jstor.org/stable/2676721}
}

@ARTICLE{Leemis2008,
  author = {Leemis, Lawrence M and McQueston, Jacquelyn T},
  title = {Univariate Distribution Relationships},
  journal = {The American Statistician},
  year = {2008},
  volume = {62},
  pages = {45-53},
  number = {1},
  abstract = { Probability distributions are traditionally treated separately in
	introductory mathematical statistics textbooks. A figure is presented
	here that shows properties that individual distributions possess
	and many of the relationships between these distributions. },
  doi = {10.1198/000313008X270448},
  eprint = {http://www.tandfonline.com/doi/pdf/10.1198/000313008X270448},
  owner = {meddwilb},
  timestamp = {2014.03.24},
  url = {http://www.tandfonline.com/doi/abs/10.1198/000313008X270448}
}

@ARTICLE{Lehmann1993,
  author = {Lehmann, E. L.},
  title = {The Fisher, Neyman-Pearson Theories of Testing Hypotheses: One Theory
	or Two?},
  journal = {Journal of the American Statistical Association},
  year = {1993},
  volume = {88},
  pages = {1242-1249},
  number = {424},
  abstract = { Abstract The Fisher and Neyman-Pearson approaches to testing statistical
	hypotheses are compared with respect to their attitudes to the interpretation
	of the outcome, to power, to conditioning, and to the use of fixed
	significance levels. It is argued that despite basic philosophical
	differences, in their main practical aspects the two theories are
	complementary rather than contradictory and that a unified approach
	is possible that combines the best features of both. As applications,
	the controversies about the Behrens-Fisher problem and the comparison
	of two binomials (2 Ã— 2 tables) are considered from the present
	point of view. },
  doi = {10.1080/01621459.1993.10476404},
  eprint = { http://amstat.tandfonline.com/doi/pdf/10.1080/01621459.1993.10476404
	},
  file = {:P\:\\CTRU\\Projects\\Health_Sciences\\Methodology\\2013 NIHR Research Methods Fellowship\\Literature\\Papers\\Lehmann1993.pdf:PDF},
  owner = {meddwilb},
  timestamp = {2014.04.24},
  url = { http://amstat.tandfonline.com/doi/abs/10.1080/01621459.1993.10476404 
}
}

@ARTICLE{Lilford1995,
  author = {Richard J Lilford and J G Thornton and D Braunholtz},
  title = {Clinical trials and rare diseases: a way out of a conundrum},
  journal = {BMJ},
  year = {1995},
  volume = {311},
  pages = {1621--1625},
  number = {7020},
  month = {12},
  doi = {10.1136/bmj.311.7020.1621},
  owner = {meddwilb},
  timestamp = {2014.06.13},
  url = {http://www.bmj.com/content/311/7020/1621?variant=full-text}
}

@Article{Lin2008,
  author    = {Xun Lin and Randy Allred and Glen Andrews},
  title     = {A two-stage phase {II} trial design utilizing both primary and secondary endpoints},
  journal   = {Pharmaceut. Statist.},
  year      = {2008},
  volume    = {7},
  number    = {2},
  pages     = {88--92},
  doi       = {10.1002/pst.255},
  url       = {http://dx.doi.org/10.1002/pst.255},
  comment   = {Considers 2 efficacy endpoints. Extends Simon's two stage design. At the interim analysis only the primary response endpoint is considered in the decision rule, but at the final analysis the other endpoint is also considered (the other endpoint requires more time to measure so not enough information is available to use it at the interim). The test at the second stage is of a null 00 against an alternative 01 OR 10. The second endpoint was 'clinical benefit' and was defined as response (i.e. the primary endpoint or disease stabilisation - so a composite endpoint. Response and stabilisation were mutually exclusive and modelled with a multinomial distribution. Error rates assessed through simulation, and finds desired cut-off points numerically.},
  groups    = {Multiple endpoints},
  owner     = {meddwilb},
  publisher = {Wiley-Blackwell},
  timestamp = {2016.07.25},
}

@Book{Lindley1985,
  author    = {Dennis V. Lindley},
  title     = {Making Decisions},
  year      = {1985},
  edition   = {2nd},
  publisher = {John Wiley \& Sons, Ltd},
  groups    = {Bayesian, Decision theory},
  owner     = {meddwilb},
  timestamp = {2016.07.06},
}

@Article{Lindley1997,
  author    = {Lindley, Dennis V.},
  title     = {The choice of sample size},
  year      = {1997},
  volume    = {46},
  number    = {2},
  pages     = {129--138},
  issn      = {1467-9884},
  doi       = {10.1111/1467-9884.00068},
  url       = {http://dx.doi.org/10.1111/1467-9884.00068},
  groups    = {SSR, Bayesian, Decision theory},
  journal   = {Journal of the Royal Statistical Society: Series D (The Statistician)},
  keywords  = {Binomial sampling, Coherence, Coverage probabilities, Highest posterior density intervals, Information, Log-odds, Maximization of expected utility, Sample size determination},
  owner     = {meddwilb},
  publisher = {Blackwell Publishers Ltd},
  timestamp = {2013.10.22},
}

@Article{Liu2003,
  author =    {Liu, Xiaofeng},
  title =     {Statistical Power and Optimum Sample Allocation Ratio for Treatment and Control Having Unequal Costs per Unit of Randomization},
  journal =   {Journal of Educational and Behavioral Statistics},
  year =      {2003},
  volume =    {28},
  number =    {3},
  pages =     {231-248},
  abstract =  {This article considers optimal sample allocation between the treatment and control condition in multilevel designs when the costs per sampling unit vary due to treatment assignment. Optimal unequal allocation may reduce the cost from that of a balanced design without sacrificing any power. The optimum sample allocation ratio depends only on the cost ratio between the treatment and control regardless of whether the randomization of sampling units occurs at levels 1, 2, or 3. Power functions for the exact tests for the main effect of treatment are derived for prototypical multilevel designs with unequal sample sizes in the treatment and control condition.},
  doi =       {10.3102/10769986028003231},
  eprint =    {http://jeb.sagepub.com/content/28/3/231.full.pdf+html},
  owner =     {meddwilb},
  timestamp = {2016.02.16},
  url =       {http://jeb.sagepub.com/content/28/3/231.abstract}
}

@ARTICLE{Loeb2002,
  author = {Loeb, Mark},
  title = {Application of the development stages of a cluster randomized trial
	to a framework for evaluating complex health interventions},
  journal = {BMC Health Services Research},
  year = {2002},
  volume = {2},
  pages = {13},
  number = {1},
  abstract = {INTRODUCTION:Trials of complex health interventions often pose difficult
	methodologic challenges. The objective of this paper is to assess
	the extent to which the various development steps of a cluster randomized
	trial to optimize antibiotic use in nursing homes are represented
	in a recently published framework for the design and evaluation of
	complex health interventions. In so doing, the utility of the framework
	for health services researchers is evaluated.METHODS:Using the five
	phases of the framework (theoretical, identification of components
	of the intervention, definition of trial and intervention design,
	methodological issues for main trial, promoting effective implementation),
	corresponding stages in the development of the cluster randomized
	trial using diagnostic and treatment algorithms to optimize the use
	of antibiotics in nursing homes are identified and described.RESULTS:Synthesis
	of evidence needed to construct the algorithms, survey and qualitative
	research used to define components of the algorithms, a pilot study
	to assess the feasibility of delivering the algorithms, methodological
	issues in the main trial including choice of design, allocation concealment,
	outcomes, sample size calculation, and analysis are adequately represented
	using the stages of the framework.CONCLUSIONS:The framework is a
	useful resource for researchers planning a randomized clinical trial
	of a complex intervention.},
  doi = {10.1186/1472-6963-2-13},
  issn = {1472-6963},
  owner = {meddwilb},
  pubmedid = {12110157},
  timestamp = {2013.09.17},
  url = {http://www.biomedcentral.com/1472-6963/2/13}
}

@Article{Lovell2008,
  author    = {Lovell, Karina and Bower, Peter and Richards, David and Barkham, Michael and Sibbald, Bonnie and Roberts, Chris and Davies, Linda and Rogers, Anne and Gellatly, Judith and Hennessy, Sue},
  title     = {Developing guided self-help for depression using the Medical Research Council complex interventions framework: a description of the modelling phase and results of an exploratory randomised controlled trial},
  year      = {2008},
  volume    = {8},
  number    = {1},
  pages     = {91},
  issn      = {1471-244X},
  doi       = {10.1186/1471-244X-8-91},
  url       = {http://www.biomedcentral.com/1471-244X/8/91},
  abstract  = {BACKGROUND:Current guidelines for the management of depression suggest
	the use of guided self-help for patients with mild to moderate disorders.
	However, there is little consensus concerning the optimal form and
	delivery of this intervention. To develop acceptable and effective
	interventions, a phased process has been proposed, using a modelling
	phase to examine and develop an intervention prior to preliminary
	testing in an exploratory trial. This paper (a) describes the modelling
	phase used to develop a guided self-help intervention for depression
	in primary care and (b) reports data from an exploratory randomised
	trial of the intervention.METHODS:A guided self-help intervention
	was developed following a modelling phase which involved a systematic
	review, meta synthesis and a consensus process. The intervention
	was then tested in an exploratory randomised controlled trial by
	examining (a) fidelity using analysis of taped guided self-help sessions
	(b) acceptability to patients and professionals through qualitative
	interviews (c) effectiveness through estimation of the intervention
	effect size.RESULTS:Fifty eight patients were recruited to the exploratory
	trial. Seven professionals and nine patients were interviewed, and
	22 tapes of sessions analysed for fidelity. Generally, fidelity to
	the intervention protocol was high, and the professionals delivered
	the majority of the specific components (with the exception of the
	use of feedback). Acceptability to both professionals and patients
	was also high. The effect size of the intervention on outcomes was
	small, and in line with previous analyses showing the modest effect
	of guided self-help in primary care. However, the sample size was
	small and confidence intervals around the effectiveness estimate
	were wide.CONCLUSION:The general principles of the modelling phase
	adopted in this study are designed to draw on a range of evidence,
	potentially providing an intervention that is evidence-based, patient-centred
	and acceptable to professionals. However, the pilot outcome data
	did not suggest that the intervention developed was particularly
	effective. The advantages and disadvantages of the general methods
	used in the modelling phase are discussed, and possible reasons for
	the failure to demonstrate a larger effect in this particular case
	are outlined.},
  groups    = {Barkham},
  journal   = {BMC Psychiatry},
  owner     = {meddwilb},
  pubmedid  = {19025646},
  timestamp = {2015.01.23},
}

@BOOK{Machin2009,
  title = {Sample size tables for clinical studies},
  publisher = {Wiley-Blackwell},
  year = {2009},
  author = {David Machin and Michael Campbell and Perter Fayers and Alain Pinol},
  edition = {3rd},
  owner = {meddwilb},
  timestamp = {2014.01.03}
}

@ARTICLE{Manatunga2001,
  author = {Manatunga, Amita K. and Hudgens, Michael G. and Chen, Shande},
  title = {Sample Size Estimation in Cluster Randomized Studies with Varying
	Cluster Size},
  journal = {Biometrical Journal},
  year = {2001},
  volume = {43},
  pages = {75--86},
  number = {1},
  abstract = {Cluster randomized studies are common in community trials. The standard
	method for estimating sample size for cluster randomized studies
	assumes a common cluster size. However often in cluster randomized
	studies, size of the clusters vary. In this paper, we derive sample
	size estimation for continuous outcomes for cluster randomized studies
	while accounting for the variability due to cluster size. It is shown
	that the proposed formula for estimating total cluster size can be
	obtained by adding a correction term to the traditional formula which
	uses the average cluster size. Application of these results to the
	design of a health promotion educational intervention study is discussed.},
  doi = {10.1002/1521-4036(200102)43:1<75::AID-BIMJ75>3.0.CO;2-N},
  issn = {1521-4036},
  keywords = {Cluster randomized, Longitudinal data, Sample size, Varying cluster
	size},
  owner = {meddwilb},
  publisher = {WILEY-VCH Verlag Berlin GmbH},
  timestamp = {2014.08.29},
  url = {http://dx.doi.org/10.1002/1521-4036(200102)43:1<75::AID-BIMJ75>3.0.CO;2-N}
}

@ARTICLE{Mander2012,
  author = {Mander, Adrian P. and Wason, James M.S. and Sweeting, Michael J.
	and Thompson, Simon G.},
  title = {Admissible two-stage designs for phase {II} cancer clinical trials
	that incorporate the expected sample size under the alternative hypothesis},
  journal = {Pharmaceutical Statistics},
  year = {2012},
  volume = {11},
  pages = {91--96},
  number = {2},
  abstract = {two-stage studies may be chosen optimally by minimising a single characteristic
	like the maximum sample size. However, given that an investigator
	will initially select a null treatment eï¬€ect and the clinically
	relevant diï¬€erence, it is better to choose a design that also considers
	the expected sample size for each of these values. The maximum sample
	size and the two expected sample sizes are here combined to produce
	an expected loss function to ï¬?nd designs that are admissible. Given
	the prior odds of success and the importance of the total sample
	size, minimising the expected loss gives the optimal design for this
	situation. A novel triangular graph to represent the admissible designs
	helps guide the decision-making process. The H 0-optimal, H 1-optimal,
	H 0-minimax and H 1-minimax designs are all particular cases of admissible
	designs. The commonly used H 0-optimal design is rarely good when
	allowing stopping for eï¬ƒcacy. Additionally, the Î´-minimax design,
	which minimises the maximum expected sample size, is sometimes admissible
	under the loss function. However, the results can be varied and each
	situation will require the evaluation of all the admissible designs.
	Software to do this is provided. Copyright Â© 2012 John Wiley & Sons,
	Ltd.},
  doi = {10.1002/pst.501},
  issn = {1539-1612},
  keywords = {two-stage trial design, optimal design, admissible designs, phase
	II clinical trials},
  owner = {meddwilb},
  publisher = {John Wiley \& Sons, Ltd},
  timestamp = {2013.12.11},
  url = {http://dx.doi.org/10.1002/pst.501}
}

@ARTICLE{Manor2004,
  author = {Orly Manor and David M Zucker},
  title = {Small sample inference for the fixed effects in the mixed linear
	model },
  journal = {Computational Statistics \& Data Analysis },
  year = {2004},
  volume = {46},
  pages = {801 - 817},
  number = {4},
  abstract = {The small sample performance of several procedures for testing a given
	fixed effect in a mixed linear model is investigated. Using simulations,
	constructed on the basis of a study of growth of children with Gaucher's
	disease, standard normal-theory Wald tests for both \{ML\} and \{REML\}
	estimates, the likelihood ratio test (LRT), a modified \{LRT\} based
	on Bartlett correction, and a number of adjusted tests based on t
	and F distributions are evaluated. Methods used for determining the
	denominator degrees of freedom in the t and F tests include the residual
	degrees of freedom method, the between and within degrees of freedom,
	the containment method, the naive method and the Satterthwaite method.
	A test based on a sandwich-type estimator of the variance of the
	parameter estimate is evaluated as well and the effect of mis-specifying
	the random-effects distribution is considered. Results show that
	Type I error rates for the Wald-type test with chi-square approximation
	are substantially inflated, though less so with \{REML\} estimates
	than with \{ML\} estimates. The \{LRT\} based on \{ML\} estimates
	yielded Type I error rates similar to those observed for the Wald-type
	chi-square test with \{REML\} estimates. A substantial improvement
	in Type I error rates for testing on both the intercept and slope
	is provided by each of the three following modifications: the Satterthwaite
	and naive methods with REML-based estimates and the Bartlett-corrected
	LRT. },
  doi = {http://dx.doi.org/10.1016/j.csda.2003.10.005},
  issn = {0167-9473},
  keywords = {Longitudinal studies},
  owner = {meddwilb},
  timestamp = {2014.11.10},
  url = {http://www.sciencedirect.com/science/article/pii/S016794730300238X}
}

@ARTICLE{Mariani1996,
  author = {Mariani, L. and Marubini, E.},
  title = {Design and Analysis of Phase {II} Cancer Trials: A Review of Statistical
	Methods and Guidelines for Medical Researchers},
  journal = {International Statistical Review / Revue Internationale de Statistique},
  year = {1996},
  volume = {64},
  pages = {61-88},
  number = {1},
  abstract = {A number of statistical methods have been proposed in the last three
	decades for the design and analysis of phase II trials in the oncological
	field. Herein, a review of these methods is supplied. Practical guidelines
	for the proper choice of study design and some worked examples are
	also given. Based on this material, it appears that one-sample studies
	are likely to yield reliable results in early phase II testing or
	when the trial is of a comparative nature, provided that an accurate
	estimate of the response rate is available for standard treatment.
	Multi-stage designs are useful for reducing, on the average, the
	number of patients required. In situations different from those cited
	herein, controlled trials are preferable but require a larger number
	of patients. This drawback can be lessened through inclusion in the
	analysis of information from historical controls or the adoption
	of early stopping rules. /// Dans les trois derniÃ¨res dÃ©cennies,
	plusieurs mÃ©thodes statistiques ont Ã©tÃ© proposÃ©es en oncologie
	pour le plan et l'analyse des essais thÃ©rapeutiques de phase II.
	Cet article est la prÃ©sentation de l'ensemble des mÃ©thodes proposÃ©es
	dans la litterature. De mÃªme, nous donnons le dÃ©veloppement de
	quelques exemples et des indications pratiques pour la choix appropriÃ©
	d'un plan d'essai. A ce propos, on remarque que: 1) les essais non-controlÃ©es
	produisent gÃ©nÃ¨ralement des rÃ©sultats fiables pour les essais
	initiaux de phase II et pour les essais comparatifs; pour les essais
	comparatifs, il doit exister une estimation soignÃ©e du taux de rÃ©ponse
	avec le traitement standard; 2) l'analyse sÃ©quentielle de groupes
	est utile pour rÃ©duire le nombre de patients nÃ©cessaires en moyenne
	dans l'essai thÃ©rapeutique. Dans d'autres situations, les essais
	contrÃ´lÃ©es sont prÃ©fÃ©rables mais ils demandent un plus grand
	nombre de sujets. Cependant, l'adoption d'analyses intermediaires
	ou l'analyse basÃ©e sur le contrÃ´le historique peuvent le reduire.},
  copyright = {Copyright Â© 1996 International Statistical Institute (ISI)},
  issn = {03067734},
  jstor_articletype = {research-article},
  jstor_formatteddate = {Apr., 1996},
  language = {English},
  owner = {meddwilb},
  publisher = {International Statistical Institute (ISI)},
  timestamp = {2014.07.04},
  url = {http://www.jstor.org/stable/1403424}
}

@ARTICLE{Martinez-Cantin2014,
  author = {Ruben Martinez{-}Cantin},
  title = {BayesOpt: {A} Bayesian Optimization Library for Nonlinear Optimization,
	Experimental Design and Bandits},
  journal = {CoRR},
  year = {2014},
  volume = {abs/1405.7430},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl = {http://dblp.uni-trier.de/rec/bib/journals/corr/Martinez-Cantin14},
  owner = {meddwilb},
  timestamp = {Tue, 04 Nov 2014 12:50:11 +0100},
  url = {http://arxiv.org/abs/1405.7430}
}

@Article{McCracken2004,
  author    = {Lance M McCracken and Kevin E Vowles and Christopher Eccleston},
  title     = {Acceptance of chronic pain: component analysis and a revised assessment method},
  year      = {2004},
  volume    = {107},
  number    = {1â€“2},
  pages     = {159 - 166},
  issn      = {0304-3959},
  doi       = {http://dx.doi.org/10.1016/j.pain.2003.10.012},
  url       = {http://www.sciencedirect.com/science/article/pii/S0304395903004299},
  journal   = {Pain},
  keywords  = {Chronic Pain Acceptance Questionnaire},
  owner     = {meddwilb},
  timestamp = {2014.02.27},
}

@BOOK{McCulloch2008,
  title = {Generalized, Linear, and Mixed Models},
  publisher = {Wiley},
  year = {2008},
  author = {Charles E. McCulloch and Shayle R. Searle and John M. Neuhaus},
  edition = {2nd},
  owner = {meddwilb},
  timestamp = {2013.11.22}
}

@ARTICLE{McCulloch2009,
  author = {Peter McCulloch and Douglas G Altman and W Bruce Campbell and David
	R Flum and Paul Glasziou and John C Marshall and Jon Nicholl},
  title = {No surgical innovation without evaluation: the {IDEAL} recommendations},
  journal = {The Lancet },
  year = {2009},
  volume = {374},
  pages = {1105 - 1112},
  number = {9695},
  abstract = {Summary Surgery and other invasive therapies are complex interventions,
	the assessment of which is challenged by factors that depend on operator,
	team, and setting, such as learning curves, quality variations, and
	perception of equipoise. We propose recommendations for the assessment
	of surgery based on a five-stage description of the surgical development
	process. We also encourage the widespread use of prospective databases
	and registries. Reports of new techniques should be registered as
	a professional duty, anonymously if necessary when outcomes are adverse.
	Case series studies should be replaced by prospective development
	studies for early technical modifications and by prospective research
	databases for later pre-trial evaluation. Protocols for these studies
	should be registered publicly. Statistical process control techniques
	can be useful in both early and late assessment. Randomised trials
	should be used whenever possible to investigate efficacy, but adequate
	pre-trial data are essential to allow power calculations, clarify
	the definition and indications of the intervention, and develop quality
	measures. Difficulties in doing randomised clinical trials should
	be addressed by measures to evaluate learning curves and alleviate
	equipoise problems. Alternative prospective designs, such as interrupted
	time series studies, should be used when randomised trials are not
	feasible. Established procedures should be monitored with prospective
	databases to analyse outcome variations and to identify late and
	rare events. Achievement of improved design, conduct, and reporting
	of surgical research will need concerted action by editors, funders
	of health care and research, regulatory bodies, and professional
	societies. },
  doi = {http://dx.doi.org/10.1016/S0140-6736(09)61116-8},
  issn = {0140-6736},
  owner = {meddwilb},
  timestamp = {2014.07.07},
  url = {http://www.sciencedirect.com/science/article/pii/S0140673609611168}
}

@ARTICLE{MebaneJr.2011,
  author = {Walter R. {Mebane, Jr.} and Jasjeet S. Sekhon},
  title = {Genetic Optimization Using Derivatives: The {rgenoud} Package for
	{R}},
  journal = {Journal of Statistical Software},
  year = {2011},
  volume = {42},
  pages = {1--26},
  number = {11},
  month = {6},
  accepted = {2007-11-21},
  bibdate = {2007-11-21},
  coden = {JSSOBK},
  day = {14},
  issn = {1548-7660},
  owner = {meddwilb},
  submitted = {2007-02-11},
  timestamp = {2014.08.11},
  url = {http://www.jstatsoft.org/v42/i11}
}

@BOOK{Miettinen1998,
  title = {Nonlinear multiobjemulti optimization},
  publisher = {Kluwer Academic Publishers},
  year = {1998},
  author = {Kaisa M. Miettinen},
  owner = {meddwilb},
  timestamp = {2014.06.13}
}

@Article{Miller2005,
  author    = {Miller, Frank},
  title     = {Variance Estimation in Clinical Studies with Interim Sample Size Reestimation},
  year      = {2005},
  volume    = {61},
  number    = {2},
  pages     = {355--361},
  issn      = {1541-0420},
  doi       = {10.1111/j.1541-0420.2005.00315.x},
  url       = {http://dx.doi.org/10.1111/j.1541-0420.2005.00315.x},
  groups    = {SSR},
  journal   = {Biometrics},
  keywords  = {Adaptive design, Bias of variance estimation, Bounds for bias, Clinical studies, Interim sample size reestimation, Variance estimation},
  owner     = {meddwilb},
  publisher = {Blackwell Publishing},
  timestamp = {2016.02.24},
}

@Article{Moerbeek2003,
  author =    {Mirjam Moerbeek and Gerard J.P. van Breukelen and Martijn P.F. Berger},
  title =     {A comparison between traditional methods and multilevel regression for the analysis of multicenter intervention studies },
  journal =   {Journal of Clinical Epidemiology },
  year =      {2003},
  volume =    {56},
  number =    {4},
  pages =     {341 - 350},
  abstract =  {This article reviews three traditional methods for the analysis of multicenter trials with persons nested within clusters, i.e., centers, namely naÄ±Ìˆve regression (persons as units of analysis), fixed effects regression, and the use of summary measures (clusters as units of analysis), and compares these methods with multilevel regression. The comparison is made for continuous (quantitative) outcomes, and is based on the estimator of the treatment effect and its standard error, because these usually are of main interest in intervention studies. When the results of the experiment have to be valid for some larger population of centers, the centers in the intervention study have to present a random sample from this population and multilevel regression may be used. It is shown that the treatment effect and especially its standard error, are generally incorrectly estimated by the traditional methods, which should, therefore, not in general be used as an alternative to multilevel regression. },
  doi =       {http://dx.doi.org/10.1016/S0895-4356(03)00007-6},
  issn =      {0895-4356},
  keywords =  {Nested data},
  owner =     {meddwilb},
  timestamp = {2016.02.16},
  url =       {http://www.sciencedirect.com/science/article/pii/S0895435603000076}
}

@Article{Moerbeek2000,
  author    = {Moerbeek, Mirjam and van Breukelen, Gerard J. P. and Berger, Martijn P. F.},
  title     = {Design Issues for Experiments in Multilevel Populations},
  year      = {2000},
  volume    = {25},
  number    = {3},
  pages     = {271-284},
  doi       = {10.3102/10769986025003271},
  eprint    = {http://jeb.sagepub.com/content/25/3/271.full.pdf+html},
  url       = {http://jeb.sagepub.com/content/25/3/271.abstract},
  abstract  = {For the design of experiments in multilevel populations the following questions may arise: What is the optimal level of randomization? Given a certain budget for sampling and measuring, what is the optimal allocation of units? What is the required budget for obtaining a certain power on the test of no treatment effect? In this article these questions will be dealt with for populations with two or three levels of nesting and continuous outcomes. Multilevel models are used to model the relationship between experimental condition and the outcome variable. The estimator of the regression, coefficient associated with treatment condition, a parameter assumed to be fixed in this paper; is of main interest and should be estimated as efficiently as possible. Therefore, its variance is used as a criterion for optimizing the level of randomization and the allocation of units.},
  groups    = {Multilevel, multi-D / sim SS},
  journal   = {Journal of Educational and Behavioral Statistics},
  owner     = {meddwilb},
  timestamp = {2016.02.16},
}

@Article{Moineddin2007,
  author    = {Moineddin, Rahim and Matheson, Flora and Glazier, Richard},
  title     = {A simulation study of sample size for multilevel logistic regression models},
  year      = {2007},
  volume    = {7},
  number    = {1},
  pages     = {34},
  issn      = {1471-2288},
  doi       = {10.1186/1471-2288-7-34},
  url       = {http://www.biomedcentral.com/1471-2288/7/34},
  abstract  = {BACKGROUND:Many studies conducted in health and social sciences collect
	individual level data as outcome measures. Usually, such data have
	a hierarchical structure, with patients clustered within physicians,
	and physicians clustered within practices. Large survey data, including
	national surveys, have a hierarchical or clustered structure; respondents
	are naturally clustered in geographical units (e.g., health regions)
	and may be grouped into smaller units. Outcomes of interest in many
	fields not only reflect continuous measures, but also binary outcomes
	such as depression, presence or absence of a disease, and self-reported
	general health. In the framework of multilevel studies an important
	problem is calculating an adequate sample size that generates unbiased
	and accurate estimates.METHODS:In this paper simulation studies are
	used to assess the effect of varying sample size at both the individual
	and group level on the accuracy of the estimates of the parameters
	and variance components of multilevel logistic regression models.
	In addition, the influence of prevalence of the outcome and the intra-class
	correlation coefficient (ICC) is examined.RESULTS:The results show
	that the estimates of the fixed effect parameters are unbiased for
	100 groups with group size of 50 or higher. The estimates of the
	variance covariance components are slightly biased even with 100
	groups and group size of 50. The biases for both fixed and random
	effects are severe for group size of 5. The standard errors for fixed
	effect parameters are unbiased while for variance covariance components
	are underestimated. Results suggest that low prevalent events require
	larger sample sizes with at least a minimum of 100 groups and 50
	individuals per group.CONCLUSION:We recommend using a minimum group
	size of 50 with at least 50 groups to produce valid estimates for
	multi-level logistic regression models. Group size should be adjusted
	under conditions where the prevalence of events is low such that
	the expected number of events in each group should be greater than
	one.},
  groups    = {Optimisation},
  journal   = {BMC Medical Research Methodology},
  owner     = {meddwilb},
  pubmedid  = {17634107},
  timestamp = {2013.10.21},
}

@Article{Morgan2003,
  author    = {Morgan, Caroline C.},
  title     = {Sample size re-estimation in group-sequential response-adaptive clinical trials},
  year      = {2003},
  volume    = {22},
  number    = {24},
  pages     = {3843--3857},
  issn      = {1097-0258},
  doi       = {10.1002/sim.1677},
  url       = {http://dx.doi.org/10.1002/sim.1677},
  abstract  = {In clinical trials where the variances of the response variables are unknown, in accurate estimates of these can affect the type II error rate considerably. More accurate estimates of the variances may be obtained by taking a look at the data available part way through the trial and re-calculating the required sample size based on these new estimates. The main impetus for sample size re-estimation came from a two-stage procedure developed by Stein in 1945 and the literature is now replete with variations on this approach. In this paper, existing sample size re-estimation methods for both fixed sample and sequential clinical trial models will be reviewed. These will then be extended for use in group-sequential response-adaptive designs. In particular, a test for a recently developed group-sequential response-adaptive design, which compares two treatments with immediate normally distributed responses and unknown variances, is presented based on a modified version of Stein's test. The principal modifications involve updating the required sample size at each interim analysis and calculating thetest statistic based on the current estimates of the variances. Hence, all the available information is used at each stage. Simulation is used to assess to what extent the updating of the required sample size at each interim analysis in the new test helps to attain the nominal error rates. The test is compared to modified versions of a simple test and a Stein-type group sequential t-test studied in the recent literature. These tests calculate the required sample sizes based on less accurate estimates of the variances. The type I error rate is close to the nominal value and the power is more accurately maintained in the new test. Copyright Â© 2003 John Wiley & Sons, Ltd.},
  groups    = {SSR},
  journal   = {Statistics in Medicine},
  keywords  = {adaptive sampling, interim analyzes, power family tests, internal pilot studies, Stein's test},
  owner     = {meddwilb},
  publisher = {John Wiley \& Sons, Ltd.},
  timestamp = {2016.02.23},
}

@ARTICLE{Muller1999,
  author = {Peter Muller},
  title = {Simulation Based Optimal Design},
  journal = {Bayesian Statistics},
  year = {1999},
  volume = {6},
  pages = {459-474},
  abstract = {We review simulation based methods in optimal design. Expected utility
	maximization, i.e., optimal
	
	design, is concerned with maximizing an integral expression representing
	expected utility with respect
	
	to some design parameter. Except in special cases neither the maximization
	nor the integration can be
	
	solved analytically and approximations and/or simulation based methods
	are needed. On one hand the
	
	integration problem is easier to solve than the integration appearing
	in posterior inference problems.
	
	This is because the expectation is with respect to the joint distribution
	of parameters and data, which
	
	typically allows efficient random variate generation. On the other
	hand, the problem is difficult because
	
	the integration is embedded in the maximization and has to possibly
	be evaluated many times for
	
	different design parameters.
	
	
	We discuss four related strategies: prior simulation; smoothing of
	Monte Carlo simulations; Markov
	
	chain Monte Carlo (MCMC) simulation in an augmented probability model;
	a simulated annealing type
	
	approach.},
  owner = {meddwilb},
  timestamp = {2015.09.10},
  url = {http://dell9.ma.utexas.edu/users/pmueller/pap/M99.pdf}
}

@BOOK{Murray1998,
  title = {Design and Analysis of Group-Randomized Trials},
  publisher = {Oxford University Press},
  year = {1998},
  author = {David M. Murray},
  volume = {27},
  series = {Monographs in Epidemiology and Biostatistics},
  owner = {meddwilb},
  timestamp = {2013.09.09}
}

@ARTICLE{Murray2003,
  author = {Murray, David M. and Blitstein, Jonathan L.},
  title = {Methods To Reduce The Impact Of Intraclass Correlation In Group-Randomized
	Trials},
  journal = {Evaluation Review},
  year = {2003},
  volume = {27},
  pages = {79-103},
  number = {1},
  abstract = {This study reports intraclass correlation (ICC) for dependent variables
	used in group-randomized trials (GRTs). The authors also document
	the effect of two methods suggested to reduce the impact of ICC in
	GRTs; these two methods are modeling time and regression adjustment
	for covariates. They coded and analyzed 1,188 ICC estimates from
	17 published, in press, and unpublished articles representing 21
	studies. Findings confirm that both methods can improve the efficiency
	of analyses shown to be valid across conditions common in GRTs. Investigators
	planning GRTs should obtain ICC estimates matched to their planned
	analysis so that they can size their studies properly.},
  doi = {10.1177/0193841X02239019},
  eprint = {http://erx.sagepub.com/content/27/1/79.full.pdf+html},
  owner = {meddwilb},
  timestamp = {2013.09.09},
  url = {http://erx.sagepub.com/content/27/1/79.abstract}
}

@ARTICLE{Murray2010,
  author = {Murray, Elizabeth and Treweek, Shaun and Pope, Catherine and MacFarlane,
	Anne and Ballini, Luciana and Dowrick, Christopher and Finch, Tracy
	and Kennedy, Anne and Mair, Frances and O'Donnell, Catherine and
	Ong, Bie and Rapley, Tim and Rogers, Anne and May, Carl},
  title = {Normalisation process theory: a framework for developing, evaluating
	and implementing complex interventions},
  journal = {BMC Medicine},
  year = {2010},
  volume = {8},
  pages = {63},
  number = {1},
  abstract = {BACKGROUND:The past decade has seen considerable interest in the development
	and evaluation of complex interventions to improve health. Such interventions
	can only have a significant impact on health and health care if they
	are shown to be effective when tested, are capable of being widely
	implemented and can be normalised into routine practice. To date,
	there is still a problematic gap between research and implementation.
	The Normalisation Process Theory (NPT) addresses the factors needed
	for successful implementation and integration of interventions into
	routine work (normalisation).DISCUSSION:In this paper, we suggest
	that the NPT can act as a sensitising tool, enabling researchers
	to think through issues of implementation while designing a complex
	intervention and its evaluation. The need to ensure trial procedures
	that are feasible and compatible with clinical practice is not limited
	to trials of complex interventions, and NPT may improve trial design
	by highlighting potential problems with recruitment or data collection,
	as well as ensuring the intervention has good implementation potential.SUMMARY:The
	NPT is a new theory which offers trialists a consistent framework
	that can be used to describe, assess and enhance implementation potential.
	We encourage trialists to consider using it in their next trial.},
  doi = {10.1186/1741-7015-8-63},
  issn = {1741-7015},
  owner = {meddwilb},
  pubmedid = {20961442},
  timestamp = {2013.10.10},
  url = {http://www.biomedcentral.com/1741-7015/8/63}
}

@ARTICLE{Muth&eacute;n2002,
  author = {Bengt O. Muth\&eacute;n},
  title = {Beyond SEM: general latent variable modelling},
  journal = {Behaviormetrika},
  year = {2002},
  volume = {29},
  pages = {81-117},
  number = {1},
  file = {:Papers\\Muthen02-Latent-Variable-Modeling.pdf:PDF},
  owner = {meddwilb},
  timestamp = {2013.09.11}
}

@ARTICLE{Nair2008,
  author = {Nair, Vijay and Strecher, Victor and Fagerlin, Angela and Ubel, Peter
	and Resnicow, Kenneth and Murphy, Susan and Little, Roderick and
	Chakraborty, Bibhas and Zhang, Aijun},
  title = {Screening Experiments and the Use of Fractional Factorial Designs
	in Behavioral Intervention Research},
  journal = {Am J Public Health},
  year = {2008},
  volume = {98},
  pages = {1354--1359},
  number = {8},
  month = aug,
  booktitle = {American Journal of Public Health},
  comment = {doi: 10.2105/AJPH.2007.127563},
  doi = {10.2105/AJPH.2007.127563},
  issn = {0090-0036},
  owner = {meddwilb},
  publisher = {American Public Health Association},
  timestamp = {2013.11.20},
  url = {http://dx.doi.org/10.2105/AJPH.2007.127563}
}

@MANUAL{Naval2013,
  title = {MOPSOCD: Multi-objective Particle Swarm Optimization with Crowding
	Distance},
  author = {Pros Naval},
  year = {2013},
  note = {R package version 0.5.1},
  owner = {meddwilb},
  timestamp = {2014.02.24},
  url = {http://CRAN.R-project.org/package=mopsocd}
}

@ARTICLE{Neyman1956,
  author = {Neyman, J.},
  title = {Note on an Article by Sir Ronald Fisher},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  year = {1956},
  volume = {18},
  pages = {288-294},
  file = {:P\:\\CTRU\\Projects\\Health_Sciences\\Methodology\\2013 NIHR Research Methods Fellowship\\Literature\\Papers\\Neyman1956.pdf:PDF},
  owner = {meddwilb},
  timestamp = {2014.08.19}
}

@ARTICLE{Neyman1933,
  author = {Neyman, J. and Pearson, E. S.},
  title = {On the Problem of the Most Efficient Tests of Statistical Hypotheses},
  journal = {Philosophical Transactions of the Royal Society of London. Series
	A, Containing Papers of a Mathematical or Physical Character},
  year = {1933},
  volume = {231},
  pages = {289-337},
  number = {694-706},
  doi = {10.1098/rsta.1933.0009},
  eprint = {http://rsta.royalsocietypublishing.org/content/231/694-706/289.full.pdf+html},
  owner = {meddwilb},
  timestamp = {2014.03.26},
  url = {http://rsta.royalsocietypublishing.org/content/231/694-706/289.short}
}

@Article{OHagan2006,
  author    = {A. O'Hagan},
  title     = {Bayesian analysis of computer code outputs: A tutorial},
  year      = {2006},
  volume    = {91},
  pages     = {1290 - 1300},
  note      = {The Fourth International Conference on Sensitivity Analysis of Model Output (SAMO 2004) \{SAMO\} 2004 The Fourth International Conference on Sensitivity Analysis of Model Output (SAMO 2004)},
  issn      = {0951-8320},
  doi       = {http://dx.doi.org/10.1016/j.ress.2005.11.025},
  url       = {http://www.sciencedirect.com/science/article/pii/S0951832005002383},
  groups    = {Optimisation},
  journal   = {Reliability Engineering \& System Safety},
  keywords  = {Bayesian statistics},
  owner     = {meddwilb},
  timestamp = {2014.02.07},
}

@ARTICLE{OHagan2001,
  author = {O'Hagan, Anthony and Stevens, John W.},
  title = {Bayesian Assessment of Sample Size for Clinical Trials of Cost-Effectiveness},
  journal = {Medical Decision Making},
  year = {2001},
  volume = {21},
  pages = {219-230},
  number = {3},
  abstract = {The authors present an analysis of the choice of sample sizes for
	demonstrating cost-effectiveness of a new treatment or procedure,
	when data on both cost and efficacy will be collected in a clinical
	trial. The Bayesian approach to statistics is employed, as well as
	a novel Bayesian criterion that provides insight into the sample
	size problem and offers a very flexible formulation.},
  doi = {10.1177/0272989X0102100307},
  eprint = {http://mdm.sagepub.com/content/21/3/219.full.pdf+html},
  owner = {meddwilb},
  timestamp = {2014.10.27},
  url = {http://mdm.sagepub.com/content/21/3/219.abstract}
}

@Article{OHagan2005,
  author    = {O'Hagan, Anthony and Stevens, John W. and Campbell, Michael J.},
  title     = {Assurance in clinical trial design},
  journal   = {Pharmaceutical Statistics},
  year      = {2005},
  volume    = {4},
  number    = {3},
  pages     = {187--201},
  issn      = {1539-1612},
  doi       = {10.1002/pst.175},
  url       = {http://dx.doi.org/10.1002/pst.175},
  abstract  = {Conventional clinical trial design involves considerations of power,
	and sample size is typically chosen to achieve a desired power conditional
	on a specified treatment effect. In practice, there is considerable
	uncertainty about what the true underlying treatment effect may be,
	and so power does not give a good indication of the probability that
	the trial will demonstrate a positive outcome.Assurance is the unconditional
	probability that the trial will yield a positive outcome. A
	positive outcome usually means a statistically significant result,
	according to some standard frequentist significance test. The assurance
	is then the prior expectation of the power, averaged over the prior
	distribution for the unknown true treatment effect.We argue that
	assurance is an important measure of the practical utility of a proposed
	trial, and indeed that it will often be appropriate to choose the
	size of the sample (and perhaps other aspects of the design) to achieve
	a desired assurance, rather than to achieve a desired power conditional
	on an assumed treatment effect. We extend the theory of assurance
	to two-sided testing and equivalence trials. We also show that assurance
	is straightforward to compute in some simple problems of normal,
	binary and gamma distributed data, and that the method is not restricted
	to simple conjugate prior distributions for parameters. Several illustrations
	are given. Copyright Â© 2005 John Wiley & Sons, Ltd.},
  groups    = {SSR},
  keywords  = {assurance, Bayesian analysis, Bayesian clinical trial simulation, binary data, design of experiments, expected power, power, preposterior analysis, prior distribution, sample size},
  owner     = {meddwilb},
  publisher = {John Wiley \& Sons, Ltd.},
  timestamp = {2014.12.10},
}

@ARTICLE{OQuigley1990,
  author = {John O'Quigley and Margaret Pepe and Lloyd Fisher},
  title = {Continual Reassessment Method: A Practical Design for Phase 1 Clinical
	Trials in Cancer},
  journal = {Biometrics},
  year = {1990},
  volume = {46},
  pages = {33-48},
  number = {1},
  doi = {10.2307/2531628},
  owner = {meddwilb},
  timestamp = {2013.12.02},
  url = {http://www.jstor.org/stable/2531628}
}

@ARTICLE{Oakley2006,
  author = {Ann Oakley and Vicki Strange and Chris Bonell and Elizabeth Allen
	and Judith Stephenson},
  title = {Process evaluation in randomised controlled trials of complex interventions},
  journal = {BMJ},
  year = {2006},
  volume = {332},
  pages = {413--416},
  doi = {10.1136/bmj.332.7538.413},
  owner = {meddwilb},
  timestamp = {2013.09.18}
}

@Article{Ogawa1952,
  author =    {Ogawa, Junjiro},
  title =     {Analytical derivation of sampling distribution of intraclass correlation coefficient},
  journal =   {Osaka Math. J.},
  year =      {1952},
  volume =    {4},
  number =    {1},
  pages =     {69--76},
  fjournal =  {Osaka Mathematical Journal},
  owner =     {meddwilb},
  publisher = {Osaka University and Osaka City University, Departments of Mathematics},
  timestamp = {2016.01.27},
  url =       {http://projecteuclid.org/euclid.ojm/1200687725}
}

@ARTICLE{Omar2000,
  author = {Omar, Rumana Z. and Thompson, Simon G.},
  title = {Analysis of a cluster randomized trial with binary outcome data using
	a multi-level model},
  journal = {Statistics in Medicine},
  year = {2000},
  volume = {19},
  pages = {2675--2688},
  number = {19},
  doi = {10.1002/1097-0258(20001015)19:19<2675::AID-SIM556>3.0.CO;2-A},
  issn = {1097-0258},
  owner = {meddwilb},
  publisher = {John Wiley \& Sons, Ltd.},
  timestamp = {2013.09.13},
  url = {http://dx.doi.org/10.1002/1097-0258(20001015)19:19<2675::AID-SIM556>3.0.CO;2-A}
}

@ARTICLE{Paul2007,
  author = {Paul, Gillian and Smith, Susan and Whitford, David and O'Shea, Eamon
	and O'Kelly, Fergus and O'Dowd, Tom},
  title = {Peer support in type 2 diabetes: a randomised controlled trial in
	primary care with parallel economic and qualitative analyses: pilot
	study and protocol},
  journal = {BMC Family Practice},
  year = {2007},
  volume = {8},
  pages = {45},
  number = {1},
  abstract = {BACKGROUND:Diabetes is a chronic illness, which requires the individual
	to assume responsibility for their own care with the aim of maintaining
	glucose and blood pressure levels as close to normal as possible.
	Traditionally self-management training for diabetes has been delivered
	in a didactic manner. In recent times alternatives to the traditional
	delivery of diabetes care have been investigated, for example, the
	concept of peer support which emphasises patient rather than professional
	domination. This paper describes the pilot study and protocol for
	a study that aims to evaluate the effectiveness of a peer support
	intervention for people with type 2 diabetes in a primary care setting.METHODS/DESIGN:A
	pilot study was conducted to access the feasibility of a randomized
	controlled trial of a peer support intervention. We used the MRC
	Framework for the evaluation of complex interventions. Elements of
	the intervention were defined and the study protocol was finalized.
	In this cluster randomised controlled trial twenty general practices
	are assigned to control and intervention groups. Each practice compiles
	a diabetes register and randomly selects 21 patients. All practices
	implement a standardised diabetes care system. In the intervention
	group all practices recruit three peer supporters. The peer supporters
	are trained to conduct nine group meetings in their general practice
	over a period of two years. Each meeting has a structured component.
	The primary outcomes are blood pressure, total cholesterol, HBA1c
	and the Diabetes Well-being score. In addition to biophysical, psychosocial,
	economic and health service utilization data peer supporter activity
	and qualitative data are collected.DISCUSSION:Peer support is a complex
	intervention and evaluating such an intervention presents challenges
	to researchers. This study will evaluate whether a peer support programme
	for patients with type 2 diabetes improves biophysical and psychosocial
	outcomes and whether it is an acceptable, cost effective intervention
	in the primary care setting.TRIAL REGISTRATION:Current Controlled
	Trials ISRCTN42541690},
  doi = {10.1186/1471-2296-8-45},
  issn = {1471-2296},
  owner = {meddwilb},
  pubmedid = {17672892},
  timestamp = {2013.09.18},
  url = {http://www.biomedcentral.com/1471-2296/8/45}
}

@Article{Pearson1955,
  author    = {E. S. Pearson},
  title     = {Statistical Concepts in the Relation to Reality},
  journal   = {Journal of the Royal Statistical Society. Series B (Methodological)},
  year      = {1955},
  volume    = {17},
  number    = {2},
  pages     = {204-207},
  issn      = {00359246},
  url       = {http://www.jstor.org/stable/2983954},
  abstract  = {This paper contains a reply to some criticisms made by Sir Ronald Fisher in his recent article on "Statistical Methods and Scientific Induction".},
  publisher = {[Royal Statistical Society, Wiley]},
}

@Article{Peckham2014,
  author    = {Emily J. Peckham and Clare Relton and Jackie Raw and Clare Walters and Kate Thomas and Christine Smith and Kapil Kapur and Elmuhtady Said},
  title     = {Interim results of a randomised controlled trial ofÂ homeopathic treatment for irritable bowel syndrome},
  journal   = {Homeopathy},
  year      = {2014},
  number    = {0},
  pages     = {-},
  issn      = {1475-4916},
  doi       = {http://dx.doi.org/10.1016/j.homp.2014.05.001},
  url       = {http://www.sciencedirect.com/science/article/pii/S1475491614000514},
  abstract  = {Irritable bowel syndrome (IBS) is a chronic condition for which there
	is no consensus onÂ the optimum treatment. Gastroenterology problems
	are some of the most common conditions treated by homeopaths, yet
	few trials have explored the effectiveness of individualised homeopathic
	treatment for IBS. A three-armed trial was conducted which compared:
	usual care, homeopathic treatment plus usual care and supportive
	listening plus usual care. The primary outcome was change in irritable
	bowel symptom severity score between baseline and 26 weeks, calculated
	using ANCOVA. An interim \{ANCOVA\} adjusted for baseline \{IBS\}
	severity, age and employment status found no statistically significant
	difference between the three arms. However, a post-hoc test comparing
	homeopathic treatment plus usual care to usual care alone found a
	statistically significant difference in favour of homeopathic treatment.
	In addition, 62.5 percent of patients in the homeopathic treatment
	arm (compared to 25.0 percent of those in the usual care arm), achieved
	a clinically relevant change in irritable bowel symptom severity
	score, which indicates a promising effect for homeopathic treatment,
	though these results should be interpreted with caution due to the
	low number of participants in the study. },
  comment   = {Tests for normality of change in IBS-SSS},
  keywords  = {Irritable bowel syndrome},
  owner     = {meddwilb},
  timestamp = {2014.06.12},
}

@ARTICLE{Perneger1998,
  author = {Thomas V Perneger},
  title = {What's wrong with Bonferroni adjustments},
  journal = {BMJ},
  year = {1998},
  volume = {316},
  pages = {1236--1238},
  number = {7139},
  month = {4},
  doi = {10.1136/bmj.316.7139.1236},
  owner = {meddwilb},
  timestamp = {2014.04.24}
}

@ARTICLE{Perrone2003,
  author = {Francesco Perrone and Massimo Di Maio and Ermelinda De Maio and Paolo
	Maione and Alessandro Ottaiano and Matilde Pensabene and Giuseppe
	Di Lorenzo and Alessandra Vernaglia Lombardi and Giuseppe Signoriello
	and Ciro Gallo},
  title = {Statistical design in phase \{II\} clinical trials and its application
	in breast cancer },
  journal = {The Lancet Oncology },
  year = {2003},
  volume = {4},
  pages = {305 - 311},
  number = {5},
  abstract = {Summary Several statistical designs for phase \{II\} studies have
	been proposed, but they are frequently misunderstood or not applied
	at all. In this review we describe the major characteristics of the
	available designs. To investigate the extent to which statistical
	designs were used in some recent phase \{II\} studies, and which
	designs were the most common, we did a survey of 145 trials involving
	treatment of breast cancer. Studies selected for the survey were
	published between 1995 and 1999 in one of seven specific oncology
	journals (all with impact factor consistently higher than 2). 94
	of the studies (64Â·8%) did not have an identifiable statistical
	design. However, among the 51 studies with statistical design there
	was a notable heterogeneity in the type of design applied. We put
	together a list of factors associated with use of statistical design
	at univariate analysis. These factors included: referral to a previous
	phase I study, recent trial start date, private sponsorship, single-agent
	treatment, and multicentre organisation. Single-agent treatment (OR
	2Â·35; 95% \{CI\} 1Â·01â€“5Â·51) and multicentre organisation (OR
	3Â·24; 95% \{CI\} 1Â·47â€“7Â·15) were independently predictive of
	the presence of statistical design. Publication in journals with
	high impact factors and short intervals between the start of the
	study and publication were also correlated with statistical design.
	},
  doi = {http://dx.doi.org/10.1016/S1470-2045(03)01078-7},
  issn = {1470-2045},
  owner = {meddwilb},
  timestamp = {2014.11.24},
  url = {http://www.sciencedirect.com/science/article/pii/S1470204503010787}
}

@ARTICLE{Peto1976,
  author = {R Peto and M C Pike and P Armitage and N E Breslow and D R Cox and
	S V Howard and N Mantel and K McPherson and J Peto and P G Smith},
  title = {Design and analysis of randomized clinical trials requiring prolonged
	observation of each patient. I. Introduction and design},
  journal = {British Journal of Cancer},
  year = {1976},
  volume = {34},
  pages = {585 - 612},
  doi = {10.1038/bjc.1976.220},
  owner = {meddwilb},
  timestamp = {2013.09.16}
}

@ARTICLE{Petticrew2011,
  author = {Petticrew, Mark},
  title = {When are complex interventions `complex'? When are simple interventions
	`simple'?},
  journal = {The European Journal of Public Health},
  year = {2011},
  volume = {21},
  pages = {397-398},
  number = {4},
  doi = {10.1093/eurpub/ckr084},
  eprint = {http://eurpub.oxfordjournals.org/content/21/4/397.full.pdf+html},
  owner = {meddwilb},
  timestamp = {2014.07.11},
  url = {http://eurpub.oxfordjournals.org/content/21/4/397.short}
}

@ARTICLE{Petticrew2013,
  author = {Mark Petticrew and Laurie Andersonand Randy Elder and Jeremy Grimshaw
	and David Hopkins and Robert Hahn and Lauren Krause and Elizabeth
	Kristjansson and Shawna Mercer and Teresa Sipe and Peter Tugwell
	and Erin Ueffing and Elizabeth Waters and Vivian Welch},
  title = {Complex interventions and their implications for systematic reviews:
	a pragmatic approach},
  journal = {Journal of Clinical Epidemiology},
  year = {2013},
  volume = {66},
  pages = {1209-1214},
  number = {11},
  doi = {10.1016/j.jclinepi.2013.06.004},
  owner = {meddwilb},
  timestamp = {2013.11.28}
}

@Article{Pezeshk2003,
  author    = {Pezeshk, Hamid},
  title     = {Bayesian techniques for sample size determination in clinical trials: a short review},
  year      = {2003},
  volume    = {12},
  number    = {6},
  pages     = {489-504},
  doi       = {10.1191/0962280203sm345oa},
  eprint    = {http://smm.sagepub.com/content/12/6/489.full.pdf+html},
  url       = {http://smm.sagepub.com/content/12/6/489.abstract},
  abstract  = {The aim of this paper is to review some key techniques of Bayesian
	methods of sample size determination. The approach is to cover a
	small number of simple problems, such as estimating the mean of a
	normal distribution. The methods considered are in two groups: inferential
	and decision theoretic. In the inferential Bayesian methods of sample
	size determination, we are solely concerned with the inference about
	the parameter(s) of interest. The fully Bayesian or decision theoretic
	approach treats the problem as a decision problem and employs a loss
	or utility function.},
  groups    = {SSR, Bayesian, Decision theory},
  journal   = {Statistical Methods in Medical Research},
  owner     = {meddwilb},
  timestamp = {2014.12.10},
}

@Article{Pezeshk2008,
  author    = {Pezeshk, Hamid and Nematollahi, Nader and Maroufy, Vahed and Gittins, John},
  title     = {The choice of sample size: a mixed Bayesian / frequentist approach},
  year      = {2008},
  doi       = {10.1177/0962280208089298},
  eprint    = {http://smm.sagepub.com/content/early/2008/04/29/0962280208089298.full.pdf+html},
  url       = {http://smm.sagepub.com/content/early/2008/04/29/0962280208089298.abstract},
  abstract  = {Sample size computations are largely based on frequentist or classical
	methods. In the Bayesian approach the prior information on the unknown
	parametersis taken into account. In this work we consider a fully
	Bayesian approach to the sample size determination problem which
	was introduced by Grundy etal. and developed by Lindley. This approach
	treats the problem as a decision problem and employs a utility function
	to find the optimal sample size of a trial. Furthermore, we assume
	that a regulatory authority,which is deciding on whether or not to
	grant a licence to a new treatment, uses a frequentist approach.
	We then find the optimal sample size for the trial by maximising
	the expected netbenefit, which is the expected benefit of subsequent
	use of the new treatment minus the cost of the trial. },
  groups    = {Bayesian, Decision theory},
  journal   = {Statistical Methods in Medical Research},
  owner     = {meddwilb},
  timestamp = {2015.05.13},
}

@InProceedings{Picheny2010,
  author    = {Picheny, V. and Ginsbourger, D. and Richet, Y.},
  title     = {Noisy Expected Improvement and On-line Computation Time Allocation for the Optimization of Simulators with Tunable Fidelity},
  booktitle = {2nd International Conference on Engineering Optimization},
  year      = {2010},
  url       = {https://hal.archives-ouvertes.fr/hal-00489321v2/document},
  groups    = {Optimisation},
  owner     = {meddwilb},
  timestamp = {2016.08.04},
}

@ARTICLE{Pincus2013,
  author = {Pincus, Tamar and Anwar, Shamaila and McCracken, Lance and McGregor,
	Alison and Graham, Liz and Collinson, Michelle and Farrin, Amanda
	and on behalf of the OBI Trial Management Team},
  title = {Testing the credibility, feasibility and acceptability of an optimised
	behavioural intervention (OBI) for avoidant chronic low back pain
	patients: protocol for a randomised feasibility study},
  journal = {Trials},
  year = {2013},
  volume = {14},
  pages = {172},
  number = {1},
  abstract = {BACKGROUND:Chronic back pain continues to be a costly and prevalent
	condition. The latest NICE guidelines issued in 2009 state that for
	patients with persistent back pain (of between six weeks and twelve
	months duration), who are highly distressed and/or disabled and for
	whom exercise, manual therapy and acupuncture has not been beneficial,
	the evidence supports a combination of around 100 hours of combined
	physical and psychological treatment. This is costly, and may prove
	unacceptable to many patients. A key recommendation of these guidelines
	was for further randomised controlled trials (RCTs) of psychological
	treatment and to target treatment to specific sub-groups of patients.
	Recent trials that have included psychological interventions have
	shown only moderate improvement at best, and results are not maintained
	long term. There is therefore a need to test theoretically driven
	interventions that focus on specific high-risk sub-groups, in which
	the intervention is delivered at full integrity against a credible
	control.METHODS/DESIGN:A feasibility study of a pragmatic randomised
	controlled trial comparing psychologist-delivered Contextual Cognitive
	Behavioural Therapy (CCBT) against Treatment As Usual (TAU) physiotherapy
	delivered by physiotherapists for the treatment of chronic lower
	back pain in 'avoidant' patients. Ninety-two patients referred for
	physiotherapy will be recruited and randomised on a 1:1 basis to
	receive CCBT or TAU. Treatment groups will be balanced by centre
	and pain interference score. Primary outcomes include assessing the
	credibility and acceptability of the intervention, and to demonstrate
	proof of principle through a greater change in pain acceptance in
	the CCBT arm, measured by the Acceptance and Action -II and the Chronic
	Pain Acceptance questionnaires. In addition, the feasibility of carrying
	out a full trial will be explored with reference to recruitment and
	follow-up rates including the assessment of the burden of outcome
	measure completion. Secondary patient outcomes include disability,
	pain, fear of movement, mood, quality of life, and global recovery.
	Outcomes are measured at three and six months post-randomisation.DISCUSSION:This
	paper details the rationale, design, therapist training system and
	recruitment methods to be used in a feasibility study which will
	inform the design and efficient implementation of a future definitive
	RCT.TRIAL REGISTRATION:ISRCTN43733490},
  doi = {10.1186/1745-6215-14-172},
  issn = {1745-6215},
  owner = {meddwilb},
  pubmedid = {23764140},
  timestamp = {2014.07.16},
  url = {http://www.trialsjournal.com/content/14/1/172}
}

@ARTICLE{Plant2011,
  author = {Plant, H. and Moore, S. and Richardson, A and Cornwall, A. and Medina,
	J. and Ream, E.},
  title = {Nurses' experience of delivering a supportive intervention for family
	members of patients with lung cancer},
  journal = {European Journal of Cancer Care},
  year = {2011},
  volume = {20},
  pages = {436--444},
  number = {4},
  abstract = {Families contribute to maintaining the well-being of people with cancer
	through providing emotional and practical support, frequently at
	significant cost to their own well-being, and often with little help
	from healthcare professionals. This paper describes nurses' experience
	of providing an innovative service to support the families of people
	with lung cancer. A process of group reflection by the three nurses
	involved in delivering the intervention has produced an autoethnographic
	account of taking part in this study. Three main themes relating
	to the nature and process of delivering the intervention were identified:
	‘meeting diverse need’, ‘differing models of delivery’ and ‘dilemma
	and emotion’. Supporting family members of patients with lung cancer
	can be immensely rewarding for nurses and potentially bring significant
	benefit. However, this kind of work can also be demanding in terms
	of time and emotional cost. These findings demonstrate the value
	of incorporating process evaluation in feasibility studies for articulating,
	refining and developing complex interventions. Determining the applicability
	and utility of the intervention for other practice settings requires
	further evaluation.},
  doi = {10.1111/j.1365-2354.2011.01249.x},
  issn = {1365-2354},
  keywords = {supportive interventions, family members, lung cancer, nursing roles},
  owner = {meddwilb},
  publisher = {Blackwell Publishing Ltd},
  timestamp = {2013.09.18},
  url = {http://dx.doi.org/10.1111/j.1365-2354.2011.01249.x}
}

@Article{Proschan2005,
  author    = {Michael A. Proschan},
  title     = {Two-Stage Sample Size Re-Estimation Based on a Nuisance Parameter: A Review},
  year      = {2005},
  volume    = {15},
  number    = {4},
  month     = {jul},
  pages     = {559--574},
  doi       = {10.1081/bip-200062852},
  url       = {http://dx.doi.org/10.1081/BIP-200062852},
  groups    = {Pilot/feasibility},
  journal   = {Journal of Biopharmaceutical Statistics},
  owner     = {meddwilb},
  publisher = {Informa {UK} Limited},
  timestamp = {2016.07.13},
}

@ARTICLE{Raab2001,
  author = {Raab, Gillian M. and Butcher, Izzy},
  title = {Balance in cluster randomized trials},
  journal = {Statistics in Medicine},
  year = {2001},
  volume = {20},
  pages = {351--365},
  number = {3},
  doi = {10.1002/1097-0258(20010215)20:3<351::AID-SIM797>3.0.CO;2-C},
  issn = {1097-0258},
  owner = {meddwilb},
  publisher = {John Wiley \& Sons, Ltd.},
  timestamp = {2013.09.10},
  url = {http://dx.doi.org/10.1002/1097-0258(20010215)20:3<351::AID-SIM797>3.0.CO;2-C}
}

@ARTICLE{Rahimi2009,
  author = {Rahimi, R and Nikfar, S and Rezaie, A and Abdollahi, M},
  title = {Efficacy of tricyclic antidepressants in irritable bowel syndrome:
	a meta-analysis},
  journal = {World journal of gastroenterology : WJG},
  year = {2009},
  volume = {15},
  pages = {1548â€”1553},
  number = {13},
  month = {April},
  doi = {10.3748/wjg.15.1548},
  issn = {1007-9327},
  owner = {meddwilb},
  timestamp = {2014.06.03},
  url = {http://europepmc.org/abstract/MED/19340896}
}

@Book{Raiffa1961,
  author    = {Raiffa, Howard and Schlaifer, Robert},
  title     = {Applied Statistical Decision Theory},
  year      = {1961},
  publisher = {Harvard College},
  groups    = {Bayesian, Decision theory},
  owner     = {meddwilb},
  timestamp = {2016.07.06},
}

@ARTICLE{Raudenbush1997,
  author = {Stephen W. Raudenbush},
  title = {Statistical analysis and optimal design for cluster randomized trials},
  journal = {Psychological Methods},
  year = {1997},
  volume = {2},
  pages = {173-185},
  number = {2},
  abstract = {In many intervention studies, therapy outcome evaluations, and educational
	field trials, random treatment assignment of clusters rather than
	persons is desirable for political feasibility, logistics, or ecological
	validity. However, cluster randomized designs are widely regarded
	as lacking statistical precision. This article considers when and
	to what extent using a pretreatment covariate can increase experimental
	precision. To answer this question, the author first optimizes allocation
	of resources within and between clusters for the no-covariate case.
	Optimal sample sizes at each level depend on variation within and
	between clusters and on the cost of sampling at each level. Next,
	the author considers optimal allocation when a covariate is added.
	In this case, the explanatory power of the covariate at each level
	becomes highly relevant for choosing optimal sample sizes. A key
	conclusion is that statistical analysis that fully uses information
	about the covariate-outcome relationship can substantially increase
	the efficiency of the cluster randomized trial, especially when the
	cost of sampling clusters is high and the covariate accounts for
	substantial variation between clusters. Recent multilevel studies
	indicate that these conditions are common.},
  doi = {10.1037/1082-989X.2.2.173},
  file = {:Papers\\Raudenbush97-Statistical-Analysis-and-Optimal-Design.pdf:PDF},
  owner = {meddwilb},
  timestamp = {2013.09.10}
}

@ARTICLE{Redfern2006,
  author = {Redfern, Judith and McKevitt, Christopher and Wolfe, Charles D.A.},
  title = {Development of Complex Interventions in Stroke Care: A Systematic
	Review},
  journal = {Stroke},
  year = {2006},
  volume = {37},
  pages = {2410-2419},
  number = {9},
  abstract = {Background and Purposeâ€” Stroke care is complex, requiring input
	from professionals, patients and carers. Identifying and developing
	appropriate intervention components to meet these complex needs is
	difficult. The Medical Research Council (MRC) Framework for developing
	and evaluating â€˜complexâ€™ (nonpharmacological) interventions aims
	to improve intervention development. This study uses the Framework
	to review complex interventions in stroke care.Methodsâ€” Systematic
	review with multiple search strategies (electronic databases, recent
	journals, gray literature) was used. The MRC Framework was used to
	guide the search strategy and assess study quality. â€˜Complex interventionsâ€™
	were defined as educational/psychosocial interventions to change
	knowledge, beliefs or behaviors.Resultsâ€” Sixty-seven studies were
	included: 39 randomized controlled trials (RCT) and 28 other designs.
	Complex interventions targeted healthcare professionals (17), and
	patients, carers and the general population (21 targeting primary
	or secondary prevention; 30 targeting adjustment and recovery after
	stroke). Compared with recovery studies, primary and secondary prevention
	studies were significantly less likely to have been evaluated in
	RCTs. Interventions evaluated in RCTs were significantly less likely
	to influence primary outcomes (26%) compared with other designs (44%).
	Theoretical grounding to support intervention choice was reported
	in 40 studies but only 14 were theoretically â€˜well developedâ€™;
	21 RCTs listed multiple primary outcome measures, with 10 listing
	5 or more. Of these only 3 reported considering statistical power
	before recruitment and none was sufficiently powered.Conclusionâ€”
	Few complex interventions in stroke care have been adequately developed
	or evaluated. This may explain failures to demonstrate efficacy.
	In future, greater attention is needed to theoretical development
	and methodological quality.},
  doi = {10.1161/01.STR.0000237097.00342.a9},
  eprint = {http://stroke.ahajournals.org/content/37/9/2410.full.pdf+html},
  owner = {meddwilb},
  timestamp = {2013.09.17},
  url = {http://stroke.ahajournals.org/content/37/9/2410.abstract}
}

@ARTICLE{Redfern2008,
  author = {Judith Redfern and Anthony D. Rudd and Charles D.A. Wolfe and Christopher
	McKevitt},
  title = {Stop Stroke: Development of an innovative intervention to improve
	risk factor management after stroke },
  journal = {Patient Education and Counseling },
  year = {2008},
  volume = {72},
  pages = {201 - 209},
  number = {2},
  doi = {http://dx.doi.org/10.1016/j.pec.2008.03.006},
  issn = {0738-3991},
  keywords = {Complex intervention},
  owner = {meddwilb},
  timestamp = {2013.09.17},
  url = {http://www.sciencedirect.com/science/article/pii/S0738399108001651}
}

@ARTICLE{Resnicow2010,
  author = {Ken Resnicow and Nanhua Zhang and Roger D. Vaughan and Sasiragha
	Priscilla Reddy and Shamagonam James and David M. Murray},
  title = {When Intraclass Correlation Coefficients Go Awry: A Case Study From
	a School-Based Smoking Prevention Study in South Africa},
  journal = {American Journal of Public Health},
  year = {2010},
  volume = {100},
  pages = {1714-1718},
  number = {9},
  doi = {10.2105/AJPH.2009.160879},
  owner = {meddwilb},
  timestamp = {2013.09.11}
}

@ARTICLE{Richardson2007,
  author = {Richardson, Alison and Plant, Hilary and Moore, Sally and Medina,
	Jibby and Cornwall, Amanda and Ream, Emma},
  title = {Developing supportive care for family members of people with lung
	cancer: a feasibility study},
  journal = {Supportive Care in Cancer},
  year = {2007},
  volume = {15},
  pages = {1259-1269},
  number = {11},
  doi = {10.1007/s00520-007-0233-z},
  issn = {0941-4355},
  keywords = {Carers; Family members; Cancer; Supportive care; Quality of life;
	Needs assessment},
  language = {English},
  owner = {meddwilb},
  publisher = {Springer-Verlag},
  timestamp = {2013.09.17},
  url = {http://dx.doi.org/10.1007/s00520-007-0233-z}
}

@Book{Robert2004,
  title =     {Monte Carlo Statistical Methods},
  publisher = {Springer},
  year =      {2004},
  author =    {Robert, Christian P. and Casella, George},
  edition =   {2nd},
  owner =     {meddwilb},
  timestamp = {2016.07.13}
}

@ARTICLE{Roberts1999,
  author = {Roberts, Chris},
  title = {The implications of variation in outcome between health professionals
	for the design and analysis of randomized controlled trials},
  journal = {Statistics in Medicine},
  year = {1999},
  volume = {18},
  pages = {2605--2615},
  number = {19},
  abstract = {Methodological work on randomized trials has largely concerned pharmacological
	interventions in which the effects of the attending health professional
	may be regarded as minor. In other clinical settings, such as surgery,
	talk or physical therapies, staff specific variation may make generalization
	problematic, undermining the value of the trial. Such variation has
	been the basis of some objections to controlled trial methodology
	and non-acceptance of trial results. The implication of this source
	of variation will be considered for studies in which different types
	of health professional deliver the intervention in each arm of the
	trial. Such a trial may involve individual patient or group randomization.
	Whichever method is used, it is argued that variation in outcome
	between health professionals may lead to design effects. These issues
	will be illustrated using data from a large trial comparing primary
	care service delivered by two types of medical doctor. Random effect
	models are most suitable for analyzing this type of trial, as they
	allow adjustment for patient characteristics whilst controlling for
	design effects. This type of model illustrates that there can be
	substantial variation in the performance within each category of
	doctor. Copyright Â© 1999 John Wiley & Sons, Ltd.},
  doi = {10.1002/(SICI)1097-0258(19991015)18:19<2605::AID-SIM237>3.0.CO;2-N},
  issn = {1097-0258},
  owner = {meddwilb},
  publisher = {John Wiley \& Sons, Ltd.},
  timestamp = {2014.02.18},
  url = {http://dx.doi.org/10.1002/(SICI)1097-0258(19991015)18:19<2605::AID-SIM237>3.0.CO;2-N}
}

@ARTICLE{Roberts2005,
  author = {Roberts, Chris and Roberts, Stephen A},
  title = {Design and analysis of clinical trials with clustering effects due
	to treatment},
  journal = {Clinical Trials},
  year = {2005},
  volume = {2},
  pages = {152-162},
  number = {2},
  abstract = {Where patients receive therapy as a group, there are good theoretical
	reasons to believe that variation in the outcome will be smaller
	for patients treated in the same group than for patients treated
	in different groups. Similarly, where different therapists treat
	different groups of patients, outcome for patients treated by the
	same therapist may differ less than outcome for patients treated
	by different therapists. Clinical trials evaluating such therapies
	need to consider this potential lack of independence. As with cluster-randomized
	trials, this has implications for the precision of treatment effects
	estimates and statistical power. There are nevertheless differences
	between clustering due to the organization of treatment and that
	due to randomization. In cluster-randomized trials the distribution
	of cluster sizes in each treatment arm should be similar as a consequence
	of randomization unless there is differential loss to follow-up.
	With clustering due to therapy group or therapist, cluster size may
	differ systematically between treatment arms, due to size of therapy
	groups or differing health professional caseload. Intra-cluster correlation
	may also differ between treatment arms. The implications of differential
	cluster size and intracluster correlation for design and analysis
	will be illustrated by data from two trials, the first comparing
	nurse practitioner care with general practitioner care, and the second
	comparing a group therapy with individual treatment as usual. The
	special case where a group therapy or therapist is compared with
	an unclustered treatment is examined in detail using a simulation
	study. The implications of differential clustering effects for sample
	size and power are addressed. It is argued that the design and analysis
	of this type of trial should take account of possible heterogeneity
	in cluster size and intracluster correlation.},
  doi = {10.1191/1740774505cn076oa},
  eprint = {http://ctj.sagepub.com/content/2/2/152.full.pdf+html},
  owner = {meddwilb},
  timestamp = {2014.02.05},
  url = {http://ctj.sagepub.com/content/2/2/152.abstract}
}

@ARTICLE{Roberts2013,
  author = {Roberts, Chris and Walwyn, Rebecca E. A.},
  title = {Design and analysis of non-pharmacological treatment trials with
	multiple therapists per patient},
  journal = {Statistics in Medicine},
  year = {2013},
  volume = {32},
  pages = {81--98},
  number = {1},
  abstract = {In trials of physical and talking therapies, nesting of patients within
	therapists has statistical implications analogous to those of cluster
	randomised trials. Nevertheless, the clustering effect may be more
	complex, as it interacts with treatment. For some therapies, individual
	patients may receive care from multiple therapists of the same type,
	so that patients are no longer strictly nested within therapists,
	creating a â€˜multiple-membershipâ€™ relationship between patients
	and therapists.This paper considers methods of analysis and sample
	size estimation for trials with multiple-membership clustering effects.
	It is motivated by a trial of a psychotherapy for the treatment of
	adolescent depression with cognitive behavioural therapy. We tested
	methods and issues in a Monte Carlo simulation study, simulating
	trials with multiple membership. Results demonstrate satisfactory
	performance in terms of convergence and give estimates of the intra-cluster
	correlation coefficient and empirical test size similar to a simple
	hierarchicalâ€‰design.We derive formulae for sample size and power
	for multiple-membership trial designs. We then compare estimates
	of power from this formula with empirical power derived from the
	simulation study. Finally, we show that we can easily extend formulae
	for sample size and power to allow consideration of power and sample
	size for certain types of more complex interventions. These include
	situations where therapists of different types deliver separate components
	of the intervention, creating a cross-classified relationship, or
	where several therapists deliver a group-administered treatment,
	creating further levels. Copyright Â© 2012 John Wiley & Sons, Ltd.},
  doi = {10.1002/sim.5521},
  issn = {1097-0258},
  keywords = {therapist effect, non-pharmacological treatment, multiple membership,
	randomised controlled trials},
  owner = {meddwilb},
  publisher = {John Wiley \& Sons, Ltd},
  timestamp = {2014.02.05},
  url = {http://dx.doi.org/10.1002/sim.5521}
}

@BOOK{Robertson1988,
  title = {Order Restricted Statistical Inference},
  publisher = {New York: John Wiley and Sons},
  year = {1988},
  author = {Robertson, T. and Wright, F. T. and Dykstra, R.},
  owner = {meddwilb},
  timestamp = {2014.12.16}
}

@Article{Roustant2012,
  author    = {Olivier Roustant and David Ginsbourger and Yves Deville},
  title     = {{DiceKriging}, {DiceOptim}: Two {R} Packages for the Analysis of Computer Experiments by Kriging-Based Metamodeling and Optimization},
  year      = {2012},
  volume    = {51},
  number    = {1},
  pages     = {1--55},
  url       = {http://www.jstatsoft.org/v51/i01/},
  groups    = {Optimisation},
  journal   = {Journal of Statistical Software},
  owner     = {meddwilb},
  timestamp = {2014.02.17},
}

@ARTICLE{Rowlands2005,
  author = {Rowlands, Gill and Sims, Jane and Kerry, Sally},
  title = {A lesson learnt: the importance of modelling in randomized controlled
	trials for complex interventions in primary care},
  journal = {Family Practice},
  year = {2005},
  volume = {22},
  pages = {132-139},
  number = {1},
  abstract = {Background. The Randomised Controlled Trial (RCT) is recognised as
	the â€˜gold standardâ€™ in quantitative research. However RCTs testing
	health care interventions can be difficult to design and implement.
	Health care interventions are often complex in themselves and are
	always applied in complex settings. Such interventions require a
	process of careful â€˜modellingâ€™ to maximize the chances of successful
	trials that will add to knowledge.Objectives. To describe the terms
	â€˜complexâ€™ and â€˜modellingâ€™ as used in the setting of randomised
	controlled trials of complex interventions. To give a practical example
	of an RCT involving a complex intervention applied in a health care
	setting to illustrate how this might take place in practice.Methods.
	We describe an RCT designed and conducted by the authors. We then
	use our trial as an example to illustrate how complex interventions
	such as ours might benefit from modelling during the design of the
	intervention and the setting within which the intervention is to
	be tested.Results. Our project was designed and tested before current
	guidance on complex interventions was published; our RCT was therefore
	not â€˜modelledâ€™ but was based on the outcome of a single quantitative
	pilot study. As part of our study we ran a parallel qualitative study,
	which highlighted several areas of complexity both in our intervention,
	and in the setting within which we applied it. In this paper we show
	how modelling might have allowed us to recognise these complexities
	at an early stage and might therefore have resulted in a study more
	likely to have demonstrated useful outcomes.Conclusion. Careful modelling
	of complex interventions is an essential step in designing trials
	of innovations in health care and health care services. Such a process
	ensures that interventions fit with and reflect the complexities
	of the settings within which interventions will be applied, and should
	ensure that the outcomes chosen are those most appropriate to demonstrate
	any benefits or risks.},
  doi = {10.1093/fampra/cmh704},
  eprint = {http://fampra.oxfordjournals.org/content/22/1/132.full.pdf+html},
  owner = {meddwilb},
  timestamp = {2013.10.10},
  url = {http://fampra.oxfordjournals.org/content/22/1/132.abstract}
}

@MANUAL{RStudio2014,
  title = {shiny: Web Application Framework for R},
  author = {{RStudio} and {Inc.}},
  year = {2014},
  note = {R package version 0.10.1},
  owner = {meddwilb},
  timestamp = {2014.09.08},
  url = {http://CRAN.R-project.org/package=shiny}
}

@ARTICLE{Rubinstein2009,
  author = {Rubinstein, Larry and Crowley, John and Ivy, Percy and LeBlanc, Michael
	and Sargent, Dan},
  title = {Randomized Phase {II} Designs},
  journal = {Clinical Cancer Research},
  year = {2009},
  volume = {15},
  pages = {1883-1890},
  number = {6},
  abstract = {As the use of molecularly targeted agents, which are anticipated to
	increase overall survival (OS)and progression-free survival (PFS)
	but not necessarily tumor response, has increased in oncology, there
	has been a corresponding increase in the recommendation and use of
	randomized phase II designs. Such designs reduce the potential for
	bias, existent in comparisons with historical controls, but also
	substantially increase the sample size requirements. We review the
	principal statistical designs for historically controlled and randomized
	phase II trials, along with their advantages, disadvantages, and
	statistical design considerations. We review the arguments for and
	against the use of randomization in phase II studies, the situations
	in which the use of historical controls is preferred, and the situations
	in which the use of randomized designs is preferred. We review methods
	used to calculate predicted OS or PFS values from historical controls,
	adjusted so as to be appropriate for an experimental sample with
	particular prognostic characteristics. We show how adjustment of
	the type I and type II error bounds for randomized studies can facilitate
	the detection of appropriate target increases in median PFS or OS
	with sample sizes appropriate for phase II studies. Although there
	continue to be differences among investigators concerning the use
	of randomization versus historical controls in phase II trials, there
	is agreement that each approach will continue to be appropriate,
	and the optimal approach will depend upon the circumstances of the
	individual trial.},
  doi = {10.1158/1078-0432.CCR-08-2031},
  eprint = {http://clincancerres.aacrjournals.org/content/15/6/1883.full.pdf+html},
  owner = {meddwilb},
  timestamp = {2014.07.11},
  url = {http://clincancerres.aacrjournals.org/content/15/6/1883.abstract}
}

@ARTICLE{Rubinstein2005,
  author = {Rubinstein, Lawrence V. and Korn, Edward L. and Freidlin, Boris and
	Hunsberger, Sally and Ivy, S. Percy and Smith, Malcolm A.},
  title = {Design Issues of Randomized Phase II Trials and a Proposal for Phase
	II Screening Trials},
  journal = {Journal of Clinical Oncology},
  year = {2005},
  volume = {23},
  pages = {7199-7206},
  number = {28},
  abstract = {Future progress in improving cancer therapy can be expedited by better
	prioritization of new treatments for phase III evaluation. Historically,
	phase II trials have been key components in the prioritization process.
	There has been a long-standing interest in using phase II trials
	with randomization against a standard-treatment control arm or an
	additional experimental arm to provide greater assurance than afforded
	by comparison to historic controls that the new agent or regimen
	is promising and warrants further evaluation. Relevant trial designs
	that have been developed and utilized include phase II selection
	designs, randomized phase II designs that include a reference standard-treatment
	control arm, and phase II/III designs. We present our own explorations
	into the possibilities of developing â€œphase II screening trials,â€?
	in which preliminary and nondefinitive randomized comparisons of
	experimental regimens to standard treatments are made (preferably
	using an intermediate end point) by carefully adjusting the false-positive
	error rates (Î± or type I error) and false-negative error rates (Î²
	or type II error), so that the targeted treatment benefit may be
	appropriate while the sample size remains restricted. If the ability
	to conduct a definitive phase III trial can be protected, and if
	investigators feel that by judicious choice of false-positive probability
	and false-negative probability and magnitude of targeted treatment
	effect they can appropriately balance the conflicting demands of
	screening out useless regimens versus reliably detecting useful ones,
	the phase II screening trial design may be appropriate to apply.},
  doi = {10.1200/JCO.2005.01.149},
  eprint = {http://jco.ascopubs.org/content/23/28/7199.full.pdf+html},
  owner = {meddwilb},
  timestamp = {2015.05.22},
  url = {http://jco.ascopubs.org/content/23/28/7199.abstract}
}

@ARTICLE{Ruepert2011,
  author = {Ruepert, Lisa and Quartero, A Otto and {de Wit}, Niek J and {van
	der Heijden}, Geert J and Rubin, Gregory and Muris, Jean WM},
  title = {Bulking agents, antispasmodics and antidepressants for the treatment
	of irritable bowel syndrome},
  journal = {Cochrane Database of Systematic Reviews},
  year = {2011},
  number = {8},
  doi = {10.1002/14651858.CD003460.pub3},
  owner = {meddwilb},
  timestamp = {2014.06.03}
}

@ARTICLE{Ruggeri2012,
  author = {Ruggeri, Mirella and Bonetto, Chiara and Lasalvia, Antonio and De
	Girolamo, Giovanni and Fioritti, Angelo and Rucci, Paola and Santonastaso,
	Paolo and Neri, Giovanni and Pileggi, Francesca and Ghigi, Daniela
	and Miceli, Maurizio and Scarone, Silvio and Cocchi, Angelo and Torresani,
	Stefano and Faravelli, Carlo and Zimmermann, Christa and Meneghelli,
	Anna and Cremonese, Carla and Scocco, Paolo and Leuci, Emanuela and
	Mazzi, Fausto and Gennarelli, Massimo and Brambilla, Paolo and Bissoli,
	Sarah and Bertani, Maria and Tosato, Sarah and De Santi, Katia and
	Poli, Sara and Cristofalo, Doriana and Tansella, Michele and THE
	GET UP GROUP},
  title = {A multi-element psychosocial intervention for early psychosis (GET
	UP PIANO TRIAL) conducted in a catchment area of 10 million inhabitants:
	study protocol for a pragmatic cluster randomized controlled trial},
  journal = {Trials},
  year = {2012},
  volume = {13},
  pages = {73},
  number = {1},
  abstract = {BACKGROUND:Multi-element interventions for first-episode psychosis
	(FEP) are promising, but have mostly been conducted in non-epidemiologically
	representative samples, thereby raising the risk of underestimating
	the complexities involved in treating FEP in 'real-world' services.METHODS/DESIGN:The
	Psychosis early Intervention and Assessment of Needs and Outcome
	(PIANO) trial is part of a larger research program (Genetics, Endophenotypes
	and Treatment: Understanding early Psychosis - GET UP) which aims
	to compare, at 9months, the effectiveness of a multi-component psychosocial
	intervention versus treatment as usual (TAU) in a large epidemiologically
	based cohort of patients with FEP and their family members recruited
	from all public community mental health centers (CMHCs) located in
	two entire regions of Italy (Veneto and Emilia Romagna), and in the
	cities of Florence, Milan and Bolzano. The GET UP PIANO trial has
	a pragmatic cluster randomized controlled design. The randomized
	units (clusters) are the CMHCs, and the units of observation are
	the centers' patients and their family members. Patients in the experimental
	group will receive TAU plus: 1) cognitive behavioral therapy sessions,
	2) psycho-educational sessions for family members, and 3) case management.
	Patient enrolment will take place over a 1-year period. Several psychopathological,
	psychological, functioning, and service use variables will be assessed
	at baseline and follow-up. The primary outcomes are: 1) change from
	baseline to follow-up in positive and negative symptoms' severity
	and subjective appraisal; 2) relapse occurrences between baseline
	and follow-up, that is, episodes resulting in admission and/or any
	case-note records of re-emergence of positive psychotic symptoms.
	The expected number of recruited patients is about 400, and that
	of relatives about 300. Owing to the implementation of the intervention
	at the CMHC level, the blinding of patients, clinicians, and raters
	is not possible, but every effort will be made to preserve the independency
	of the raters. We expect that this study will generate evidence on
	the best treatments for FEP, and will identify barriers that may
	hinder its feasibility in 'real-world' clinical settings, patient/family
	conditions that may render this intervention ineffective or inappropriate,
	and clinical, psychological, environmental, and service organization
	predictors of treatment effectiveness, compliance, and service satisfaction.TRIAL
	REGISTRATION:ClinicalTrials.gov Identifier NCT01436331},
  doi = {10.1186/1745-6215-13-73},
  issn = {1745-6215},
  owner = {meddwilb},
  pubmedid = {22647399},
  timestamp = {2014.07.16},
  url = {http://www.trialsjournal.com/content/13/1/73}
}

@Article{Sambucini2008,
  author    = {Sambucini, Valeria},
  title     = {A Bayesian predictive two-stage design for phase {II} clinical trials},
  year      = {2008},
  volume    = {27},
  number    = {8},
  pages     = {1199--1224},
  issn      = {1097-0258},
  doi       = {10.1002/sim.3021},
  url       = {http://dx.doi.org/10.1002/sim.3021},
  abstract  = {In this paper, we propose a Bayesian two-stage design for phase II
	clinical trials, which represents a predictive version of the single
	threshold design (STD) recently introduced by Tan and Machin. The
	STD two-stage sample sizes are determined specifying a minimum threshold
	for the posterior probability that the true response rate exceeds
	a pre-specified target value and assuming that the observed response
	rate is slightly higher than the target. Unlike the STD, we do not
	refer to a fixed experimental outcome, but take into account the
	uncertainty about future data. In both stages, the design aims to
	control the probability of getting a large posterior probability
	that the true response rate exceeds the target value. Such a probability
	is expressed in terms of prior predictive distributions of the data.
	The performance of the design is based on the distinction between
	analysis and design priors, recently introduced in the literature.
	The properties of the method are studied when all the design parameters
	vary. Copyright Â© 2007 John Wiley & Sons, Ltd.},
  groups    = {Bayesian, Phase II},
  journal   = {Statistics in Medicine},
  keywords  = {analysis and design priors, Bayesian approach, prior predictive distributions, sample size determination, two-stage design},
  owner     = {meddwilb},
  publisher = {John Wiley \& Sons, Ltd.},
  timestamp = {2014.02.14},
}

@Article{Sankoh2003,
  author    = {Sankoh, Abdul J. and D'Agostino, Ralph B. and Huque, Mohammad F.},
  title     = {Efficacy endpoint selection and multiplicity adjustment methods in clinical trials with inherent multiple endpoint issues},
  year      = {2003},
  volume    = {22},
  number    = {20},
  pages     = {3133--3150},
  issn      = {1097-0258},
  doi       = {10.1002/sim.1557},
  url       = {http://dx.doi.org/10.1002/sim.1557},
  abstract  = {The ideal approach for the design and analysis of clinical trials
	is to select a single primary endpoint that provides a complete characterization
	of the disease under study and permits an efficient evaluation of
	the effect of a test drug. However, this is often not possible for
	a number of diseases or clinical trials. This paper examines some
	practical clinical decision-making scenarios for the selection and
	analysis of efficacy outcome measures in clinical trials with inherent
	multiplicity components. Copyright Â© 2003 John Wiley & Sons, Ltd.},
  groups    = {Multiple endpoints},
  journal   = {Statistics in Medicine},
  keywords  = {closed tests, composite endpoints, decision-making scenario, endpoint-specific conclusions, multiple endpoints},
  owner     = {meddwilb},
  publisher = {John Wiley \& Sons, Ltd.},
  timestamp = {2014.04.23},
}

@ARTICLE{Sargent1998,
  author = {Sargent, Daniel J.},
  title = {A General Framework for Random Effects Survival Analysis in the Cox
	Proportional Hazards Setting},
  journal = {Biometrics},
  year = {1998},
  volume = {54},
  pages = {1486-1497},
  number = {4},
  abstract = {The use of random effects modeling in statistics has increased greatly
	in recent years. The introduction of such modeling into event-time
	analysis has proceeded more slowly, however. Previously, random effects
	models for survival data have either required assumptions regarding
	the form of the baseline hazard function or restrictions on the classes
	of models that can be fit. In this paper, we develop a method of
	random effect analysis of survival data, the hierarchical Cox model,
	that is an extension of Cox's original formulation in that the baseline
	hazard function remains unspecified. This method also allows an arbitrary
	distribution for the random effects. We accomplish this using Markov
	chain Monte Carlo methods in a Bayesian setting. The method is illustrated
	with three models for a dataset with times to multiple occurrences
	of mammory tumors for 48 rats treated with a carcinogen and then
	randomized to either treatment or control. This analysis is more
	satisfying than standard approaches, such as studying the first event
	for each subject, which does not fully use the data, or assuming
	independence, which in this case would overestimate the precision.},
  copyright = {Copyright © 1998 International Biometric Society},
  issn = {0006341X},
  jstor_articletype = {research-article},
  jstor_formatteddate = {Dec., 1998},
  language = {English},
  owner = {meddwilb},
  publisher = {International Biometric Society},
  timestamp = {2013.09.11},
  url = {http://www.jstor.org/stable/2533673}
}

@ARTICLE{Sargent2001,
  author = {Daniel J. Sargent and Victor Chan and Richard M. Goldberg},
  title = {A Three-Outcome Design for Phase {II} Clinical Trials},
  journal = {Controlled Clinical Trials },
  year = {2001},
  volume = {22},
  pages = {117 - 125},
  number = {2},
  abstract = {The goal of a phase \{II\} trial is to make a preliminary determination
	regarding the activity and tolerability of a new treatment and thus
	to determine whether the treatment warrants further study in the
	phase \{III\} setting. Phase \{II\} clinical trials are typically
	designed in the hypothesis testing framework with two possible outcomes,
	either reject the null hypothesis \{H0\} or reject the alternative
	hypothesis Ha, based on the observed activity level. However, in
	cases where the observed activity is â€œborderline,â€? the decision
	regarding the future of the agent is not as clear as the prespecified
	hypothesis test would indicate. In this paper we propose an alternative
	design that allows for three outcomes: reject H0, reject Ha, or reject
	neither. We describe the theoretical properties of this design and
	illustrate it with several examples. We focus on the clinical implications
	of the three-outcome design. Control Clin Trials 2001;22:117â€“125},
  doi = {http://dx.doi.org/10.1016/S0197-2456(00)00115-X},
  issn = {0197-2456},
  keywords = {Optimal design},
  owner = {meddwilb},
  timestamp = {2013.12.10},
  url = {http://www.sciencedirect.com/science/article/pii/S019724560000115X}
}

@ARTICLE{Sargent2009,
  author = {Sargent, Daniel J. and Taylor, Jeremy M. G.},
  title = {Current Issues in Oncology Drug Development, with a Focus on Phase
	{II} Trials},
  journal = {Journal of Biopharmaceutical Statistics},
  year = {2009},
  volume = {19},
  pages = {556-562},
  number = {3},
  note = {PMID: 19384696},
  abstract = {In this commentary we discuss several challenges that are of current
	relevance to the design of clinical trials in oncology. We argue
	that the compartmentalization of trials into the three standard phases,
	with non overlapping aims, is not necessary and in fact may slow
	the clinical development of agents. Combined Phase I/II trials and/or
	Phase I trials that at minimum collect efficacy data and more optimally
	include a preliminary measure of efficacy in dosing determination
	should be more widely utilized. Similarly, we posit that randomized
	Phase II trials should be used more frequently, as opposed to the
	traditional historical single arm Phase II trial that usually does
	not have a valid comparison group. The use of non binary endpoints
	is a simple modification that can improve the efficiency of early
	phase trials. The heterogeneity in scientific goals and contexts
	in early phase oncology trials is considerable, and the potential
	to improve the design to match these goals is great. We review these
	and other issues in the context of 5 manuscripts related to Phase
	II trials published in this volume. Our overall premise is that the
	potential benefits associated with the oncology clinical trial community
	moving away from the one size fits all paradigm of trial design are
	great, and that more flexible and efficient designs tailored to match
	the goals of each study are currently available and being used successfully.},
  doi = {10.1080/10543400902802474},
  eprint = {http://www.tandfonline.com/doi/pdf/10.1080/10543400902802474},
  owner = {meddwilb},
  timestamp = {2013.12.10},
  url = {http://www.tandfonline.com/doi/abs/10.1080/10543400902802474}
}

@ARTICLE{Satterthwaite1941,
  author = {Satterthwaite, Franklin E.},
  title = {Synthesis of variance},
  journal = {Psychometrika},
  year = {1941},
  volume = {6},
  pages = {309-316},
  number = {5},
  doi = {10.1007/BF02288586},
  issn = {0033-3123},
  language = {English},
  owner = {meddwilb},
  publisher = {Springer-Verlag},
  timestamp = {2014.03.31},
  url = {http://dx.doi.org/10.1007/BF02288586}
}

@ARTICLE{Satterthwaite1946,
  author = {Satterthwaite, F. E.},
  title = {An Approximate Distribution of Estimates of Variance Components},
  journal = {Biometrics Bulletin},
  year = {1946},
  volume = {2},
  pages = {pp. 110-114},
  number = {6},
  copyright = {Copyright © 1946 International Biometric Society},
  doi = {10.2307/3002019},
  issn = {00994987},
  jstor_articletype = {research-article},
  jstor_formatteddate = {Dec., 1946},
  language = {English},
  owner = {meddwilb},
  publisher = {International Biometric Society},
  timestamp = {2014.03.20},
  url = {http://www.jstor.org/stable/3002019}
}

@ARTICLE{Savage1951,
  author = {Savage, L. J.},
  title = {The Theory of Statistical Decision},
  journal = {Journal of the American Statistical Association},
  year = {1951},
  volume = {46},
  pages = {pp. 55-67},
  number = {253},
  copyright = {Copyright © 1951 American Statistical Association},
  issn = {01621459},
  jstor_articletype = {research-article},
  jstor_formatteddate = {Mar., 1951},
  language = {English},
  owner = {meddwilb},
  publisher = {American Statistical Association},
  timestamp = {2014.08.13},
  url = {http://www.jstor.org/stable/2280094}
}

@ARTICLE{Savage1976,
  author = {Savage, Leonard J.},
  title = {On Rereading R. A. Fisher},
  journal = {The Annals of Statistics},
  year = {1976},
  volume = {4},
  pages = {pp. 441-500},
  number = {3},
  abstract = {Fisher's contributions to statistics are surveyed. His background,
	skills, temperament, and style of thought and writing are sketched.
	His mathematical and methodological contributions are outlined. More
	attention is given to the technical concepts he introduced or emphasized,
	such as consistency, sufficiency, efficiency, information, and maximum
	likelihood. Still more attention is given to his conception and concepts
	of probability and inference, including likelihood, the fiducial
	argument, and hypothesis testing. Fisher is at once very near to
	and very far from modern statistical thought generally.},
  copyright = {Copyright Â© 1976 Institute of Mathematical Statistics},
  issn = {00905364},
  jstor_articletype = {research-article},
  jstor_formatteddate = {May, 1976},
  language = {English},
  owner = {meddwilb},
  publisher = {Institute of Mathematical Statistics},
  timestamp = {2014.07.07},
  url = {http://www.jstor.org/stable/2958221}
}

@Article{Saxon2012,
  author    = {Saxon, David and Barkham, Michael},
  title     = {Patterns of therapist variability: Therapist effects and the contribution of patient severity and risk},
  year      = {2012},
  volume    = {80},
  number    = {4},
  pages     = {535-546},
  url       = {http://dx.doi.org/10.1037/a0028898},
  file      = {:P\:\\CTRU\\Projects\\Health_Sciences\\Methodology\\2013 NIHR Research Methods Fellowship\\Literature\\Papers\\Saxon2012.pdf:PDF},
  groups    = {Barkham},
  journal   = {Journal of Consulting and Clinical Psychology, Vol 80(4), Aug 2012, 535-546.},
  owner     = {meddwilb},
  timestamp = {2015.01.23},
}

@ARTICLE{Schnurr2007,
  author = {Schnurr, P. P. and Friedman, M. J. and Engel, C. C. and et al},
  title = {Cognitive behavioral therapy for posttraumatic stress disorder in
	women: A randomized controlled trial},
  journal = {JAMA},
  year = {2007},
  volume = {297},
  pages = {820-830},
  number = {8},
  doi = {10.1001/jama.297.8.820},
  eprint = {/data/Journals/JAMA/5113/joc70009_820_830.pdf},
  owner = {meddwilb},
  timestamp = {2014.12.16},
  url = { + http://dx.doi.org/10.1001/jama.297.8.820}
}

@ARTICLE{Sehn2011,
  author = {Sehn, Laurie H. and MacDonald, David and Rubin, Sheldon and Cantin,
	Guy and Rubinger, Morel and Lemieux, Bernard and Basi, Sanraj and
	Imrie, Kevin and Gascoyne, Randy D. and Sussman, Jonathan and Chen,
	Bingshu E. and Djurfeldt, Marina and Shepherd, Lois and Couban, Stephen
	and Crump, Michael},
  title = {Bortezomib Added to R-CVP Is Safe and Effective for Previously Untreated
	Advanced-Stage Follicular Lymphoma: A Phase {II} Study by the National
	Cancer Institute of Canada Clinical Trials Group},
  journal = {Journal of Clinical Oncology},
  year = {2011},
  volume = {29},
  pages = {3396-3401},
  number = {25},
  abstract = {Purpose Bortezomib has demonstrated promising activity in patients
	with follicular lymphoma (FL). This is the first study to evaluate
	the safety and efficacy of bortezomib added to rituximab, cyclophosphamide,
	vincristine, and prednisone (R-CVP) in previously untreated advanced-stage
	FL.Patients and Methods This is a phase II multicenter trial adding
	bortezomib (1.3 mg/m2 days 1 and 8) to standard-dose R-CVP (BR-CVP)
	for up to eight cycles in patients with newly diagnosed stage III/IV
	FL requiring therapy. Two co-primary end points, complete response
	rate (complete response [CR]/CR unconfirmed [CRu]) and incidence
	of grade 3 or 4 neurotoxicity, were assessed.Results Between December
	2006 and March 2009, 94 patients were treated with BR-CVP. Median
	patient age was 57 years (range, 29 to 84 years), and the majority
	had a high (47%) or intermediate (43%) Follicular Lymphoma International
	Prognostic Index score. BR-CVP was extremely well tolerated, with
	90% of patients completing the intended eight cycles. No patients
	developed grade 4 neurotoxicity, and only five of 94 patients (5%;
	95% CI, 0.8% to 9.9%) developed grade 3 neurotoxicity, which was
	largely reversible. On the basis of an intention-to-treat analysis,
	46 of 94 patients (49%; 95% CI, 38.8% to 59.0%) achieved a CR/CRu,
	and 32 of 94 patients (34%) achieved a partial response, for an overall
	response rate of 83% (95% CI, 75.4% to 90.6%).Conclusion The addition
	of bortezomib to standard-dose R-CVP for advanced-stage FL is feasible
	and well tolerated with minimal additional toxicity. The complete
	response rate in this high-risk population compares favorably to
	historical results of patients receiving R-CVP. Given these results,
	a phase III trial comparing BR-CVP with R-CVP is planned.},
  doi = {10.1200/JCO.2010.33.6594},
  eprint = {http://jco.ascopubs.org/content/29/25/3396.full.pdf+html},
  owner = {meddwilb},
  timestamp = {2014.02.28},
  url = {http://jco.ascopubs.org/content/29/25/3396.abstract}
}

@ARTICLE{Senn2001,
  author = {Senn, S},
  title = {Two cheers for P-values?},
  journal = {Journal of Epidemiology and Biostatistics},
  year = {2001},
  volume = {6},
  pages = {193-204},
  number = {2},
  abstract = {P-values are a practical success but a critical failure. Scientists
	the world over use them, but scarcely a statistician can be found
	to defend them. Bayesians in particular find them ridiculous, but
	even the modern frequentist has little time for them. In this essay,
	I consider what, if anything, might be said in their favour.},
  doi = {doi:10.1080/135952201753172953},
  keyword = {HYPOTHESIS-TESTS, JEFFREYS-LINDLEY PARADOX, REPLICATION PROBABILITIES,
	SIGNIFICANCE TESTS},
  owner = {meddwilb},
  timestamp = {2014.04.01},
  url = {http://www.ingentaconnect.com/content/tandf/jeb/2001/00000006/00000002/art00002}
}

@Article{Senn2007,
  author    = {Stephen Senn and Frank Bretz},
  title     = {Power and sample size when multiple endpoints are considered},
  year      = {2007},
  volume    = {6},
  number    = {3},
  pages     = {161--170},
  doi       = {10.1002/pst.301},
  url       = {http://dx.doi.org/10.1002/pst.301},
  groups    = {Multiple endpoints},
  journal   = {Pharmaceut. Statist.},
  owner     = {meddwilb},
  publisher = {Wiley-Blackwell},
  timestamp = {2016.07.14},
}

@ARTICLE{Serlin2003,
  author = {Ronald C. Serlin and Bruce E. Wampold and Joel R. Levin},
  title = {Should Providers of Treatment Be Regarded as a Random Factor? If
	It Ain’t Broke, Don’t “Fix” It: A Comment on {Siemer and Joormann}
	(2003)},
  journal = {Psychological Methods},
  year = {2003},
  volume = {8},
  pages = {524-534},
  number = {4},
  doi = {10.1037/1082-989X.8.4.524},
  file = {:P\:\\CTRU\\Projects\\Health_Sciences\\Methodology\\2013 NIHR Research Methods Fellowship\\Literature\\Papers\\Serlin2003.pdf:PDF},
  owner = {meddwilb},
  timestamp = {2014.03.06}
}

@ARTICLE{Seymour2010,
  author = {Seymour, Lesley and Ivy, S. Percy and Sargent, Daniel and Spriggs,
	David and Baker, Laurence and Rubinstein, Larry and Ratain, Mark
	J. and Le Blanc, Michael and Stewart, David and Crowley, John and
	Groshen, Susan and Humphrey, Jeffrey S. and West, Pamela and Berry,
	Donald},
  title = {The Design of Phase {II} Clinical Trials Testing Cancer Therapeutics:
	Consensus Recommendations from the Clinical Trial Design Task Force
	of the National Cancer Institute Investigational Drug Steering Committee},
  journal = {Clinical Cancer Research},
  year = {2010},
  volume = {16},
  pages = {1764-1769},
  number = {6},
  abstract = {The optimal design of phase II studies continues to be the subject
	of vigorous debate, especially studies of newer molecularly targeted
	agents. The observations that many new therapeutics in definitive
	phase III studies, coupled with the numbers of new agents to be tested
	as well as the increasing costs and complexity of clinical trials,
	further emphasize the critical importance of robust and efficient
	phase II design. The Clinical Trial Design Task Force (CTD-TF) of
	the National Cancer Institute (NCI) Investigational Drug Steering
	Committee (IDSC) has published a series of discussion papers on phase
	II trial design in Clinical Cancer Research. The IDSC has developed
	formal recommendations about aspects of phase II trial design that
	are the subject of frequent debate, such as endpoints (response versus
	progression-free survival), randomization (single-arm designs versus
	randomization), inclusion of biomarkers, biomarker-based patient
	enrichment strategies, and statistical design (e.g., two-stage designs
	versus multiple-group adaptive designs). Although these recommendations
	in general encourage the use of progression-free survival as the
	primary endpoint, randomization, inclusion of biomarkers, and incorporation
	of newer designs, we acknowledge that objective response as an endpoint
	and single-arm designs remain relevant in certain situations. The
	design of any clinical trial should always be carefully evaluated
	and justified based on characteristic specific to the situation.},
  doi = {10.1158/1078-0432.CCR-09-3287},
  eprint = {http://clincancerres.aacrjournals.org/content/16/6/1764.full.pdf+html},
  owner = {meddwilb},
  timestamp = {2014.07.11},
  url = {http://clincancerres.aacrjournals.org/content/16/6/1764.abstract}
}

@Article{Shapiro1995,
  author    = {Shapiro, David A. and Rees, Anne and Barkham, Michael and Hardy, Gillian},
  title     = {Effects of treatment duration and severity of depression on the maintenance of gains after cognitive-behavioral and psychodynamic-interpersonal psychotherapy},
  year      = {1995},
  volume    = {63},
  number    = {3},
  pages     = {378-387},
  url       = {http://dx.doi.org/10.1037/0022-006X.63.3.378},
  groups    = {Barkham},
  journal   = {Journal of Consulting and Clinical Psychology},
  owner     = {meddwilb},
  timestamp = {2015.01.23},
}

@ARTICLE{Sheiner1991,
  author = {Lewis B Sheiner},
  title = {The intellectual health of clinical drug evaluation},
  journal = {Clinical Pharmacology and Therapeutics},
  year = {1991},
  volume = {50},
  pages = {4 - 9},
  doi = {10.1038/clpt.1991.97},
  owner = {meddwilb},
  timestamp = {2014.01.06}
}

@ARTICLE{Shiell2008,
  author = {Alan Shiell and Penelope Hawe and Lisa Gold},
  title = {Complex interventions or complex systems? Implications for health
	economic evaluation},
  journal = {BMJ},
  year = {2008},
  volume = {336},
  pages = {1281--1283},
  number = {7656},
  month = {6},
  doi = {10.1136/bmj.39569.510521.AD},
  owner = {meddwilb},
  timestamp = {2013.10.10}
}

@ARTICLE{Siemer2003,
  author = {Matthias Siemer and Jutta Joormann},
  title = {Power and Measures of Effect Size in Analysis of Variance With Fixed
	Versus Random Nested Factors},
  journal = {Psychological Methods},
  year = {2003},
  volume = {8},
  pages = {497-517},
  number = {4},
  abstract = {Ignoring a nested factor can influence the validity of statistical
	decisions about treatment effectiveness. Previous discussions have
	centered on consequences of ignoring nested factors versus treating
	them as random factors on Type I errors and measures of effect size
	(B. E. Wampold & R. C. Serlin, see record 2000-16737-003). The authors
	(a) discuss circumstances under which the treatment of nested provider
	effects as fixed as opposed to random is appropriate; (b) present
	2 formulas for the correct estimation of effect sizes when nested
	factors are fixed; (c) present the results of Monte Carlo simulations
	of the consequences of treating providers as fixed versus random
	on effect size estimates, Type I error rates, and power; and (d)
	discuss implications of mistaken considerations of provider effects
	for the study of differential treatment effects in psychotherapy
	research. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
  doi = {10.1037/1082-989X.8.4.497},
  file = {:P\:\\CTRU\\Projects\\Health_Sciences\\Methodology\\2013 NIHR Research Methods Fellowship\\Literature\\Papers\\Siemer2003.pdf:PDF},
  owner = {meddwilb},
  timestamp = {2014.03.06}
}

@ARTICLE{Siemer2003a,
  author = {Matthias Siemer and Jutta Joormann},
  title = {Assumptions and Consequences of Treating Providers in Therapy Studies
	as Fixed Versus Random Effects: Reply to {Crits-Christoph, Tu, and
	Gallop (2003) and Serlin, Wampold, and Levin (2003)}},
  journal = {Psychological Methods},
  year = {2003},
  volume = {8},
  pages = {535-544},
  number = {4},
  doi = {10.1037/1082-989X.8.4.535},
  file = {:P\:\\CTRU\\Projects\\Health_Sciences\\Methodology\\2013 NIHR Research Methods Fellowship\\Literature\\Papers\\Siemer2003a.pdf:PDF},
  owner = {meddwilb},
  timestamp = {2014.03.06}
}

@Article{Sill2012,
  author    = {Sill, Michael W and Rubinstein, Larry and Litwin, Samuel and Yothers, Greg},
  title     = {A method for utilizing co-primary efficacy outcome measures to screen regimens for activity in two-stage Phase {II} clinical trials},
  journal   = {Clinical Trials},
  year      = {2012},
  volume    = {9},
  number    = {4},
  pages     = {385-395},
  doi       = {10.1177/1740774512450101},
  eprint    = {http://ctj.sagepub.com/content/9/4/385.full.pdf+html},
  url       = {http://ctj.sagepub.com/content/9/4/385.abstract},
  abstract  = {Background Most Phase II clinical trials utilize a single primary
	end point to determine the promise of a regimen for future study.
	However, many disorders manifest themselves in complex ways. For
	example, migraine headaches can cause pain, auras, photophobia, and
	emesis. Investigators may believe that a drug is effective at reducing
	migraine pain and the severity of emesis during an attack. Nevertheless,
	they could still be interested in proceeding with the development
	of the drug if it is effective against only one of these symptoms.
	Such a study would be a candidate for a clinical trial with co-primary
	end points.Purpose The purpose of the article is to provide a method
	for designing a single arm, two-stage clinical trial with dichotomous
	co-primary end points of efficacy that has the ability to detect
	activity on either response measure with high probability when the
	drug is active on one or both measures, while at the same time rejecting
	the drug with high probability when there is little activity on both
	dimensions. The design enables early closure for futility and is
	flexible with regard to attained accrual.Methods The design is proposed
	in the context of cancer clinical trials with tumor response and
	progression-free survival (PFS) status after a certain period. Both
	end points are assumed to be distributed as binomial random variables,
	and uninteresting probabilities of success are determined from historical
	controls. Given the necessity of accrual flexibility, exhaustive
	searching algorithms to find optimum designs do not seem feasible
	at this time. Instead, critical values are determined for realized
	sample sizes using specific procedures. Then accrual windows are
	found to achieve a design’s desired level of significance, probability
	of early termination (PET), and power.Results The design is illustrated
	with a clinical trial that examined bevacizumab in patients with
	recurrent endometrial cancer. This study was negative by tumor response
	but positive by 6-month PFS. The procedure was compared to modified
	procedures in the literature, indicating that the method is competitive.Limitations
	Although the procedure allows investigators to construct designs
	with desired levels of significance and power, the PET under the
	null hypothesis is smaller than for single end point studies.Conclusions
	The impact of adding an additional end point on the sample size is
	often minimal, but the study gains sensitivity to activity on another
	dimension of treatment response. The operating characteristics are
	fairly robust to the level of association between the two end points.
	Software is available online.},
  comment   = {Two primary outcomes, binary, (e.g. tumour response and progression free at six months), two stages, at each stage we proceed if \emph{either} endpoints exceed critical values.  

Operating characteristics are: one type I error (proceeding past stage 2 under 00); two type II errors (not proceeding past stage 2 under 10 / 01); three probabilities of early termination (i.e. after stage 1) under hypotheses 00, 10, 01.

Criteria are minimal expected sample size under 00, and minimal maximum sample size.},
  groups    = {Multiple endpoints},
  owner     = {meddwilb},
  timestamp = {2014.08.07},
}

@ARTICLE{Simon1989,
  author = {Richard Simon},
  title = {Optimal two-stage designs for phase {II} clinical trials},
  journal = {Controlled Clinical Trials},
  year = {1989},
  volume = {10},
  pages = {1 - 10},
  number = {1},
  doi = {http://dx.doi.org/10.1016/0197-2456(89)90015-9},
  issn = {0197-2456},
  keywords = {clinical trials},
  owner = {meddwilb},
  timestamp = {2013.10.21},
  url = {http://www.sciencedirect.com/science/article/pii/0197245689900159}
}

@ARTICLE{Simon1985,
  author = {Richard Simon and Robert E. Wittes and Susan S. Ellenberg},
  title = {Randomized phase {II} clinical trials},
  journal = {Cancer Treatment Reports},
  year = {1985},
  volume = {69},
  pages = {1375-81},
  number = {12},
  file = {:P\:\\CTRU\\Projects\\Health_Sciences\\Methodology\\2013 NIHR Research Methods Fellowship\\Literature\\Papers\\Simon1985.pdf:PDF},
  owner = {meddwilb},
  timestamp = {2013.10.16},
  url = {http://europepmc.org/abstract/MED/4075313}
}

@ARTICLE{Slymen1997,
  author = {Slymen, D J and Hovell, M F},
  title = {Cluster versus individual randomization in adolescent tobacco and
	alcohol studies: illustrations for design decisions.},
  journal = {International Journal of Epidemiology},
  year = {1997},
  volume = {26},
  pages = {765-771},
  number = {4},
  abstract = {BACKGROUND: The decision to randomize by clusters of subjects such
	as a classroom or clinic versus individual randomization where some
	contamination may occur is examined within the framework of sample
	size issues. Estimates for background rates and intraclass correlations
	are also provided for adolescent tobacco and alcohol outcomes derived
	from a recent study using cluster randomization. METHODS: A ratio
	of adjusted sample sizes is derived which is a function of the intraclass
	correlation and cluster size for cluster randomization and total
	amount of contamination for individual randomization. Using estimated
	incidence rates and intraclass correlations, we provide a comparison
	of sample sizes for two plausible study outcomes. RESULTS: Small
	clusters such as a family or small classroom tend to have stronger
	within cluster dependence and cluster randomization would be clearly
	favoured over individual randomization. For moderately sized clusters,
	if contamination levels are likely to be high then cluster randomization
	would be a better choice. However in some situations where lower
	levels of contamination are expected, individual randomization may
	be preferred. With larger clusters, individual randomization should
	be considered when contamination rates are expected to be low. CONCLUSIONS:
	Investigators must carefully consider the choice of cluster randomization
	versus individual randomization in the context of likely contamination.
	In this paper we provided a basis for making this decision as well
	as examples to illustrate these decisions, and parameter estimates
	that will be especially useful for investigators in adolescent tobacco
	and alcohol studies.},
  doi = {10.1093/ije/26.4.765},
  eprint = {http://ije.oxfordjournals.org/content/26/4/765.full.pdf+html},
  owner = {meddwilb},
  timestamp = {2013.09.10},
  url = {http://ije.oxfordjournals.org/content/26/4/765.abstract}
}

@Book{Smith1988,
  title =     {Decision analysis: a Bayesian approach},
  publisher = {Chapman and Hall Ltd.},
  year =      {1988},
  author =    {Smith, J. Q.},
  owner =     {meddwilb},
  timestamp = {2016.07.11}
}

@Article{Smith2010,
  author    = {Smith, Mike K. and Marshall, Andrea},
  title     = {Importance of protocols for simulation studies in clinical drug development},
  year      = {2010},
  doi       = {10.1177/0962280210378949},
  eprint    = {http://smm.sagepub.com/content/early/2010/08/03/0962280210378949.full.pdf+html},
  url       = {http://smm.sagepub.com/content/early/2010/08/03/0962280210378949.abstract},
  abstract  = {Clinical trial simulation studies can be used to assess the impact
	of many aspects of trial design, conduct, analysis and decision making
	on trial performance metrics. Simulation studies can play a vital
	role in improving the efficiency of the drug development process
	within the pharmaceutical industry, but only if they are well designed
	and conducted.It is imperative therefore that a protocol or simulation
	plan is developed, documenting how the simulation study is to be
	conducted, analysed and reported. This article emphasises the specific
	considerations necessary for designing good quality simulation studies.
	These include defining data generation processes, data analytic methods,
	decision criteria and also determining the presentation of results
	for all intended audiences.With clinical trial simulations becoming
	a vital part of the drug development process, the protocol for clinical
	trial simulations may in future become part of the regulatory peer
	review process. More rigour in the planning and execution of simulation
	studies will ensure that the design, analysis and decision-making
	process for the subsequent clinical trial is based on credible evidence
	that can be independently verified.},
  groups    = {multi-D / sim SS},
  journal   = {Statistical Methods in Medical Research},
  owner     = {meddwilb},
  timestamp = {2014.12.16},
}

@ARTICLE{Smith2011,
  author = {S M Smith and G Paul and A Kelly and D L Whitford and E O{\textquoteright}Shea
	and T O{\textquoteright}Dowd},
  title = {Peer support for patients with type 2 diabetes: cluster randomised
	controlled trial},
  journal = {BMJ},
  year = {2011},
  volume = {342},
  pages = {:d715},
  doi = {10.1136/bmj.d715},
  owner = {meddwilb},
  timestamp = {2013.09.18}
}

@InBook{Snijders2005,
  author    = {Snijders, Tom A. B.},
  title     = {Encyclopedia of Statistics in Behavioral Science},
  booktitle = {Encyclopedia of Statistics in Behavioral Science},
  year      = {2005},
  editor    = {B.S. Everitt and D.C. Howell},
  volume    = {3},
  publisher = {John Wiley \& Sons, Ltd},
  isbn      = {9780470013199},
  chapter   = {Power and Sample Size in Multilevel Linear Models},
  doi       = {10.1002/0470013192.bsa492},
  url       = {http://dx.doi.org/10.1002/0470013192.bsa492},
  groups    = {multi-D / sim SS},
  keywords  = {power, statistical tests, design, multilevel analysis, sample size, multisite trial, cluster randomization},
  owner     = {meddwilb},
  timestamp = {2013.10.21},
}

@ARTICLE{Snijders1993,
  author = {Snijders, Tom A. B. and Bosker, Roel J.},
  title = {Standard Errors and Sample Sizes for Two-Level Research},
  journal = {Journal of Educational and Behavioral Statistics},
  year = {1993},
  volume = {18},
  pages = {237-259},
  number = {3},
  abstract = {The hierarchical linear model approach to a two-level design is considered,
	some variables at the lower level having fixed and others having
	random regression coefficients. An approximation is derived to the
	covariance matrix of the estimators of the fixed regression coefficients
	(for variables at the lower and the higher level) under the assumption
	that the sample sizes at either level are large enough. This covariance
	matrix is expressed as a function of parameters occurring in the
	model. If a research planner can make a reasonable guess as to these
	parameters, this approximation can be used as a guide to the choice
	of sample sizes at either level.},
  doi = {10.3102/10769986018003237},
  eprint = {http://jeb.sagepub.com/content/18/3/237.full.pdf+html},
  owner = {meddwilb},
  timestamp = {2014.12.04},
  url = {http://jeb.sagepub.com/content/18/3/237.abstract}
}

@Article{Soldz2006,
  author    = {Soldz, Stephen},
  title     = {Models and meanings: Therapist effects and the stories we tell},
  year      = {2006},
  volume    = {16},
  number    = {2},
  pages     = {173-177},
  doi       = {10.1080/10503300500264937},
  eprint    = { http://dx.doi.org/10.1080/10503300500264937 },
  url       = { http://dx.doi.org/10.1080/10503300500264937 
},
  groups    = {Barkham},
  journal   = {Psychotherapy Research},
  keywords  = {Therapist effects},
  owner     = {meddwilb},
  timestamp = {2015.01.26},
}

@Article{SPIEGEL2009,
  author    = {SPIEGEL, B. and BOLUS, R. and HARRIS, L. A. and LUCAK, S. and NALIBOFF, B. and ESRAILIAN, E. and CHEY, W. D. and LEMBO, A. and KARSAN, H. and TILLISCH, K. and TALLEY, J. and MAYER, E. and CHANG, L.},
  title     = {Measuring irritable bowel syndrome patient-reported outcomes with an abdominal pain numeric rating scale},
  journal   = {Alimentary Pharmacology \& Therapeutics},
  year      = {2009},
  volume    = {30},
  number    = {11-12},
  pages     = {1159--1170},
  issn      = {1365-2036},
  doi       = {10.1111/j.1365-2036.2009.04144.x},
  url       = {http://dx.doi.org/10.1111/j.1365-2036.2009.04144.x},
  abstract  = {Backgroundâ€‚ Controversy exists on how to measure patient-reported
	outcomes in irritable bowel syndrome (IBS) clinical trials effectively.
	Pain numeric rating scales (NRS) are widely used in the non-IBS pain
	literature. The Food and Drug Administration has proposed using the
	NRS in IBS.Aimâ€‚ To test the psychometrics of an abdominal pain
	NRS in IBS.Methodsâ€‚ We analysed data from a longitudinal cohort
	of Rome III IBS subjects. At entry, subjects completed a 10-point
	NRS, bowel symptoms, IBS severity measurements (IBS-SSS, FBDSI),
	health-related quality of life indices (IBS-QOL, EQ5D), and the Worker
	Productivity Activity Index (WPAI). We repeated assessments at 3Â months
	along with a response scale to calculate the minimal clinically important
	difference.Resultsâ€‚ There were 277 subjects (82% women; ageÂ =Â 42Â Â±Â 15)
	at baseline and 90 at 3Â months. The NRS correlated cross-sectionally
	with IBS-SSS (rÂ =Â 0.60; PÂ <Â 0.0011), FBDSI (rÂ =Â 0.49; PÂ <Â 0.0001),
	IBS-QOL (rÂ = 0.43; PÂ <Â 0.0001), EQ5D (rÂ =Â 0.48; PÂ <Â 0.0001),
	presenteeism (rÂ =Â 0.39; PÂ <Â 0.0001), absenteeism (rÂ =Â 0.17;
	PÂ =Â 0.04) and distension (rÂ = 0.46; PÂ <Â 0.0001), but not stool
	frequency or form. The minimal clinically important difference was
	2.2 points, correlating with a 29.5% reduction over time.Conclusionsâ€‚
	An abdominal pain NRS exhibits excellent validity and can be readily
	interpreted with a minimal clinically important difference in patients
	with IBS. These data support the use of the NRS in IBS clinical trials.},
  comment   = {Suggests MCID as IBS-SSS >= 95, as opposed to original 50.},
  owner     = {meddwilb},
  publisher = {Blackwell Publishing Ltd},
  timestamp = {2014.06.12},
}

@ARTICLE{Spiegel2009,
  author = {Brennan Spiegel and Michael Camilleri and Roger Bolus and Viola Andresen
	and William D. Chey and Sheri Fehnel and Allen Mangel and Nicholas
	J. Talley and William E. Whitehead},
  title = {Psychometric Evaluation of Patient-Reported Outcomes in Irritable
	Bowel Syndrome Randomized Controlled Trials: A Rome Foundation Report
	},
  journal = {Gastroenterology },
  year = {2009},
  volume = {137},
  pages = {1944 - 1953.e3},
  number = {6},
  abstract = {Background &amp; Aims There is debate about how best to measure patient-reported
	outcomes (PROs) in irritable bowel syndrome (IBS). We pooled data
	to measure the psychometric properties of \{IBS\} end points, including
	binary responses (eg, â€œadequate reliefâ€?) and 50% improvement
	in symptom severity. Methods We pooled data from 12 \{IBS\} drug
	trials involving 10,066 participants. We tested the properties of
	binary response and 50% improvement end points, including the impact
	of baseline severity on performance, and measured construct validity
	using clinical anchors. Results There were 9044 evaluable subjects
	(age, 44 years; 85% female; 58% \{IBS\} constipation-prominent [IBS-C];
	31% \{IBS\} diarrhea-prominent [IBS-D]). Using the binary end point,
	the proportion responding in the mild, moderate, and severe groups
	was 42%, 40%, and 38%, respectively (P = .0008). There was no effect
	of baseline severity on binary response (odds ratio [OR], 0.99; 95%
	confidence interval [CI], 0.99â€“1.0; P = .07). The proportions reaching
	50% improvement in pain were 45%, 41%, and 41%, respectively; there
	was a small, yet significant, impact of baseline severity (OR, 1.04;
	95% CI, 1.03â€“1.05; P &lt; .0001) that did not meet clinical relevance
	criteria. Both end points revealed strong construct validity and
	detected â€œminimally clinically important differencesâ€? in symptoms.
	Both provided better discriminant spread in IBS-D than IBS-C. Conclusions
	Both the traditional binary and 50% improvement end points are equivalent
	in their psychometric properties. Neither is impacted by baseline
	severity, and both demonstrate excellent construct validity. They
	are optimized for the IBS-D population but also appear valid in IBS-C.
	},
  doi = {http://dx.doi.org/10.1053/j.gastro.2009.08.047},
  issn = {0016-5085},
  owner = {meddwilb},
  timestamp = {2014.06.12},
  url = {http://www.sciencedirect.com/science/article/pii/S0016508509014814}
}

@ARTICLE{Spiegelhalter2011,
  author = {Spiegelhalter, David and Pearson, Mike and Short, Ian},
  title = {Visualizing Uncertainty About the Future},
  journal = {Science},
  year = {2011},
  volume = {333},
  pages = {1393-1400},
  number = {6048},
  abstract = {We are all faced with uncertainty about the future, but we can get
	the measure of some uncertainties in terms of probabilities. Probabilities
	are notoriously difficult to communicate effectively to lay audiences,
	and in this review we examine current practice for communicating
	uncertainties visually, using examples drawn from sport, weather,
	climate, health, economics, and politics. Despite the burgeoning
	interest in infographics, there is limited experimental evidence
	on how different types of visualizations are processed and understood,
	although the effectiveness of some graphics clearly depends on the
	relative numeracy of an audience. Fortunately, it is increasingly
	easy to present data in the form of interactive visualizations and
	in multiple types of representation that can be adjusted to user
	needs and capabilities. Nonetheless, communicating deeper uncertainties
	resulting from incomplete or disputed knowledgeâ€”or from essential
	indeterminacy about the futureâ€”remains a challenge.},
  doi = {10.1126/science.1191181},
  eprint = {http://www.sciencemag.org/content/333/6048/1393.full.pdf},
  owner = {meddwilb},
  timestamp = {2013.09.19},
  url = {http://www.sciencemag.org/content/333/6048/1393.abstract}
}

@ARTICLE{Spiegelhalter2001,
  author = {Spiegelhalter, David J.},
  title = {Bayesian methods for cluster randomized trials with continuous responses},
  journal = {Statistics in Medicine},
  year = {2001},
  volume = {20},
  pages = {435--452},
  number = {3},
  doi = {10.1002/1097-0258(20010215)20:3<435::AID-SIM804>3.0.CO;2-E},
  issn = {1097-0258},
  owner = {meddwilb},
  publisher = {John Wiley \& Sons, Ltd.},
  timestamp = {2013.09.12},
  url = {http://dx.doi.org/10.1002/1097-0258(20010215)20:3<435::AID-SIM804>3.0.CO;2-E}
}

@BOOK{Spiegelhalter2004,
  title = {Bayesian Approaches to Clinical Trials and Health-Care Evaluation},
  publisher = {Wiley},
  year = {2004},
  author = {David J. Spiegelhalter and Keith R. Abrams and Jonathan P. Myles},
  owner = {meddwilb},
  timestamp = {2013.11.19}
}

@Article{Spiegelhalter1986,
  author    = {Spiegelhalter, D. J. and Freedman, L. S.},
  title     = {A predictive approach to selecting the size of a clinical trial, based on subjective clinical opinion},
  year      = {1986},
  volume    = {5},
  number    = {1},
  pages     = {1--13},
  issn      = {1097-0258},
  doi       = {10.1002/sim.4780050103},
  url       = {http://dx.doi.org/10.1002/sim.4780050103},
  abstract  = {The â€˜textbookâ€™ approach to determining sample size in a clinical
	trial has some fundamental weaknesses which we discuss. We describe
	a new predictive method which takes account of prior clinical opinion
	about the treatment difference. The method adopts the point of clinical
	equivalence (determined by interviewing the clinical participants)
	as the null hypothesis. Decision rules at the end of the study are
	based on whether the interval estimate of the treatment difference
	(classical or Bayesian) includes the null hypothesis. The prior distribution
	is used to predict the probabilities of making the decisions to use
	one or other treatment or to reserve final judgement. It is recommended
	that sample size be chosen to control the predicted probability of
	the last of these decisions. An example is given from a multi-centre
	trial of superficial bladder cancer.},
  groups    = {SSR},
  journal   = {Statistics in Medicine},
  keywords  = {Power, Prediction, Prior opinion, Pragmatic trial, Sample size},
  owner     = {meddwilb},
  publisher = {Wiley Subscription Services, Inc., A Wiley Company},
  timestamp = {2014.12.10},
}

@Article{Spiegelhalter1986a,
  author    = {David J. Spiegelhalter and Laurence S. Freedman and Patrick R. Blackburn},
  title     = {Monitoring clinical trials: Conditional or predictive power?},
  year      = {1986},
  volume    = {7},
  number    = {1},
  month     = {mar},
  pages     = {8--17},
  doi       = {10.1016/0197-2456(86)90003-6},
  url       = {http://dx.doi.org/10.1016/0197-2456(86)90003-6},
  groups    = {Bayesian},
  journal   = {Controlled Clinical Trials},
  owner     = {meddwilb},
  publisher = {Elsevier {BV}},
  timestamp = {2016.07.20},
}

@ARTICLE{Spiegelhalter1993,
  author = {Spiegelhalter, David J. and Freedman, Laurence S. and Parmar, Mahesh
	K. B.},
  title = {Applying Bayesian ideas in drug development and clinical trials},
  journal = {Statistics in Medicine},
  year = {1993},
  volume = {12},
  pages = {1501--1511},
  number = {15-16},
  abstract = {The Bayesian paradigm emphasizes that studies are not performed in
	isolation, and that external evidence can be used formally in the
	design, monitoring and reporting of clinical trials. A variety of
	tools for assessing the current evidence for treatment efficacy are
	presented, making use of graphical display to provide insight into
	ethical and efficiency issues in starting and stopping trials â€“
	these are illustrated with a trial in osteosarcoma that is currently
	taking place. Finally we recommend that an additional â€˜interpretationâ€™
	section is placed in clinical reports to provide a bridge between
	â€˜resultsâ€™ and â€˜discussionâ€™ â€“ it is this section that would
	contain the Bayesian perspective.},
  doi = {10.1002/sim.4780121516},
  issn = {1097-0258},
  owner = {meddwilb},
  publisher = {Wiley Subscription Services, Inc., A Wiley Company},
  timestamp = {2014.09.29},
  url = {http://dx.doi.org/10.1002/sim.4780121516}
}

@ARTICLE{Spiegelhalter1994,
  author = {Spiegelhalter, David J. and Freedman, Laurence S. and Parmar, Mahesh
	K. B.},
  title = {Bayesian Approaches to Randomized Trials},
  journal = {Journal of the Royal Statistical Society. Series A (Statistics in
	Society)},
  year = {1994},
  volume = {157},
  pages = {357-416},
  number = {3},
  abstract = {Statistical issues in conducting randomized trials include the choice
	of a sample size, whether to stop a trial early and the appropriate
	analysis and interpretation of the trial results. At each of these
	stages, evidence external to the trial is useful, but generally such
	evidence is introduced in an unstructured and informal manner. We
	argue that a Bayesian approach allows a formal basis for using external
	evidence and in addition provides a rational way for dealing with
	issues such as the ethics of randomization, trials to show treatment
	equivalence, the monitoring of accumulating data and the prediction
	of the consequences of continuing a study. The motivation for using
	this methodology is practical rather than ideological.},
  copyright = {Copyright © 1994 Royal Statistical Society},
  doi = {10.2307/2983527},
  issn = {09641998},
  jstor_articletype = {research-article},
  jstor_formatteddate = {1994},
  language = {English},
  owner = {meddwilb},
  publisher = {Wiley for the Royal Statistical Society},
  timestamp = {2014.03.18},
  url = {http://www.jstor.org/stable/2983527}
}

@Article{Stallard2012,
  author    = {Stallard, Nigel},
  title     = {Optimal sample sizes for phase {II} clinical trials and pilot studies},
  year      = {2012},
  volume    = {31},
  number    = {11-12},
  pages     = {1031--1042},
  issn      = {1097-0258},
  doi       = {10.1002/sim.4357},
  url       = {http://dx.doi.org/10.1002/sim.4357},
  groups    = {SSR, Bayesian, Decision theory},
  journal   = {Statistics in Medicine},
  keywords  = {adaptive programmes of clinical development, Bayesian decision theory, clinical trial design},
  owner     = {meddwilb},
  publisher = {John Wiley \& Sons, Ltd},
  timestamp = {2013.10.22},
}

@Article{Stallard1999,
  author    = {Stallard, Nigel and Thall, Peter F. and Whitehead, John},
  title     = {Decision Theoretic Designs for Phase {II} Clinical Trials with Multiple Outcomes},
  year      = {1999},
  volume    = {55},
  number    = {3},
  pages     = {971--977},
  issn      = {1541-0420},
  doi       = {10.1111/j.0006-341X.1999.00971.x},
  url       = {http://dx.doi.org/10.1111/j.0006-341X.1999.00971.x},
  abstract  = {Summary. In many phase II clinical trials, it is essential to assess
	both efficacy and safety. Although several phase II designs that
	accommodate multiple outcomes have been proposed recently, none are
	derived using decision theory. This paper describes a Bayesian decision
	theoretic strategy for constructing phase II designs based on both
	efficacy and adverse events. The gain function includes utilities
	assigned to patient outcomes, a reward for declaring the new treatment
	promising, and costs associated with the conduct of the phase II
	trial and future phase III testing. A method for eliciting gain function
	parameters from medical collaborators and for evaluating the design's
	frequentist operating characteristics is described. The strategy
	is illustrated by application to a clinical trial of peripheral blood
	stem cell transplantation for multiple myeloma.},
  groups    = {Multiple endpoints},
  journal   = {Biometrics},
  keywords  = {Backward induction, Clinical trial design, Optimal stopping, Safety and efficacy monitoring, Sequential procedure},
  owner     = {meddwilb},
  publisher = {Blackwell Publishing Ltd},
  timestamp = {2014.04.04},
}

@Article{Stallard2001,
  author    = {Stallard, Nigel and Whitehead, John and Todd, Susan and Whitehead, Anne},
  title     = {Stopping rules for phase {II} studies},
  year      = {2001},
  volume    = {51},
  number    = {6},
  pages     = {523--529},
  issn      = {1365-2125},
  doi       = {10.1046/j.0306-5251.2001.01381.x},
  url       = {http://dx.doi.org/10.1046/j.0306-5251.2001.01381.x},
  abstract  = {This paper, the second in a series of three papers concerned with
	the statistical aspects of interim analyses in clinical trials, is
	concerned with stopping rules in phase II clinical trials. Phase
	II trials are generally small-scale studies, and may include one
	or more experimental treatments with or without a control. A common
	feature is that the results primarily determine the course of further
	clinical evaluation of a treatment rather than providing definitive
	evidence of treatment efficacy. This means that there is more flexibility
	available in the design and analysis of such studies than in phase
	III trials. This has led to a range of different approaches being
	taken to the statistical design of stopping rules for such trials.
	This paper briefly describes and compares the different approaches.
	In most cases the stopping rules can be described and implemented
	easily without knowledge of the detailed statistical and computational
	methods used to obtain the rules.},
  groups    = {Phase II},
  journal   = {British Journal of Clinical Pharmacology},
  keywords  = {clinical trial design, early stopping, group sequential designs},
  owner     = {meddwilb},
  publisher = {Blackwell Science Ltd},
  timestamp = {2014.08.07},
}

@ARTICLE{Stone2007,
  author = {Andrew Stone and Catherine Wheeler and Kevin Carroll and Alan Barge},
  title = {Optimizing randomized phase {II} trials assessing tumor progression},
  journal = {Contemporary Clinical Trials },
  year = {2007},
  volume = {28},
  pages = {146 - 152},
  number = {2},
  abstract = {The traditional development paradigm for phase \{II\} trials in oncology
	has been challenged in recent years by the introduction of cytostatic
	therapies. These agents slow the growth of tumors rather than cause
	high rates of shrinkage, this argues for the use of endpoints that
	measure growth inhibition such as progression free survival. We have
	previously argued the need for randomized trials in this setting.
	Here we discuss methodological solutions to enhance the development
	decision at the end of phase \{II\} in the context of progression
	endpoints employed in randomized trials. There are well recognized
	issues associated with progression endpoints relating to bias in
	the timing and interpretation of assessments. In this paper we present
	design and analysis solutions that will minimize bias by using methods
	that are either partially or completely time independent. We also
	discuss other design features to maximize the information yielded
	in a phase \{II\} setting. We advocate the creation of progression
	endpoints that utilize all available progression data rather than
	early fixed timepoint analyses and show that little is to be gained
	by assessing progression status any more frequently than would be
	required in routine clinical practice. Such design and analysis measures
	will optimize the development decision made at the end of phase \{II\}
	clinical evaluation. },
  doi = {http://dx.doi.org/10.1016/j.cct.2006.05.003},
  issn = {1551-7144},
  keywords = {Phase \{II\} trials},
  owner = {meddwilb},
  timestamp = {2013.12.10},
  url = {http://www.sciencedirect.com/science/article/pii/S1551714406000590}
}

@ARTICLE{Storer1989,
  author = {Barry E. Storer},
  title = {Design and Analysis of Phase {I} Clinical Trials},
  journal = {Biometrics},
  year = {1989},
  volume = {45},
  pages = {925-937},
  number = {3},
  doi = {10.2307/2531693},
  owner = {meddwilb},
  timestamp = {2013.12.02},
  url = {http://www.jstor.org/stable/2531693}
}

@Article{Stout2005,
  author    = {Quentin F. Stout and Janis Hardwick},
  title     = {Optimal screening designs with flexible cost and constraint structures},
  year      = {2005},
  volume    = {132},
  number    = {1-2},
  month     = {jun},
  pages     = {149--162},
  doi       = {10.1016/j.jspi.2004.06.020},
  url       = {http://dx.doi.org/10.1016/j.jspi.2004.06.020},
  groups    = {Bayesian, Decision theory},
  journal   = {Journal of Statistical Planning and Inference},
  owner     = {meddwilb},
  publisher = {Elsevier {BV}},
  timestamp = {2016.07.08},
}

@ARTICLE{Tan2003,
  author = {Say-Beng Tan and Keith B G Dear and Paolo Bruzzi and David Machin},
  title = {Strategy for randomised clinical trials in rare cancers},
  journal = {BMJ},
  year = {2003},
  volume = {327},
  pages = {47--49},
  number = {7405},
  month = {7},
  doi = {10.1136/bmj.327.7405.47},
  owner = {meddwilb},
  timestamp = {2014.06.13},
  url = {http://www.bmj.com/content/327/7405/47}
}

@ARTICLE{Tan2002,
  author = {Tan, Say-Beng and Machin, David},
  title = {Bayesian two-stage designs for phase {II} clinical trials},
  journal = {Statistics in Medicine},
  year = {2002},
  volume = {21},
  pages = {1991--2012},
  number = {14},
  abstract = {Many different statistical designs have been used in phase II clinical
	trials. The majority of these are based on frequentist statistical
	approaches. Bayesian methods provide a good alternative to frequentist
	approaches as they allow for the incorporation of relevant prior
	information and the presentation of the trial results in a manner
	which, some feel, is more intuitive and helpful. In this paper, we
	propose two new Bayesian designs for phase II clinical trials. These
	designs have been developed specifically to make them as user friendly
	and as familiar as possible to those who have had experience working
	with two-stage frequentist phase II designs. Thus, unlike many of
	the Bayesian designs already proposed in the literature, our designs
	do not require a distribution for the response rate of the currently
	used drug or the explicit specification of utility or loss functions.
	We study the properties of our designs and compare them with the
	Simon two-stage optimal and minimax designs. We also apply them to
	an example of two recently concluded phase II trials conducted at
	the National Cancer Centre in Singapore. Sample size tables for the
	designs are given. Copyright Â© 2002 John Wiley & Sons, Ltd.},
  doi = {10.1002/sim.1176},
  issn = {1097-0258},
  keywords = {phase II clinical trial, Bayesian, two-stage design, sample size},
  owner = {meddwilb},
  publisher = {John Wiley \& Sons, Ltd.},
  timestamp = {2013.12.11},
  url = {http://dx.doi.org/10.1002/sim.1176}
}

@Article{Tavare1995,
  author    = {Tavare, C. J. and Sobel, E. L. and Gilles, F. H.},
  title     = {Misclassification of a prognostic dichotomous variable: Sample size and parameter estimate adjustment},
  year      = {1995},
  volume    = {14},
  number    = {12},
  pages     = {1307--1314},
  issn      = {1097-0258},
  doi       = {10.1002/sim.4780141204},
  url       = {http://dx.doi.org/10.1002/sim.4780141204},
  abstract  = {Under general conditions, Lagakos showed that for an explanatory variable observed with error, the asymptotic relative efficiency (ARE) when using the observed rather than the true values in linear models, logistic models and proportional hazards models for survival is the square of the correlation between the true and observed variables. The result is useful for samle size adjustment when this correlation is estimable. Often, one cannot observe correct values of the explanatory variable under any circumstances. We show, however, that under the models considered by Lagakos for a dichotomous explanatory variable, the ARE equals the kappa statistic in a readâ€”reread protocol. Consequently, one need not know â€˜truthâ€™ in this situation to estimate the ARE and to adjust sample size to maintain desired power; divide the estimated sample size obtained with the assumption of no measurement error by the consistent estimate of the kappa statistic (which is unlikely to be zero or negative). We then develop heuristically an adjusted estimate of the Î² parameter in a proportional hazards survival model. The work was motivated by analyses of the Childhood Brain Tumour Consortium database. Examples from this database illustrate the method.},
  groups    = {SSR},
  journal   = {Statistics in Medicine},
  owner     = {meddwilb},
  publisher = {Wiley Subscription Services, Inc., A Wiley Company},
  timestamp = {2016.02.23},
}

@Article{Teare2014,
  author    = {Teare, M and Dimairo, Munyaradzi and Shephard, Neil and Hayman, Alex and Whitehead, Amy and Walters, Stephen},
  title     = {Sample size requirements to estimate key design parameters from external pilot randomised controlled trials: a simulation study},
  year      = {2014},
  volume    = {15},
  number    = {1},
  pages     = {264},
  issn      = {1745-6215},
  doi       = {10.1186/1745-6215-15-264},
  url       = {http://www.trialsjournal.com/content/15/1/264},
  abstract  = {BACKGROUND:External pilot or feasibility studies can be used to estimate
	key unknown parameters to inform the design of the definitive randomised
	controlled trial (RCT). However, there is little consensus on how
	large pilot studies need to be, and some suggest inflating estimates
	to adjust for the lack of precision when planning the definitive
	RCT.METHODS:We use a simulation approach to illustrate the sampling
	distribution of the standard deviation for continuous outcomes and
	the event rate for binary outcomes. We present the impact of increasing
	the pilot sample size on the precision and bias of these estimates,
	and predicted power under three realistic scenarios. We also illustrate
	the consequences of using a confidence interval argument to inflate
	estimates so the required power is achieved with a pre-specified
	level of confidence. We limit our attention to external pilot and
	feasibility studies prior to a two-parallel-balanced-group superiority
	RCT.RESULTS:For normally distributed outcomes, the relative gain
	in precision of the pooled standard deviation (SDp) is less than
	10% (for each five subjects added per group) once the total sample
	size is 70. For true proportions between 0.1 and 0.5, we find the
	gain in precision for each five subjects added to the pilot sample
	is less than 5% once the sample size is 60. Adjusting the required
	sample sizes for the imprecision in the pilot study estimates can
	result in excessively large definitive RCTs and also requires a pilot
	sample size of 60 to 90 for the true effect sizes considered here.CONCLUSIONS:We
	recommend that an external pilot study has at least 70 measured subjects
	(35 per group) when estimating the SDp for a continuous outcome.
	If the event rate in an intervention group needs to be estimated
	by the pilot then a total of 60 to 100 subjects is required. Hence
	if the primary outcome is binary a total of at least 120 subjects
	(60 in each group) may be required in the pilot trial. It is very
	much more efficient to use a larger pilot study, than to guard against
	the lack of precision by using inflated estimates.},
  groups    = {Pilot/feasibility},
  journal   = {Trials},
  owner     = {meddwilb},
  pubmedid  = {24993581},
  timestamp = {2014.09.10},
}

@Article{Thabane2010,
  author    = {Thabane, Lehana and Ma, Jinhui and Chu, Rong and Cheng, Ji and Ismaila, Afisi and Rios, Lorena and Robson, Reid and Thabane, Marroon and Giangregorio, Lora and Goldsmith, Charles},
  title     = {A tutorial on pilot studies: the what, why and how},
  year      = {2010},
  volume    = {10},
  number    = {1},
  pages     = {1},
  issn      = {1471-2288},
  doi       = {10.1186/1471-2288-10-1},
  url       = {http://www.biomedcentral.com/1471-2288/10/1},
  abstract  = {Pilot studies for phase III trials - which are comparative randomized
	trials designed to provide preliminary evidence on the clinical efficacy
	of a drug or intervention - are routinely performed in many clinical
	areas. Also commonly know as "feasibility" or "vanguard" studies,
	they are designed to assess the safety of treatment or interventions;
	to assess recruitment potential; to assess the feasibility of international
	collaboration or coordination for multicentre trials; to increase
	clinical experience with the study medication or intervention for
	the phase III trials. They are the best way to assess feasibility
	of a large, expensive full-scale study, and in fact are an almost
	essential pre-requisite. Conducting a pilot prior to the main study
	can enhance the likelihood of success of the main study and potentially
	help to avoid doomed main studies. The objective of this paper is
	to provide a detailed examination of the key aspects of pilot studies
	for phase III trials including: 1) the general reasons for conducting
	a pilot study; 2) the relationships between pilot studies, proof-of-concept
	studies, and adaptive designs; 3) the challenges of and misconceptions
	about pilot studies; 4) the criteria for evaluating the success of
	a pilot study; 5) frequently asked questions about pilot studies;
	7) some ethical aspects related to pilot studies; and 8) some suggestions
	on how to report the results of pilot investigations using the CONSORT
	format.},
  groups    = {Pilot/feasibility},
  journal   = {BMC Medical Research Methodology},
  owner     = {meddwilb},
  pubmedid  = {20053272},
  timestamp = {2013.09.17},
}

@Article{Thabane2013,
  author =    {Lehana Thabane and Lawrence Mbuagbaw and Shiyuan Zhang and Zainab Samaan and Maura Marcucci and Chenglin Ye and Marroon Thabane and Lora Giangregorio and Brittany Dennis and Daisy Kosa and Victoria Borg Debono and Rejane Dillenburg and Vincent Fruci and Monica Bawor and Juneyoung Lee and George Wells and Charles H Goldsmith},
  title =     {A tutorial on sensitivity analyses in clinical trials: the what, why, when and how},
  journal =   {{BMC} Med Res Methodol},
  year =      {2013},
  volume =    {13},
  number =    {1},
  month =     {jul},
  doi =       {10.1186/1471-2288-13-92},
  owner =     {meddwilb},
  publisher = {Springer Nature},
  timestamp = {2016.07.15},
  url =       {http://dx.doi.org/10.1186/1471-2288-13-92}
}

@Article{Thall2008,
  author    = {Peter F. Thall},
  title     = {Some geometric methods for constructing decision criteria based on two-dimensional parameters},
  year      = {2008},
  volume    = {138},
  number    = {2},
  month     = {feb},
  pages     = {516--527},
  doi       = {10.1016/j.jspi.2007.06.013},
  url       = {http://dx.doi.org/10.1016/j.jspi.2007.06.013},
  groups    = {Decision theory, Multiple endpoints},
  journal   = {Journal of Statistical Planning and Inference},
  owner     = {meddwilb},
  publisher = {Elsevier {BV}},
  timestamp = {2016.07.25},
}

@Article{Thall1999,
  author    = {Peter F. Thall and Su-Chun Cheng},
  title     = {Treatment Comparisons Based on Two-Dimensional Safety and Efficacy Alternatives in Oncology Trials},
  year      = {1999},
  volume    = {55},
  number    = {3},
  month     = {sep},
  pages     = {746--753},
  doi       = {10.1111/j.0006-341x.1999.00746.x},
  url       = {http://dx.doi.org/10.1111/j.0006-341X.1999.00746.x},
  groups    = {Multiple endpoints},
  journal   = {Biometrics},
  owner     = {meddwilb},
  publisher = {Wiley-Blackwell},
  timestamp = {2016.07.25},
}

@Article{Thall2001,
  author    = {Peter F. Thall and Su-Chun Cheng},
  title     = {Optimal two-stage designs for clinical trials based on safety and efficacy},
  year      = {2001},
  volume    = {20},
  number    = {7},
  pages     = {1023--1032},
  doi       = {10.1002/sim.717},
  url       = {http://dx.doi.org/10.1002/sim.717},
  groups    = {Multiple endpoints},
  journal   = {Statist. Med.},
  owner     = {meddwilb},
  publisher = {Wiley-Blackwell},
  timestamp = {2016.07.25},
}

@ARTICLE{Thall2013,
  author = {Thall, Peter F. and Nguyen, Hoang Q. and Braun, Thomas M. and Qazilbash,
	Muzaffar H.},
  title = {Using Joint Utilities of the Times to Response and Toxicity to Adaptively
	Optimize Schedule-Dose Regimes},
  journal = {Biometrics},
  year = {2013},
  volume = {69},
  pages = {673--682},
  number = {3},
  abstract = {A Bayesian two-stage phase Iâ€“II design is proposed for optimizing
	administration schedule and dose of an experimental agent based on
	the times to response and toxicity in the case where schedules are
	non-nested and qualitatively different. Sequentially adaptive decisions
	are based on the joint utility of the two event times. A utility
	function is constructed by partitioning the two-dimensional positive
	real quadrant of possible event time pairs into rectangles, eliciting
	a numerical utility for each rectangle, and fitting a smooth parametric
	function to the elicited values. We assume that each event time follows
	a gamma distribution with shape and scale parameters both modeled
	as functions of schedule and dose. A copula is assumed to obtain
	a bivariate distribution. To ensure an ethical trial, adaptive safety
	and efficacy acceptability conditions are imposed on the (schedule,
	dose) regimes. In stage 1 of the design, patients are randomized
	fairly among schedules and, within each schedule, a dose is chosen
	using a hybrid algorithm that either maximizes posterior mean utility
	or randomizes among acceptable doses. In stage 2, fair randomization
	among schedules is replaced by the hybrid algorithm. A modified version
	of this algorithm is used for nested schedules. Extensions of the
	model and utility function to accommodate death or discontinuation
	of follow up are described. The method is illustrated by an autologous
	stem cell transplantation trial in multiple myeloma, including a
	simulation study.},
  doi = {10.1111/biom.12065},
  issn = {1541-0420},
  keywords = {Adaptive decision making, Bayesian design, Phase I/II clinical trial,
	Stem cell transplantation, Utility},
  owner = {meddwilb},
  timestamp = {2014.06.11},
  url = {http://dx.doi.org/10.1111/biom.12065}
}

@Article{Thall1995,
  author    = {Peter F. Thall and Richard M. Simon and Elihu H. Estey},
  title     = {{Bayes}ian sequential monitoring designs for single-arm clinical trials with multiple outcomes},
  year      = {1995},
  volume    = {14},
  number    = {4},
  month     = {feb},
  pages     = {357--379},
  doi       = {10.1002/sim.4780140404},
  url       = {http://dx.doi.org/10.1002/sim.4780140404},
  groups    = {Bayesian, Multiple endpoints},
  journal   = {Statist. Med.},
  owner     = {meddwilb},
  publisher = {Wiley-Blackwell},
  timestamp = {2016.07.25},
}

@Article{Thall1998,
  author    = {Thall, Peter F. and Sung, Hsi-Guang},
  title     = {Some extensions and applications of a Bayesian strategy for monitoring multiple outcomes in clinical trials},
  year      = {1998},
  volume    = {17},
  number    = {14},
  pages     = {1563--1580},
  issn      = {1097-0258},
  doi       = {10.1002/(SICI)1097-0258(19980730)17:14<1563::AID-SIM873>3.0.CO;2-L},
  url       = {http://dx.doi.org/10.1002/(SICI)1097-0258(19980730)17:14<1563::AID-SIM873>3.0.CO;2-L},
  abstract  = {We present some practical extensions and applications of a strategy
	proposed by Thall, Simon and Estey for designing and monitoring single-arm
	clinical trials with multiple outcomes. We show by application how
	the strategy may be applied to construct designs for phase IIA activity
	trials and phase II equivalence trials. We also show how it may be
	extended to incorporate the use of mixture priors in settings where
	a Dirichlet distribution does not adequately quantify prior experience,
	randomized phase II selection trials involving two or more experimental
	treatments, and trials with group-sequential monitoring for applications
	involving multiple institutions. Â© 1998 John Wiley & Sons, Ltd.},
  groups    = {Multiple endpoints},
  journal   = {Statistics in Medicine},
  owner     = {meddwilb},
  publisher = {Wiley Subscription Services, Inc., A Wiley Company},
  timestamp = {2014.06.05},
}

@ARTICLE{Thompson2004,
  author = {Thompson, Simon G. and Warn, David E. and Turner, Rebecca M.},
  title = {Bayesian methods for analysis of binary outcome data in cluster randomized
	trials on the absolute risk scale},
  journal = {Statistics in Medicine},
  year = {2004},
  volume = {23},
  pages = {389--410},
  number = {3},
  doi = {10.1002/sim.1567},
  issn = {1097-0258},
  keywords = {cluster randomized trials, Bayesian methods, binary data, absolute
	risk scale},
  owner = {meddwilb},
  publisher = {John Wiley \& Sons, Ltd.},
  timestamp = {2013.09.12},
  url = {http://dx.doi.org/10.1002/sim.1567}
}

@ARTICLE{Tober2008,
  author = {Tober, Gillian and Clyne, Wendy and Finnegan, Olwyn and Farrin, Amanda
	and Team, Ian Russell and in collaboration with the UKATT Research
	Team},
  title = {Validation of a Scale for Rating the Delivery of Psycho-Social Treatments
	for Alcohol Dependence and Misuse: The {UKATT Process Rating Scale
	(PRS)}},
  journal = {Alcohol and Alcoholism},
  year = {2008},
  volume = {43},
  pages = {675-682},
  number = {6},
  abstract = {Aim: The aim of this study was to describe the development and validation
	of the UK Alcohol Treatment Trial Process Rating Scale (UKATT PRS),
	a manual based method for monitoring and rating the delivery of psychosocial
	treatments of alcohol dependence and misuse. Methods: Following adaptation
	and further development of a validated rating scale, the ability
	of the UKATT PRS to rate the delivery of video-recorded treatment
	in the UK Alcohol Treatment Trial (UKATT) was tested. Results: Tests
	of the validity and reliability of the UKATT PRS show that it is
	valid and reliably able to detect the two treatments for which it
	was designed and to discriminate between them. Conclusions: The UKATT
	PRS is a valid and reliable method of rating the frequency and quality
	of therapeutic style and content in the delivery of two psycho-social
	treatments of alcohol use and dependence.},
  doi = {10.1093/alcalc/agn064},
  eprint = {http://alcalc.oxfordjournals.org/content/43/6/675.full.pdf+html},
  owner = {meddwilb},
  timestamp = {2014.03.25},
  url = {http://alcalc.oxfordjournals.org/content/43/6/675.abstract}
}

@Article{TORRISI2006,
  author    = {TORRISI, R. and ORLANDO, L. and GHISINI, R. and VERONESI, P. and INTRA, M. and ROCCA, A. and BALDUZZI, A. and CARDILLO, A. and GOLDHIRSCH, A. and COLLEONI, M.},
  title     = {A Phase {II} Study of Primary Dose-dense Sequential Doxorubicin Plus Cyclophosphamide and Docetaxel in cT4 Breast Cancer},
  journal   = {Anticancer Research},
  year      = {2006},
  volume    = {26},
  number    = {5B},
  pages     = {3861-3864},
  eprint    = {http://ar.iiarjournals.org/content/26/5B/3861.full.pdf+html},
  url       = {http://ar.iiarjournals.org/content/26/5B/3861.abstract},
  abstract  = {Background: Dose-dense chemotherapy with anthracyclines and taxanes
	has improved either disease free survival or overall survival in
	high risk patients with early breast cancer. Patients and Methods:
	The activity and safety of a dose-dense schedule (q14 days) of adriamycin
	60 mg/sqm and cyclophosphamide 600 mg/sqm (AC) Ã— 4 cycles followed
	by docetaxel 75 mg/sqm for 4 cycles with hematopoietic support in
	patients with stage IIIB breast cancer was explored. Patients with
	ER â‰¥10% tumors received concomitant endocrine therapy with 3-month
	triptorelin and letrozole. Results: Fifteen patients with histologically
	proven cT4b (three patients) and cT4d (twelve patients) M0 breast
	cancer were enrolled. Median age was 48 years (range 25-66). Eight
	clinical responses including one pathological complete remission
	(pCR), three stable disease (including minor responses) and four
	progression of disease, one during AC and three during taxotere,
	were observed. Four patients had grade 3-4 non hematological toxicities
	and all except one discontinued treatment. Conclusion: Due to the
	high rate of progressive disease, this schedule should not represent
	a standard option in cT4 breast cancer.},
  comment   = {used design gives
	
	
	[1] 0.1 0.3 0.6 0.8
	
	[1] 0.10 0.15 0.10
	
	[1] 1 9 15 5 22 33
	
	[1] "Expected size: 4802.78484694937"
	
	[1] "alpha_r: 0.0865942808819473"
	
	[1] "alpha_t: 0.133534522126153"
	
	[1] "power: 0.85219836810479"
	
	
	SSS gives
	
	
	[1] 3 14 21 7 28 41
	
	[1] "Expected size: 11963.04564242"
	
	[1] "alpha_r: 0.0307022481774732"
	
	[1] "alpha_t: 0.0668048581264793"
	
	[1] "power: 0.780616164334469"
	
	
	Local search gives
	
	
	[1] 1 9 16 6 26 39
	
	[1] "Expected size: 27.8084014839632"
	
	[1] "alpha_r: 0.0766838403036838"
	
	[1] "alpha_t: 0.13522537019716"
	
	[1] "power: 0.902387670999818"},
  owner     = {meddwilb},
  timestamp = {2014.03.03},
}

@Article{Tournoux2007,
  author    = {Caroline Tournoux and Yann De Rycke and Jacques MÃ©dioni and Bernard Asselain},
  title     = {Methods of joint evaluation of efficacy and toxicity in phase {II} clinical trials},
  journal   = {Contemporary Clinical Trials},
  year      = {2007},
  volume    = {28},
  number    = {4},
  pages     = {514 - 524},
  issn      = {1551-7144},
  doi       = {http://dx.doi.org/10.1016/j.cct.2007.01.008},
  url       = {http://www.sciencedirect.com/science/article/pii/S1551714407000109},
  abstract  = {Phase \{II\} clinical trials in oncology are usually conducted to
	evaluate the anti-tumor effect. Because phase I trials are small
	studies, the maximum tolerated dose of a new drug may not be precisely
	established and the recommended dose used may lead to excessive toxicity.
	We investigate the methods proposed by Conawayâ€“Petroni and Bryantâ€“Day
	allowing early termination of phase \{II\} clinical trials and based
	on joint evaluation of treatment efficacy and safety. Both study
	designs are computed to minimize the expected accrual under the null
	hypothesis. As two criteria are considered, the null hypothesis is
	an area. Each method defines two specific type I error risks. Bryantâ€“Day
	demonstrate that response and toxicity may be considered as independent
	(Î¦&#xa0;=&#xa0;1). We compare the properties of these two methods
	with exact calculation according to objective criteria and present
	one example from a study conducted in France. The two methods differ
	with regard to the definition of the risks and the assumption of
	independence. They are similar in terms of expected accruals when
	Î¦&#xa0;=&#xa0;1. Deviations from the assumption of independence
	induce minor consequences on the type I error risks when the constraint
	on the type \{II\} error risk is less than 15%. Choosing Î¦ has a
	minimal impact on expected accrual. Finally, one type I error risk
	(Î±00) defined by Conawayâ€“Petroni dramatically increases in the
	case of deviation from the assumption made on Î¦. Due to its robustness
	in relation to a deviation from the independence assumption, we recommend
	the use of the Bryantâ€“Day method in clinical practice. },
  comment   = {Compares Bryant \& Day with Conaway \& Petroni, concluding that the former is more robust to incorrect assumptions about the degree of correlation between the two outcomes.},
  groups    = {Multiple endpoints, Phase II},
  keywords  = {Phase \{II\} clinical trials},
  owner     = {meddwilb},
  timestamp = {2014.12.10},
}

@MANUAL{Trautmann2013,
  title = {mco: Multi criteria optimization algorithms and related functions},
  author = {Heike Trautmann and Detlef Steuer and Olaf Mersmann},
  year = {2013},
  note = {R package version 1.0.12},
  owner = {meddwilb},
  timestamp = {2014.02.18},
  url = {http://CRAN.R-project.org/package=mco}
}

@ARTICLE{Tugwell2013,
  author = {Peter Tugwell and {J. André} Knottnerus and Leanne Idzerda},
  title = {Complex interventions - how should systematic reviews of their impact
	differ from reviews of simple or complicated interventions?},
  journal = {Journal of Clinical Epidemiology},
  year = {2013},
  volume = {66},
  pages = {1195-1196},
  number = {11},
  doi = {10.1136/bmj.39569.510521.AD},
  owner = {meddwilb},
  timestamp = {2013.11.28}
}

@ARTICLE{Turner2001,
  author = {Turner, Rebecca M. and Omar, Rumana Z. and Thompson, Simon G.},
  title = {Bayesian methods of analysis for cluster randomized trials with binary
	outcome data},
  journal = {Statistics in Medicine},
  year = {2001},
  volume = {20},
  pages = {453--472},
  number = {3},
  doi = {10.1002/1097-0258(20010215)20:3<453::AID-SIM803>3.0.CO;2-L},
  issn = {1097-0258},
  owner = {meddwilb},
  publisher = {John Wiley \& Sons, Ltd.},
  timestamp = {2013.09.11},
  url = {http://dx.doi.org/10.1002/1097-0258(20010215)20:3<453::AID-SIM803>3.0.CO;2-L}
}

@ARTICLE{Turner2006,
  author = {Turner, Rebecca M. and Omar, Rumana Z. and Thompson, Simon G.},
  title = {Constructing intervals for the intracluster correlation coefficient
	using Bayesian modelling, and application in cluster randomized trials},
  journal = {Statistics in Medicine},
  year = {2006},
  volume = {25},
  pages = {1443--1456},
  number = {9},
  doi = {10.1002/sim.2304},
  issn = {1097-0258},
  keywords = {intracluster correlation coefficient, clustered data, cluster randomized
	trials, interval estimation, Bayesian methods},
  owner = {meddwilb},
  publisher = {John Wiley \& Sons, Ltd.},
  timestamp = {2013.09.13},
  url = {http://dx.doi.org/10.1002/sim.2304}
}

@ARTICLE{Turner2005,
  author = {Turner, Rebecca M and Thompson, Simon G and Spiegelhalter, David
	J},
  title = {Prior distributions for the intracluster correlation coefficient,
	based on multiple previous estimates, and their application in cluster
	randomized trials},
  journal = {Clinical Trials},
  year = {2005},
  volume = {2},
  pages = {108-118},
  number = {2},
  abstract = {Numerous estimates for the intracluster correlation coefficient (ICC)
	are available in research databases and publications. When planning
	a cluster randomized trial, an anticipated value for the ICC is required;
	currently, researchers base their choice informally on the magnitude
	of previous ICC estimates. In this paper, we make use of the wealth
	of ICC information by formally constructing informative prior distributions,
	while acknowledging the varying relevance and precision of the estimates
	available. Typically, for a planned trial in a given clinical setting,
	multiple relevant ICC estimates are available from each of several
	completed studies. Our preferred model allows for the imprecision
	in each ICC estimate around its underlying true value and, separately,
	allows for the similarity of ICC values from the same study. The
	relevance of each previous estimate to the planned clinical setting
	is considered, and estimates corresponding to less relevant outcomes
	or population types are given less influence. We find that such downweighting
	can increase the precision of the anticipated ICC. In trial design,
	the prior distribution constructed allows uncertainty about the ICC
	to be acknowledged, and we describe how to choose a design that provides
	adequate power across the range of likely ICC values. Prior information
	on the ICC can also be incorporated in analysis of the trial data,
	when taking a Bayesian approach. The methods proposed enable available
	ICC information to be summarised appropriately by an informative
	prior distribution, which is of direct practical use in cluster randomized
	trials.},
  doi = {10.1191/1740774505cn072oa},
  eprint = {http://ctj.sagepub.com/content/2/2/108.full.pdf+html},
  owner = {meddwilb},
  timestamp = {2013.09.13},
  url = {http://ctj.sagepub.com/content/2/2/108.abstract}
}

@ARTICLE{Turner2004,
  author = {Turner, Rebecca M. and Toby Prevost, A. and Thompson, Simon G.},
  title = {Allowing for imprecision of the intracluster correlation coefficient
	in the design of cluster randomized trials},
  journal = {Statistics in Medicine},
  year = {2004},
  volume = {23},
  pages = {1195--1214},
  number = {8},
  abstract = {The sample size required for a cluster randomized trial depends on
	the magnitude of the intracluster correlation coefficient (ICC).
	The usual sample size calculation makes no allowance for the fact
	that the ICC is not known precisely in advance. We develop methods
	which allow for the uncertainty in a previously observed ICC, using
	a variety of distributional assumptions. Distributions for the power
	are derived, reflecting this uncertainty. Further, the observed ICC
	in a future study will not equal its true value, and we consider
	the impact of this on power. We implement calculations within a Bayesian
	simulation approach, and provide one simplification that can be performed
	using simple simulation within spreadsheet software. In our examples,
	recognizing the uncertainty in a previous ICC estimate decreases
	expected power, especially when the power calculated naively from
	the ICC estimate is high. To protect against the possibility of low
	power, sample sizes may need to be very substantially increased.
	Recognizing the variability in the future observed ICC has little
	effect if prior uncertainty has already been taken into account.
	We show how our method can be extended to the case in which multiple
	prior ICC estimates are available. The methods presented in this
	paper can be used by applied researchers to protect against loss
	of power, or to choose a design which reduces the impact of uncertainty
	in the ICC. Copyright Â© 2004 John Wiley & Sons, Ltd.},
  doi = {10.1002/sim.1721},
  issn = {1097-0258},
  keywords = {intracluster correlation coefficient, cluster randomized trials, sample
	size, power, Bayesian methods, study design},
  owner = {meddwilb},
  publisher = {John Wiley \& Sons, Ltd.},
  timestamp = {2014.08.11},
  url = {http://dx.doi.org/10.1002/sim.1721}
}

@ARTICLE{Turner2007,
  author = {Turner, Rebecca M. and White, Ian R. and Croudace, Tim},
  title = {Analysis of cluster randomized cross-over trial data: a comparison
	of methods},
  journal = {Statistics in Medicine},
  year = {2007},
  volume = {26},
  pages = {274--289},
  number = {2},
  doi = {10.1002/sim.2537},
  issn = {1097-0258},
  keywords = {cluster randomized trials, cross-over design, hierarchical models,
	cluster-level methods},
  owner = {meddwilb},
  publisher = {John Wiley \& Sons, Ltd.},
  timestamp = {2013.09.13},
  url = {http://dx.doi.org/10.1002/sim.2537}
}

@ARTICLE{Ulungu1994,
  author = {Ulungu, E. L. and Teghem, J.},
  title = {Multi-objective combinatorial optimization problems: A survey},
  journal = {Journal of Multi-Criteria Decision Analysis},
  year = {1994},
  volume = {3},
  pages = {83--104},
  number = {2},
  abstract = {In the last 20 years many multi-objective linear programming (MOLP)
	methods with continuous variables have been developed. However, in
	many real-world applications discrete variables must be introduced.
	It is well known that MOLP problems with discrete variables can have
	special difficulties and so cannot be solved by simply combining
	discrete programming methods and multi-objective programming methods.The
	present paper is intended to review the existing literature on multi-objective
	combinatorial optimization (MOCO) problems. Various classical combinatorial
	problems are examined in a multi-criteria framework. Some conclusions
	are drawn and directions for future research are suggested.},
  doi = {10.1002/mcda.4020030204},
  issn = {1099-1360},
  keywords = {Assignment and allocation problems, Knapsack problems, Location problems,
	Multi-objective combinatorial optimization (MOCO), Network flow problems
	Transportation problems, Transportation problems, Transhipment problems,
	Travelling salesman problem},
  owner = {meddwilb},
  publisher = {John Wiley \& Sons, Ltd.},
  timestamp = {2014.07.21},
  url = {http://dx.doi.org/10.1002/mcda.4020030204}
}

@MANUAL{Vaidyanathan2013,
  title = {{rCharts}: Interactive Charts using Polycharts.js},
  author = {Ramnath Vaidyanathan},
  year = {2013},
  note = {R package version 0.4.2},
  owner = {meddwilb},
  timestamp = {2014.03.05}
}

@ARTICLE{Vaughn2013,
  author = {Vaughn, AmberE and Tabak, RachelG and Bryant, MariaJ and Ward, DianneS},
  title = {Measuring parent food practices: a systematic review of existing
	measures and examination of instruments},
  journal = {International Journal of Behavioral Nutrition and Physical Activity},
  year = {2013},
  volume = {10},
  pages = {1-27},
  number = {1},
  doi = {10.1186/1479-5868-10-61},
  keywords = {Feeding; Measures development; Psychometric properties},
  language = {English},
  owner = {meddwilb},
  publisher = {BioMed Central},
  timestamp = {2014.03.25},
  url = {http://dx.doi.org/10.1186/1479-5868-10-61}
}

@ARTICLE{Walker2009,
  author = {Ian Walker and Herbie Newell},
  title = {Do molecularly targeted agents in oncology have reduced attrition
	rates?},
  journal = {Nature Reviews Drug Discovery},
  year = {2009},
  volume = {8},
  pages = {15-16},
  doi = {10.1038/nrd2758},
  owner = {meddwilb},
  timestamp = {2014.07.21}
}

@Article{Walwyn2013,
  author    = {Walwyn, Rebecca and Potts, Laura and McCrone, Paul and Johnson, Anthony and DeCesare, Julia and Baber, Hannah and Goldsmith, Kimberley and Sharpe, Michael and Chalder, Trudie and White, Peter},
  title     = {A randomised trial of adaptive pacing therapy, cognitive behaviour therapy, graded exercise, and specialist medical care for chronic fatigue syndrome (PACE): statistical analysis plan},
  year      = {2013},
  volume    = {14},
  number    = {1},
  pages     = {386},
  issn      = {1745-6215},
  doi       = {10.1186/1745-6215-14-386},
  url       = {http://www.trialsjournal.com/content/14/1/386},
  abstract  = {BACKGROUND:The publication of protocols by medical journals is increasingly
	becoming an accepted means for promoting good quality research and
	maximising transparency. Recently, Finfer and Bellomo have suggested
	the publication of statistical analysis plans (SAPs).The aim of this
	paper is to make public and to report in detail the planned analyses
	that were approved by the Trial Steering Committee in May 2010 for
	the principal papers of the PACE (Pacing, graded Activity, and Cognitive
	behaviour therapy: a randomised Evaluation) trial, a treatment trial
	for chronic fatigue syndrome. It illustrates planned analyses of
	a complex intervention trial that allows for the impact of clustering
	by care providers, where multiple care-providers are present for
	each patient in some but not all arms of the trial.RESULTS:The trial
	design, objectives and data collection are reported. Considerations
	relating to blinding, samples, adherence to the protocol, stratification,
	centre and other clustering effects, missing data, multiplicity and
	compliance are described. Descriptive, interim and final analyses
	of the primary and secondary outcomes are then outlined.CONCLUSIONS:This
	SAP maximises transparency, providing a record of all planned analyses,
	and it may be a resource for those who are developing SAPs, acting
	as an illustrative example for teaching and methodological research.
	It is not the sum of the statistical analysis sections of the principal
	papers, being completed well before individual papers were drafted.TRIAL
	REGISTRATION:ISRCTN54285094 assigned 22 May 2003; First participant
	was randomised on 18 March 2005.},
  groups    = {PACE},
  journal   = {Trials},
  owner     = {meddwilb},
  pubmedid  = {24225069},
  timestamp = {2015.01.23},
}

@PHDTHESIS{Walwyn2010a,
  author = {Rebecca E. A. Walwyn},
  title = {Therapist Variation within Meta-Analyses of Psychotherapy Trials},
  school = {University of Manchester},
  year = {2010},
  owner = {meddwilb},
  timestamp = {2014.08.20}
}

@ARTICLE{Walwyn2010,
  author = {Walwyn, Rebecca E. A. and Roberts, Chris},
  title = {Therapist variation within randomised trials of psychotherapy: implications
	for precision, internal and external validity},
  journal = {Statistical Methods in Medical Research},
  year = {2010},
  volume = {19},
  pages = {291-315},
  number = {3},
  abstract = {Nesting of patients within therapists in psychotherapy trials creates
	an additional level within the design. The multilevel nature of this
	design has implications for the precision, internal and external
	validity of estimates of the treatment effect. Prior to or during
	a trial, psychotherapies are allocated to therapists and therapists
	are assigned to patients such that the therapist becomes part of
	the causal pathway from the intervention to the patient. It is therefore
	important to consider not only the relationship between interventions
	and patients but also relationships between interventions and therapists
	and between therapists and patients. Research designs comparing the
	effects of therapeutic approaches, therapist characteristics and
	packages of the two can be unified by viewing therapists as an important
	source of variability within psychotherapy outcome studies. Methodological
	considerations arising from therapist variation will be discussed,
	drawing together and building upon the associated psychotherapy and
	statistical literatures. Parallels will also be made with related
	designs and methods of analysis.},
  doi = {10.1177/0962280209105017},
  eprint = {http://smm.sagepub.com/content/19/3/291.full.pdf+html},
  owner = {meddwilb},
  timestamp = {2013.11.15},
  url = {http://smm.sagepub.com/content/19/3/291.abstract}
}

@ARTICLE{Wampold2000,
  author = {Wampold, Bruce E. and Serlin, Ronald C.},
  title = {The consequence of ignoring a nested factor on measures of effect
	size in analysis of variance},
  journal = {Psychological Methods},
  year = {2000},
  volume = {5},
  pages = {425-433},
  number = {4},
  doi = {10.1037/1082-989X.5.4.425},
  owner = {meddwilb},
  timestamp = {2014.07.18}
}

@Article{Wang2002,
  author              = {Wang, Fei and Gelfand, Alan E.},
  title               = {A Simulation-Based Approach to Bayesian Sample Size Determination for Performance under a Given Model and for Separating Models},
  year                = {2002},
  language            = {English},
  volume              = {17},
  number              = {2},
  pages               = {pp. 193-208},
  issn                = {08834237},
  url                 = {http://www.jstor.org/stable/3182824},
  abstract            = {Sample size determination (SSD) is a crucial aspect of experimental
	design. Two SSD problems are considered here. The first concerns
	how to select a sample size to achieve specified performance with
	regard to one or more features of a model. Adopting a Bayesian perspective,
	we move the Bayesian SSD problem from the rather elementary models
	addressed in the literature to date in the direction of the wide
	range of hierarchical models which dominate the current Bayesian
	landscape. Our approach is generic and thus, in principle, broadly
	applicable. However, it requires full model specification and computationally
	intensive simulation, perhaps limiting it practically to simple instances
	of such models. Still, insight from such cases is of useful design
	value. In addition, we present some theoretical tools for studying
	performance as a function of sample size, with a variety of illustrative
	results. Such results provide guidance with regard to what is achievable.
	We also offer two examples, a survival model with censoring and a
	logistic regression model. The second problem concerns how to select
	a sample size to achieve specified separation of two models. We approach
	this problem by adopting a screening criterion which in turn forms
	a model choice criterion. This criterion is set up to choose model
	1 when the value is large, model 2 when the value is small. The SSD
	problem then requires choosing n<sub>1</sub> to make the probability
	of selecting model 1 when model 1 is true sufficiently large and
	choosing n<sub>2</sub> to make the probability of selecting model
	2 when model 2 is true sufficiently large. The required n is max
	(n<sub>1</sub>, n<sub>2</sub>). Here, we again provide two illustrations.
	One considers separating normal errors from t errors, the other separating
	a common growth curve model from a model with individual growth curves.},
  copyright           = {Copyright Â© 2002 Institute of Mathematical Statistics},
  groups              = {multi-D / sim SS},
  journal             = {Statistical Science},
  jstor_articletype   = {research-article},
  jstor_formatteddate = {May, 2002},
  owner               = {meddwilb},
  publisher           = {Institute of Mathematical Statistics},
  timestamp           = {2014.10.27},
}

@INPROCEEDINGS{Wang2010,
  author = {Yuanyuan Wang and Roger Day},
  title = {An {R} Package for Simulation Experiments Evaluating Clinical Trial
	Designs},
  booktitle = {AMIA Summits on Translational Science Proceedings},
  year = {2010},
  pages = {61–65},
  owner = {meddwilb},
  timestamp = {2014.03.04},
  url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3041540/}
}

@ARTICLE{Wason2013,
  author = {Wason, James M. S. and Jaki, Thomas and Stallard, Nigel},
  title = {Planning multi-arm screening studies within the context of a drug
	developmentâ€‰program},
  journal = {Statistics in Medicine},
  year = {2013},
  volume = {32},
  pages = {3424--3435},
  number = {20},
  abstract = {Screening trials are small trials used to decide whether an intervention
	is sufficiently promising to warrant a large confirmatory trial.
	Previous literature examined the situation where treatments are tested
	sequentially until one is considered sufficiently promising to take
	forward to a confirmatory trial. An important consideration for sponsors
	of clinical trials is how screening trials should be planned to maximize
	the efficiency of the drug development process. It has been found
	previously that small screening trials are generally the most efficient.
	In this paper we consider the design of screening trials in which
	multiple new treatments are tested simultaneously. We derive analytic
	formulae for the expected number of patients until a successful treatment
	is found, and propose methodology to search for the optimal number
	of treatments, and optimal sample size per treatment. We compare
	designs in which only the best treatment proceeds to a confirmatory
	trial and designs in which multiple treatments may proceed to a multi-arm
	confirmatory trial. We find that inclusion of a large number of treatments
	in the screening trial is optimal when only one treatment can proceed,
	and a smaller number of treatments is optimal when more than one
	can proceed. The designs we investigate are compared on a real-life
	set of screening designs. Copyright Â© 2013 John Wiley & Sons, Ltd.},
  doi = {10.1002/sim.5787},
  issn = {1097-0258},
  keywords = {multi-arm multi-stage trials, optimal design, phase II trials, screening
	trials},
  owner = {meddwilb},
  timestamp = {2015.06.08},
  url = {http://dx.doi.org/10.1002/sim.5787}
}

@BOOK{Weerahandi1995,
  title = {Exact Statistical Methods for Data Analysis},
  publisher = {Springer-Verlag},
  year = {1995},
  author = {Samaradasa Weerahandi},
  edition = {1st},
  owner = {meddwilb},
  timestamp = {2014.04.02}
}

@ARTICLE{Weiss1997,
  author = {Weiss, Robert},
  title = {Bayesian sample size calculations for hypothesis testing},
  journal = {Journal of the Royal Statistical Society: Series D (The Statistician)},
  year = {1997},
  volume = {46},
  pages = {185--191},
  number = {2},
  abstract = {A Bayesian approach to sample size calculation in hypothesis testing
	problems is developed. The sample size is chosen to make it a priori
	probable that the Bayes factor is greater than a given cut-off of
	prespecified size. Two methods for choosing a cut-off are given:
	an absolute criterion and a relative criterion. Calculations can
	be done using either exact algebraic manipulation or through simulation.
	The approach permits the propagation of uncertainty in quantities
	which are unknown and permits the computation of power and type I
	error rates either conditionally or unconditionally on particular
	values of the parameter of interest. A graphical tool is given for
	assessing the sensitivity of the predicted outcomes to model and
	sample size specification. The approach is illustrated for a one-sided
	and for a two-sided alternative hypothesis for continuous data with
	a normal prior.},
  doi = {10.1111/1467-9884.00075},
  issn = {1467-9884},
  keywords = {Bayes factor, Hypothesis test, Model selection},
  owner = {meddwilb},
  publisher = {Blackwell Publishers Ltd},
  timestamp = {2014.10.27},
  url = {http://dx.doi.org/10.1111/1467-9884.00075}
}

@ARTICLE{Welton2009,
  author = {Welton, Nicky J. and Caldwell, D. M. and Adamopoulos, E. and Vedhara,
	K.},
  title = {Mixed Treatment Comparison Meta-Analysis of Complex Interventions:
	Psychological Interventions in Coronary Heart Disease},
  journal = {American Journal of Epidemiology},
  year = {2009},
  volume = {169},
  pages = {1158-1165},
  number = {9},
  abstract = {Meta-analyses of psychological interventions typically find a pooled
	effect of â€œpsychological interventionâ€? compared with usual care.
	This answers the research question, â€œAre psychological interventions
	in general effective?â€? In fact, psychological interventions are
	usually complex with several different components. The authors propose
	that mixed treatment comparison meta-analysis methods may be a valuable
	tool when exploring the efficacy of interventions with different
	components and combinations of components, as this allows one to
	answer the research question, â€œAre interventions with a particular
	component (or combination of components) effective?â€? The authors
	illustrate the methods using a meta-analysis of psychological interventions
	for patients with coronary heart disease for a variety of outcomes.
	The authors carried out systematic literature searches to update
	an earlier Cochrane review and classified components of interventions
	into 6 types: usual care, educational, behavioral, cognitive, relaxation,
	and support. Most interventions were a combination of these components.
	There was some evidence that psychological interventions were effective
	in reducing total cholesterol and standardized mean anxiety scores,
	that interventions with behavioral components were effective in reducing
	the odds of all-cause mortality and nonfatal myocardial infarction,
	and that interventions with behavioral and/or cognitive components
	were associated with reduced standardized mean depression scores.},
  doi = {10.1093/aje/kwp014},
  eprint = {http://aje.oxfordjournals.org/content/169/9/1158.full.pdf+html},
  owner = {meddwilb},
  timestamp = {2013.09.17},
  url = {http://aje.oxfordjournals.org/content/169/9/1158.abstract}
}

@ARTICLE{Whitaker2013,
  author = {Whitaker, Rhiannon and Ballard, Clive and Stafford, Jane and Orrell,
	Martin and Moniz-Cook, Esme and Woods, Robert and Murray, Joanna
	and Knapp, Martin and Carlton, Barbara Woodward and Fossey, Jane},
  title = {Feasibility study of an optimised person-centred intervention to
	improve mental health and reduce antipsychotics amongst people with
	dementia in care homes: study protocol for a randomised controlled
	trial},
  journal = {Trials},
  year = {2013},
  volume = {14},
  pages = {13},
  number = {1},
  abstract = {BACKGROUND:People living in care homes often have complex mental and
	physical health problems, disabilities and social needs which are
	compounded by the use of psychiatric and other drugs. In the UK dementia
	care is a national priority with a vast impact on services. WHELD
	combines the most effective elements of existing approaches to develop
	a comprehensive but practical intervention. This will be achieved
	by training care staff to provide care that is focused on an understanding
	of the individual and their needs; and by using additional components
	such as exercise, activities and social interaction to improve mental
	health and quality of life (QoL) and reduce the use of sedative drugs.DESIGN:Work
	Package 3 (WP3) is the pilot randomised trial and qualitative evaluation
	to help develop a future definitive randomised controlled clinical
	trial. The study design is a cluster randomised 2x2x2 factorial design
	with two replications in 16 care homes. Each care home is randomized
	to receive one of the eight possible permutations of the four key
	interventions, with each possible combination delivered in two of
	the 16 homes. Each cluster includes a minimum of 12 participants
	(depending upon size of the care home, the number of people with
	dementia and the number consenting).DISCUSSION:The overarching goal
	of the programme is to provide an effective, simple and practical
	intervention which improves the mental health of, and reduces sedative
	drug use in, people with dementia in care homes and which can be
	implemented nationally in all UK care homes as an NHS intervention.TRIAL
	REGISTRATION:Current controlled trials ISRCTN40313497},
  doi = {10.1186/1745-6215-14-13},
  issn = {1745-6215},
  owner = {meddwilb},
  pubmedid = {23305152},
  timestamp = {2013.10.11},
  url = {http://www.trialsjournal.com/content/14/1/13}
}

@Article{White2011,
  author    = {White, PD and Goldsmith, KA and Johnson, AL and Potts, L and Walwyn, R and DeCesare, JC and Baber, HL and Burgess, M and Clark, LV and Cox, DL and Bavinton, J and Angus, BJ and Murphy, G and Murphy, M and O'Dowd, H and Wilks, D and McCrone, P and Chalder, T and Sharpe, M},
  title     = {Comparison of adaptive pacing therapy, cognitive behaviour therapy, graded exercise therapy, and specialist medical care for chronic fatigue syndrome (PACE): a randomised trial},
  year      = {2011},
  volume    = {377},
  number    = {9768},
  pages     = {823--836},
  doi       = {10.1016/S0140-6736(11)60096-2},
  url       = {http://www.thelancet.com/journals/lancet/article/PIIS0140-6736(11)60096-2/abstract},
  abstract  = {BackgroundTrial findings show cognitive behaviour therapy (CBT) and
	graded exercise therapy (GET) can be effective treatments for chronic
	fatigue syndrome, but patients' organisations have reported that
	these treatments can be harmful and favour pacing and specialist
	health care. We aimed to assess effectiveness and safety of all four
	treatments.
	
	BackgroundTrial findings show cognitive behaviour therapy (CBT) and
	graded exercise therapy (GET) can be effective treatments for chronic
	fatigue syndrome, but patients' organisations have reported that
	these treatments can be harmful and favour pacing and specialist
	health care. We aimed to assess effectiveness and safety of all four
	treatments.},
  booktitle = {The Lancet},
  comment   = {doi: 10.1016/S0140-6736(11)60096-2},
  groups    = {PACE},
  journal   = {The Lancet},
  owner     = {meddwilb},
  publisher = {Elsevier},
  timestamp = {2015.01.23},
}

@Article{White2007,
  author    = {White, Peter and Sharpe, Michael and Chalder, Trudie and DeCesare, Julia and Walwyn, Rebecca and the PACE trial group},
  title     = {Protocol for the PACE trial: A randomised controlled trial of adaptive pacing, cognitive behaviour therapy, and graded exercise as supplements to standardised specialist medical care versus standardised specialist medical care alone for patients with the chronic fatigue syndrome/myalgic encephalomyelitis or encephalopathy},
  year      = {2007},
  volume    = {7},
  number    = {1},
  pages     = {6},
  issn      = {1471-2377},
  doi       = {10.1186/1471-2377-7-6},
  url       = {http://www.biomedcentral.com/1471-2377/7/6},
  abstract  = {BACKGROUND:Chronic fatigue syndrome (CFS, also called myalgic encephalomyelitis/encephalopathy
	or ME) is a debilitating condition with no known cause or cure. Improvement
	may occur with medical care and additional therapies of pacing, cognitive
	behavioural therapy and graded exercise therapy. The latter two therapies
	have been found to be efficacious in small trials, but patient organisations'
	surveys have reported adverse effects. Although pacing has been advocated
	by patient organisations, it lacks empirical support. Specialist
	medical care is commonly provided but its efficacy when given alone
	is not established. This trial compares the efficacy of the additional
	therapies when added to specialist medical care against specialist
	medical care alone.METHODS/DESIGN:600 patients, who meet operationalised
	diagnostic criteria for CFS, will be recruited from secondary care
	into a randomised trial of four treatments, stratified by current
	comorbid depressive episode and different CFS/ME criteria. The four
	treatments are standardised specialist medical care either given
	alone, or with adaptive pacing therapy or cognitive behaviour therapy
	or graded exercise therapy. Supplementary therapies will involve
	fourteen sessions over 23 weeks and a 'booster session' at 36 weeks.
	Outcome will be assessed at 12, 24, and 52 weeks after randomisation.
	Two primary outcomes of self-rated fatigue and physical function
	will assess differential effects of each treatment on these measures.
	Secondary outcomes include adverse events and reactions, subjective
	measures of symptoms, mood, sleep and function and objective measures
	of physical activity, fitness, cost-effectiveness and cost-utility.
	The primary analysis will be based on intention to treat and will
	use logistic regression models to compare treatments. Secondary outcomes
	will be analysed by repeated measures analysis of variance with a
	linear mixed model. All analyses will allow for stratification factors.
	Mediators and moderators will be explored using multiple linear and
	logistic regression techniques with interactive terms, with the sample
	split into two to allow validation of the initial models. Economic
	analyses will incorporate sensitivity measures.DISCUSSION:The results
	of the trial will provide information about the benefits and adverse
	effects of these treatments, their cost-effectiveness and cost-utility,
	the process of clinical improvement and the predictors of efficacy.},
  groups    = {PACE},
  journal   = {BMC Neurology},
  owner     = {meddwilb},
  pubmedid  = {17397525},
  timestamp = {2015.01.23},
}

@Article{Whitehead2015,
  author    = {Whitehead, Amy L and Julious, Steven A and Cooper, Cindy L and Campbell, Michael J},
  title     = {Estimating the sample size for a pilot randomised trial to minimise the overall trial sample size for the external pilot and main trial for a continuous outcome variable},
  year      = {2015},
  doi       = {10.1177/0962280215588241},
  eprint    = {http://smm.sagepub.com/content/early/2015/06/27/0962280215588241.full.pdf+html},
  url       = {http://smm.sagepub.com/content/early/2015/06/27/0962280215588241.abstract},
  abstract  = {Sample size justification is an important consideration when planning
	a clinical trial, not only for the main trial but also for any preliminary
	pilot trial. When the outcome is a continuous variable, the sample
	size calculation requires an accurate estimate of the standard deviation
	of the outcome measure. A pilot trial can be used to get an estimate
	of the standard deviation, which could then be used to anticipate
	what may be observed in the main trial. However, an important consideration
	is that pilot trials often estimate the standard deviation parameter
	imprecisely. This paper looks at how we can choose an external pilot
	trial sample size in order to minimise the sample size of the overall
	clinical trial programme, that is, the pilot and the main trial together.
	We produce a method of calculating the optimal solution to the required
	pilot trial sample size when the standardised effect size for the
	main trial is known. However, as it may not be possible to know the
	standardised effect size to be used prior to the pilot trial, approximate
	rules are also presented. For a main trial designed with 90% power
	and two-sided 5% significance, we recommend pilot trial sample sizes
	per treatment arm of 75, 25, 15 and 10 for standardised effect sizes
	that are extra small (â‰¤0.1), small (0.2), medium (0.5) or large
	(0.8), respectively.},
  groups    = {Pilot/feasibility},
  journal   = {Statistical Methods in Medical Research},
  owner     = {meddwilb},
  timestamp = {2015.11.19},
}

@BOOK{Wickham2009,
  title = {ggplot2: elegant graphics for data analysis},
  publisher = {Springer New York},
  year = {2009},
  author = {Hadley Wickham},
  owner = {meddwilb},
  timestamp = {2014.03.05},
  url = {http://had.co.nz/ggplot2/book}
}

@ARTICLE{Wilks1938,
  author = {Wilks, S. S.},
  title = {The Large-Sample Distribution of the Likelihood Ratio for Testing
	Composite Hypotheses},
  journal = {The Annals of Mathematical Statistics},
  year = {1938},
  volume = {9},
  pages = {pp. 60-62},
  number = {1},
  copyright = {Copyright © 1938 Institute of Mathematical Statistics},
  issn = {00034851},
  jstor_articletype = {research-article},
  jstor_formatteddate = {Mar., 1938},
  language = {English},
  owner = {meddwilb},
  publisher = {Institute of Mathematical Statistics},
  timestamp = {2014.04.02},
  url = {http://www.jstor.org/stable/2957648}
}

@Article{Willan2005,
  author    = {Willan, Andrew R. and Pinto, Eleanor M.},
  title     = {The value of information and optimal clinical trial design},
  year      = {2005},
  volume    = {24},
  number    = {12},
  pages     = {1791--1806},
  issn      = {1097-0258},
  doi       = {10.1002/sim.2069},
  url       = {http://dx.doi.org/10.1002/sim.2069},
  abstract  = {Traditional sample size calculations for randomized clinical trials depend on somewhat arbitrarily chosen factors, such as type I and II errors. Type I error, the probability of rejecting the null hypothesis of no difference when it is true, is most often set to 0.05, regardless of the cost of such an error. In addition, the traditional use of 0.2 for the type II error means that the money and effort spent on the trial will be wasted 20 per cent of the time, even when the true treatment difference is equal to the smallest clinically important one and, again, will not reflect the cost of making such an error. An effectiveness trial (otherwise known as a pragmatic trial or management trial) is essentially an effort to inform decision-making, i.e. should treatment be adopted over standard? As such, a decision theoretic approach will lead to an optimal sample size determination. Using incremental net benefit and the theory of the expected value of information, and taking a societal perspective, it is shown how to determine the sample size that maximizes the difference between the cost of doing the trial and the value of the information gained from the results. The methods are illustrated using examples from oncology and obstetrics. Copyright Â© 2005 John Wiley & Sons, Ltd.},
  groups    = {Bayesian, Decision theory},
  journal   = {Statistics in Medicine},
  keywords  = {randomized clinical trials, sample size, value of information, incremental net benefit, bayesian methods},
  owner     = {meddwilb},
  publisher = {John Wiley \& Sons, Ltd.},
  timestamp = {2016.07.06},
}

@Article{Wittes1990,
  author    = {Wittes, Janet and Brittain, Erica},
  title     = {The role of internal pilot studies in increasing the efficiency of clinical trials},
  year      = {1990},
  volume    = {9},
  number    = {1-2},
  pages     = {65--72},
  issn      = {1097-0258},
  doi       = {10.1002/sim.4780090113},
  url       = {http://dx.doi.org/10.1002/sim.4780090113},
  abstract  = {Investigators often design clinical trials without knowing precisely
	the values of such necessary parameters as the variances or the event
	rates in the control group. In order to determine reasonable values
	for such parameters, they may design a small pilot study external
	to the main trial. In this paper we propose designs, which we term
	internal pilot studies, that designate a portion of the main trial
	as a pilot phase. At the end of the internal pilot study, the investigators
	recompute preselected parameters and recalculate required sample
	size. The study then proceeds with the modifications dictated by
	the internal pilot. Final analyses of the results incorporate all
	data, disregarding the fact that part of the data came from a pilot
	phase. As one example of this type of design, we consider a study
	to compare two normally distributed means. By simulation, we show
	a numerical example for which the effect of the procedure on the
	Î±-level is negligible, but the potential gain in power considerable.
	We urge considering a similar approach for a number of types of endpoints.},
  groups    = {SSR},
  journal   = {Statistics in Medicine},
  owner     = {meddwilb},
  publisher = {Wiley Subscription Services, Inc., A Wiley Company},
  timestamp = {2014.08.11},
}

@ARTICLE{Wolfe2010,
  author = {Wolfe, Charles David Alexander and Redfern, Judith and Rudd, Anthony
	George and Grieve, Andrew Peter and Heuschmann, Peter Ulrich and
	McKevitt, Christopher},
  title = {Cluster Randomized Controlled Trial of a Patient and General Practitioner
	Intervention to Improve the Management of Multiple Risk Factors After
	Stroke: Stop Stroke},
  journal = {Stroke},
  year = {2010},
  volume = {41},
  pages = {2470-2476},
  number = {11},
  abstract = {Background and Purposeâ€”Stroke is a major public health concern worldwide
	and survivors remain at high risk of recurrence. Secondary prevention
	requires management of multiple risk factors but current management
	is suboptimal. Evidence of the effectiveness of interventions to
	improve poststroke risk factor management from well-designed trials
	is limited. We assessed the effectiveness of a patient and general
	practitioner systematic follow-up intervention to improve risk factor
	management after stroke.Methodsâ€”We undertook a pragmatic cluster
	trial involving 523 consecutive incident stroke survivors identified
	using the population South London Stroke Register and registered
	with general practices in inner-city London. Practices were randomized
	to receive the intervention or usual care. The intervention entailed
	systematically identifying stroke survivorsâ€™ risk factors for recurrence
	and providing tailored evidence-based management advice to general
	practitioners, patients, and caregivers at 10 weeks, 5 months, and
	8 months poststroke. The primary outcome was management of key modifiable
	risk factors for stroke at 1 year with 3 end points: treatment with
	antihypertensive therapy, treatment with antiplatelet therapy, and
	smoking cessation. Hierarchical testing was used to adjust for multiple
	endpoints. Analysis was by intention to treat. This study is registered
	as number ISRCTN10730637.Resultsâ€”The absolute risk reduction (and
	95% CI) for each outcome was âˆ’3.7% (âˆ’13.0% to 5.6%) for treatment
	with antihypertensives; âˆ’2.3% (âˆ’12.0% to 7.6%) for treatment
	with antiplatelets; and âˆ’0.6% (âˆ’14.5% to 13.5%) for smoking cessation.
	Treatment effects were confirmed in the generalized linear model
	adjusting for clustering and predefined confounders.Conclusionsâ€”No
	improvement in risk factor management was demonstrated as a result
	of this patient, caregiver, and healthcare professional systematic
	follow-up system. Further evidence of how to effectively alter behavior
	of patients/caregivers and professionals is required if tailored
	information on risk and its treatment is to be of any clinical benefit.},
  doi = {10.1161/STROKEAHA.110.588046},
  eprint = {http://stroke.ahajournals.org/content/41/11/2470.full.pdf+html},
  owner = {meddwilb},
  timestamp = {2013.09.17},
  url = {http://stroke.ahajournals.org/content/41/11/2470.abstract}
}

@ARTICLE{Wolpert1997,
  author = {Wolpert, D.H. and Macready, W.G.},
  title = {No free lunch theorems for optimization},
  journal = {Evolutionary Computation, IEEE Transactions on},
  year = {1997},
  volume = {1},
  pages = {67-82},
  number = {1},
  month = {Apr},
  abstract = {A framework is developed to explore the connection between effective
	optimization algorithms and the problems they are solving. A number
	of “no free lunch” (NFL) theorems are presented which establish that
	for any algorithm, any elevated performance over one class of problems
	is offset by performance over another class. These theorems result
	in a geometric interpretation of what it means for an algorithm to
	be well suited to an optimization problem. Applications of the NFL
	theorems to information-theoretic aspects of optimization and benchmark
	measures of performance are also presented. Other issues addressed
	include time-varying optimization problems and a priori “head-to-head”
	minimax distinctions between optimization algorithms, distinctions
	that result despite the NFL theorems' enforcing of a type of uniformity
	over all algorithms},
  doi = {10.1109/4235.585893},
  issn = {1089-778X},
  keywords = {combinatorial mathematics;genetic algorithms;information theory;search
	problems;a priori head-to-head minimax distinctions;elevated performance;geometric
	interpretation;information-theoretic aspects;no free lunch theorems;optimization;time-varying
	optimization;Algorithm design and analysis;Bayesian methods;Evolutionary
	computation;Helium;Information theory;Iron;Minimax techniques;Performance
	analysis;Probability distribution;Simulated annealing},
  owner = {meddwilb},
  timestamp = {2014.08.11}
}

@ARTICLE{Yap2013,
  author = {Yap, Christina and Pettitt, Andrew and Billingham, Lucinda},
  title = {Screened selection design for randomised phase {II} oncology trials:
	an example in chronic lymphocytic leukaemia},
  journal = {BMC Medical Research Methodology},
  year = {2013},
  volume = {13},
  pages = {87},
  number = {1},
  abstract = {BACKGROUND:As there are limited patients for chronic lymphocytic leukaemia
	trials, it is important that statistical methodologies in Phase II
	efficiently select regimens for subsequent evaluation in larger-scale
	Phase III trials.METHODS:We propose the screened selection design
	(SSD), which is a practical multi-stage, randomised Phase II design
	for two experimental arms. Activity is first evaluated by applying
	Simon's two-stage design (1989) on each arm. If both are active,
	the play-the-winner selection strategy proposed by Simon, Wittes
	and Ellenberg (SWE) (1985) is applied to select the superior arm.
	A variant of the design, Modified SSD, also allows the arm with the
	higher response rates to be recommended only if its activity rate
	is greater by a clinically-relevant value. The operating characteristics
	are explored via a simulation study and compared to a Bayesian Selection
	approach.RESULTS:Simulations showed that with the proposed SSD, it
	is possible to retain the sample size as required in SWE and obtain
	similar probabilities of selecting the correct superior arm of at
	least 90%; with the additional attractive benefit of reducing the
	probability of selecting ineffective arms. This approach is comparable
	to a Bayesian Selection Strategy. The Modified SSD performs substantially
	better than the other designs in selecting neither arm if the underlying
	rates for both arms are desirable but equivalent, allowing for other
	factors to be considered in the decision making process. Though its
	probability of correctly selecting a superior arm might be reduced,
	it still performs reasonably well. It also reduces the probability
	of selecting an inferior arm.CONCLUSIONS:SSD provides an easy to
	implement randomised Phase II design that selects the most promising
	treatment that has shown sufficient evidence of activity, with available
	R codes to evaluate its operating characteristics.},
  doi = {10.1186/1471-2288-13-87},
  issn = {1471-2288},
  owner = {meddwilb},
  pubmedid = {23819695},
  timestamp = {2013.12.10},
  url = {http://www.biomedcentral.com/1471-2288/13/87}
}

@ARTICLE{Yekutieli1999,
  author = {Daniel Yekutieli and Yoav Benjamini},
  title = {Resampling-based false discovery rate controlling multiple test procedures
	for correlated test statistics },
  journal = {Journal of Statistical Planning and Inference },
  year = {1999},
  volume = {82},
  pages = {171 - 196},
  number = {1â€“2},
  abstract = {A new false discovery rate controlling procedure is proposed for multiple
	hypotheses testing. The procedure makes use of resampling-based p-value
	adjustment, and is designed to cope with correlated test statistics.
	Some properties of the proposed procedure are investigated theoretically,
	and further properties are investigated using a simulation study.
	According to the results of the simulation study, the new procedure
	offers false discovery rate control and greater power. The motivation
	for developing this resampling-based procedure was an actual problem
	in meteorology, in which almost 2000 hypotheses are tested simultaneously
	using highly correlated test statistics. When applied to this problem
	the increase in power was evident. The same procedure can be used
	in many other large problems of multiple testing, for example multiple
	endpoints. The procedure is also extended to serve as a general diagnostic
	tool in model selection. },
  doi = {http://dx.doi.org/10.1016/S0378-3758(99)00041-5},
  issn = {0378-3758},
  keywords = {Model selection},
  owner = {meddwilb},
  timestamp = {2014.08.11},
  url = {http://www.sciencedirect.com/science/article/pii/S0378375899000415}
}

@Article{YoavBenjamini1995,
  author    = {Yoav Benjamini, Yosef Hochberg},
  title     = {Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing},
  year      = {1995},
  volume    = {57},
  number    = {1},
  pages     = {289-300},
  issn      = {00359246},
  url       = {http://www.jstor.org/stable/2346101},
  abstract  = {The common approach to the multiplicity problem calls for controlling the familywise error rate (FWER). This approach, though, has faults, and we point out a few. A different approach to problems of multiple significance testing is presented. It calls for controlling the expected proportion of falsely rejected hypotheses-the false discovery rate. This error rate is equivalent to the FWER when all hypotheses are true but is smaller otherwise. Therefore, in problems where the control of the false discovery rate rather than that of the FWER is desired, there is potential for a gain in power. A simple sequential Bonferroni-type procedure is proved to control the false discovery rate for independent test statistics, and a simulation study shows that the gain in power is substantial. The use of the new procedure and the appropriateness of the criterion are illustrated with examples.},
  groups    = {Multiple endpoints},
  journal   = {Journal of the Royal Statistical Society. Series B (Methodological)},
  owner     = {meddwilb},
  publisher = {[Royal Statistical Society, Wiley]},
  timestamp = {2016.07.15},
}

@Article{Yu2009,
  author    = {Jihnhee Yu and James L. Kepner and Renuka Iyer},
  title     = {Exact Tests using Two Correlated Binomial Variables in Contemporary Cancer Clinical Trials},
  journal   = {Biom. J.},
  year      = {2009},
  month     = {dec},
  pages     = {n/a--n/a},
  doi       = {10.1002/bimj.200900082},
  url       = {http://dx.doi.org/10.1002/bimj.200900082},
  comment   = {Generally nothing beyond the other methods, appears to mainly contribute an efficient method for calculating the exact probabilities of a bivariate binomial test statistic.},
  groups    = {Multiple endpoints},
  owner     = {meddwilb},
  publisher = {Wiley-Blackwell},
  timestamp = {2016.07.25},
}

@Article{ZAMPINO2006a,
  author    = {ZAMPINO, M.G. and LORIZZO, K. and ROCCA, A. and LOCATELLI, M. and ZORZINO, L. and MANZONI, S. and MAZZETTA, C. and FAZIO, N. and BIFFI, R. and DE BRAUD, F.},
  title     = {Oxaliplatin Combined with 5-Fluorouracil and Methotrexate in Advanced Colorectal Cancer},
  journal   = {Anticancer Research},
  year      = {2006},
  volume    = {26},
  number    = {3B},
  pages     = {2425-2428},
  eprint    = {http://ar.iiarjournals.org/content/26/3B/2425.full.pdf+html},
  url       = {http://ar.iiarjournals.org/content/26/3B/2425.abstract},
  abstract  = {Background: A promising regimen including 5-Fluorouracil, methotrexate
	and oxaliplatin is reported. Patients and Methods: Patients with
	untreated measurable metastatic disease received bolus 5-Fluorouracil
	(600 mg/m2) on days 2 and 16, modulated by methotrexate (200 mg/m2)
	24 h earlier, alternated with 4 weeks of continuous infusion of 5-Fluorouracil
	(200 mg/m2/daily) plus oxaliplatin (130 mg/m2) on days 29 and 56,
	followed by 2 weeks of rest. Serum vascular endothelial growth factor
	(VEGF) was analyzed at baseline and before every cycle. Results:
	Fifty-eight patients were enrolled. Objective remissions were reported
	in 45.6% (95% CI=34.3%, 57.3%). The median progression-free survival
	was 7.8 months and the median overall survival was 19.4 months. No
	grade 4 toxicity was reported, except for one case of diarrhea. The
	serum VEGF evaluated in 23 patients showed a decreasing trend during
	therapy. Conclusion: The regimen was active, well tolerated and may
	be a possible option in patients not suitable for radical surgery.},
  comment   = {SSS and paper gives
	
	
	[1] 0.3 0.5 0.6 0.8
	
	[1] 0.1 0.1 0.1
	
	
	[1] 8 15 23 22 39 57
	
	[1] "Expected size: 10518.6626929021"
	
	[1] "alpha_r: 0.0419576150404191"
	
	[1] "alpha_t: 0.0473424961922745"
	
	[1] "power: 0.795115711389482"
	
	
	local search gives
	
	
	[1] 7 15 25 19 35 51
	
	[1] "Expected size: 37.4719628455787"
	
	[1] "alpha_r: 0.0910765328930157"
	
	[1] "alpha_t: 0.0700278107027275"
	
	[1] "power: 0.900221360375122"},
  owner     = {meddwilb},
  timestamp = {2014.03.03},
}

@Misc{MRC2002,
  author = {{Medical Research Council}},
  title  = {Cluster randomsied trials: methodological and ethical considerations},
  year   = {2002},
  file   = {:U\:\\Literature\\Papers\\Cluster-randomised-trials-Methodological-and-ethical-considerations.pdf:PDF},
}

@Article{Rutterford2015,
  author    = {Clare Rutterford and Andrew Copas and Sandra Eldridge},
  title     = {Methods for sample size determination in cluster randomized trials},
  year      = {2015},
  volume    = {44},
  number    = {3},
  month     = {jun},
  pages     = {1051--1067},
  doi       = {10.1093/ije/dyv113},
  url       = {http://dx.doi.org/10.1093/ije/dyv113},
  groups    = {Complex Interventions},
  journal   = {International Journal of Epidemiology},
  publisher = {Oxford University Press ({OUP})},
}

@Article{Campbell2000a,
  author   = {Campbell, Marion and Grimshaw, Jeremy and Steen, Nick and Changing Professional Practice in Europe Group (EU BIOMED II Concerted Action)},
  title    = {Sample Size Calculations for Cluster Randomised Trials},
  year     = {2000},
  volume   = {5},
  number   = {1},
  pages    = {12-16},
  doi      = {10.1177/135581960000500105},
  eprint   = {http://hsr.sagepub.com/content/5/1/12.full.pdf+html},
  url      = {http://hsr.sagepub.com/content/5/1/12.abstract},
  abstract = {Objectives: Cluster randomised trials, in which groups of individuals are randomised, are increasingly being used in the health field. Adopting a clustered approach has implications for the design of such trials, and sample size calculations need to be inflated to accommodate for the clustering effect. Reliable estimates of intracluster correlation coefficients (ICCs) are required for robust sample size calculations to be made; however, little empirical evidence is available on their likely size, and on factors which influence their magnitude. The aim of this study was to generate empirical estimates of ICCs and to explore factors which may affect their magnitude.Methods: Empirical estimates of ICCs were calculated for both process variables and patient outcomes from a number of datasets of primary and secondary care implementation studies.Results: Estimates of ICCs varied according to setting and type of outcome. Estimates of ICCs for process variables were higher than those for patient outcomes, and estimates derived from secondary care were higher than those from primary care. ICCs for process variables in primary care were of the order of 0.05?0.15, whilst those in secondary care were of the order of 0.3. Estimates for patient outcomes in primary care were generally lower than 0.05.Conclusions: Adopting cluster randomisation has implications for the design, size and analysis of clinical trials. This study gives an insight into the potential size of ICCs in primary and secondary care, and provides a practical guide to researchers to aid the planning of future studies in this area.},
  journal  = {Journal of Health Services Research \& Policy},
}

@Article{Taljaard2016,
  author    = {M. Taljaard and S. Teerenstra and N. M. Ivers and D. A. Fergusson},
  title     = {Substantial risks associated with few clusters in cluster randomized and stepped wedge designs},
  year      = {2016},
  volume    = {13},
  number    = {4},
  month     = {mar},
  pages     = {459--463},
  doi       = {10.1177/1740774516634316},
  url       = {http://dx.doi.org/10.1177/1740774516634316},
  groups    = {Complex Interventions},
  journal   = {Clinical Trials},
  publisher = {{SAGE} Publications},
}

@Article{Lindley1957,
  author    = {D. V. Lindley},
  title     = {A Statistical Paradox},
  year      = {1957},
  volume    = {44},
  number    = {1/2},
  month     = {jun},
  pages     = {187},
  doi       = {10.2307/2333251},
  url       = {http://dx.doi.org/10.2307/2333251},
  groups    = {Bayesian},
  journal   = {Biometrika},
  publisher = {{JSTOR}},
}

@InProceedings{Nakayama2014,
  author    = {Marvin K. Nakayama},
  title     = {Quantile estimation when applying conditional Monte Carlo},
  booktitle = {International Conference on Simulation and Modeling Methodologies, Technologies and Applications (SIMULTECH)},
  year      = {2014},
  pages     = {280-285},
  doi       = {doi.ieeecomputersociety.org/10.5220/0005109702800285},
  url       = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7095032},
  abstract  = {We describe how to use conditional Monte Carlo (CMC) to estimate a quantile. CMC is a variance-reduction technique that reduces variance by analytically integrating out some of the variability. We show that the CMC quantile estimator satisfies a central limit theorem and Bahadur representation. We also develop three asymptotically valid confidence intervals (CIs) for a quantile. One CI is based on a finite-difference estimator, another uses batching, and the third applies sectioning. We present numerical results demonstrating the effectiveness of CMC.},
  groups    = {Optimisation},
}

@Article{Lindley2000,
  author    = {Dennis V. Lindley},
  title     = {The Philosophy of Statistics},
  year      = {2000},
  volume    = {49},
  number    = {3},
  month     = {sep},
  pages     = {293--337},
  doi       = {10.1111/1467-9884.00238},
  url       = {http://dx.doi.org/10.1111/1467-9884.00238},
  journal   = {Journal of the Royal Statistical Society: Series D (The Statistician)},
  publisher = {Wiley-Blackwell},
}

@Article{Gould2001,
  author    = {A. Lawrence Gould},
  title     = {Sample size re-estimation: recent developments and practical considerations},
  year      = {2001},
  volume    = {20},
  number    = {17-18},
  pages     = {2625--2643},
  doi       = {10.1002/sim.733},
  url       = {http://dx.doi.org/10.1002/sim.733},
  groups    = {SSR},
  journal   = {Statist. Med.},
  publisher = {Wiley-Blackwell},
}

@Article{Freeman2015,
  author    = {Daniel Freeman and Felicity Waite and Helen Startup and Elissa Myers and Rachel Lister and Josephine McInerney and Allison G Harvey and John Geddes and Zenobia Zaiwalla and Ramon Luengo-Fernandez and Russell Foster and Lei Clifton and Ly-Mee Yu},
  title     = {Efficacy of cognitive behavioural therapy for sleep improvement in patients with persistent delusions and hallucinations ({BEST}): a prospective, assessor-blind, randomised controlled pilot trial},
  year      = {2015},
  volume    = {2},
  number    = {11},
  month     = {nov},
  pages     = {975--983},
  doi       = {10.1016/s2215-0366(15)00314-4},
  url       = {http://dx.doi.org/10.1016/S2215-0366(15)00314-4},
  groups    = {Examples},
  journal   = {The Lancet Psychiatry},
  publisher = {Elsevier {BV}},
}

@Article{Nixon2009,
  author    = {Richard M. Nixon and Anthony O{\textquotesingle}Hagan and Jeremy Oakley and Jason Madan and John W. Stevens and Nick Bansback and Alan Brennan},
  title     = {The Rheumatoid Arthritis Drug Development Model: a case study in Bayesian clinical trial simulation},
  year      = {2009},
  pages     = {n/a--n/a},
  doi       = {10.1002/pst.368},
  url       = {http://dx.doi.org/10.1002/pst.368},
  groups    = {Examples},
  journal   = {Pharmaceut. Statist.},
  publisher = {Wiley-Blackwell},
}

@Article{Wilson2015,
  author    = {D. T. Wilson and R. E. Walwyn and J. Brown and A. J. Farrin and S. R. Brown},
  title     = {Statistical challenges in assessing potential efficacy of complex interventions in pilot or feasibility studies},
  year      = {2015},
  volume    = {25},
  number    = {3},
  month     = {jun},
  pages     = {997--1009},
  doi       = {10.1177/0962280215589507},
  url       = {http://dx.doi.org/10.1177/0962280215589507},
  journal   = {Statistical Methods in Medical Research},
  publisher = {{SAGE} Publications},
}

@Article{Gould1992,
  author    = {A. Lawrence Gould},
  title     = {Interim analyses for monitoring clinical trials that do not materially affect the type I error rate},
  year      = {1992},
  volume    = {11},
  number    = {1},
  pages     = {55--66},
  doi       = {10.1002/sim.4780110107},
  url       = {http://dx.doi.org/10.1002/sim.4780110107},
  groups    = {SSR},
  journal   = {Statist. Med.},
  publisher = {Wiley-Blackwell},
}

@Article{Gould1997,
  author    = {A. Lawrence Gould},
  title     = {Issues in blinded sample size re-estimation},
  year      = {1997},
  volume    = {26},
  number    = {3},
  month     = {jan},
  pages     = {1229--1239},
  doi       = {10.1080/03610919708813436},
  url       = {http://dx.doi.org/10.1080/03610919708813436},
  groups    = {SSR},
  journal   = {Communications in Statistics - Simulation and Computation},
  publisher = {Informa {UK} Limited},
}

@Article{Berger2000,
  author    = {Berger, Vance W.},
  title     = {Pros and cons of permutation tests in clinical trials},
  year      = {2000},
  volume    = {19},
  number    = {10},
  pages     = {1319--1328},
  issn      = {1097-0258},
  doi       = {10.1002/(SICI)1097-0258(20000530)19:10<1319::AID-SIM490>3.0.CO;2-0},
  url       = {http://dx.doi.org/10.1002/(SICI)1097-0258(20000530)19:10<1319::AID-SIM490>3.0.CO;2-0},
  journal   = {Statistics in Medicine},
  publisher = {John Wiley \& Sons, Ltd.},
}

@Article{Schulz2005,
  author    = {Kenneth F Schulz and David A Grimes},
  title     = {Sample size calculations in randomised trials: mandatory and mystical},
  journal   = {The Lancet},
  year      = {2005},
  volume    = {365},
  number    = {9467},
  month     = {apr},
  pages     = {1348--1353},
  doi       = {10.1016/s0140-6736(05)61034-3},
  url       = {http://dx.doi.org/10.1016/S0140-6736(05)61034-3},
  comment   = {"An operational solution to a real problem"},
  groups    = {SSR},
  publisher = {Elsevier {BV}},
}

@Manual{Binois2016,
  author  = {Mickael Binois and Victor Picheny},
  title   = {GPareto: Gaussian Processes for Pareto Front Estimation and Optimization},
  year    = {2016},
  note    = {R package version 1.0.2},
  url     = {https://CRAN.R-project.org/package=GPareto},
  comment = {https://cran.r-project.org/web/packages/GPareto/vignettes/GPareto_vignette.pdf},
}

@Article{Ivers2011,
  author    = {N. M. Ivers and M. Taljaard and S. Dixon and C. Bennett and A. McRae and J. Taleban and Z. Skea and J. C. Brehaut and R. F. Boruch and M. P. Eccles and J. M. Grimshaw and C. Weijer and M. Zwarenstein and A. Donner},
  title     = {Impact of {CONSORT} extension for cluster randomised trials on quality of reporting and study methodology: review of random sample of 300 trials, 2000-8},
  year      = {2011},
  volume    = {343},
  number    = {sep26 1},
  month     = {sep},
  pages     = {d5886--d5886},
  doi       = {10.1136/bmj.d5886},
  url       = {http://dx.doi.org/10.1136/bmj.d5886},
  groups    = {Complex Interventions, Multilevel},
  journal   = {{BMJ}},
  publisher = {{BMJ}},
}

@Article{Hayes1999,
  author    = {R. J. Hayes and S. Bennett},
  title     = {Simple sample size calculation for cluster-randomized trials},
  year      = {1999},
  volume    = {28},
  number    = {2},
  month     = {apr},
  pages     = {319--326},
  doi       = {10.1093/ije/28.2.319},
  url       = {http://dx.doi.org/10.1093/ije/28.2.319},
  groups    = {Complex Interventions, Multilevel},
  journal   = {International Journal of Epidemiology},
  publisher = {Oxford University Press ({OUP})},
}

@Article{Wittes1999,
  author    = {Janet Wittes and Oliver Schabenberger and David Zucker and Erica Brittain and Michael Proschan},
  title     = {Internal pilot studies I: type I error rate of the naive t-test},
  year      = {1999},
  volume    = {18},
  number    = {24},
  month     = {dec},
  pages     = {3481--3491},
  doi       = {10.1002/(sici)1097-0258(19991230)18:24<3481::aid-sim301>3.0.co;2-c},
  url       = {http://dx.doi.org/10.1002/(SICI)1097-0258(19991230)18:24<3481::AID-SIM301>3.0.CO;2-C},
  groups    = {SSR},
  journal   = {Statistics in Medicine},
  publisher = {Wiley-Blackwell},
}

@Article{Proschan1995,
  author    = {Michael A. Proschan and Sally A. Hunsberger},
  title     = {Designed Extension of Studies Based on Conditional Power},
  year      = {1995},
  volume    = {51},
  number    = {4},
  month     = {dec},
  pages     = {1315},
  doi       = {10.2307/2533262},
  url       = {http://dx.doi.org/10.2307/2533262},
  groups    = {SSR},
  journal   = {Biometrics},
  publisher = {{JSTOR}},
}

@Article{Herson1986,
  author    = {Jay Herson and Stephen K. Carter},
  title     = {Calibrated phase {II} clinical trials in oncology},
  year      = {1986},
  volume    = {5},
  number    = {5},
  month     = {sep},
  pages     = {441--447},
  doi       = {10.1002/sim.4780050508},
  url       = {http://dx.doi.org/10.1002/sim.4780050508},
  groups    = {SSR, Phase II},
  journal   = {Statist. Med.},
  publisher = {Wiley-Blackwell},
}

@Article{Walley2015,
  author    = {Rosalind J. Walley and Claire L. Smith and Jeremy D. Gale and Phil Woodward},
  title     = {Advantages of a wholly Bayesian approach to assessing efficacy in early drug development: a case study},
  year      = {2015},
  volume    = {14},
  number    = {3},
  month     = {apr},
  pages     = {205--215},
  doi       = {10.1002/pst.1675},
  url       = {http://dx.doi.org/10.1002/pst.1675},
  groups    = {Bayesian},
  journal   = {Pharmaceutical Statistics},
  publisher = {Wiley-Blackwell},
}

@Article{Carroll2013,
  author    = {Kevin J. Carroll},
  title     = {Decision Making from Phase {II} to Phase {III} and the Probability of Success: Reassured by {\textquotedblleft}Assurance{\textquotedblright}?},
  year      = {2013},
  volume    = {23},
  number    = {5},
  month     = {sep},
  pages     = {1188--1200},
  doi       = {10.1080/10543406.2013.813527},
  url       = {http://dx.doi.org/10.1080/10543406.2013.813527},
  groups    = {Bayesian},
  journal   = {Journal of Biopharmaceutical Statistics},
  publisher = {Informa {UK} Limited},
}

@Article{Dallow2010,
  author    = {Nigel Dallow and Paolo Fina},
  title     = {The perils with the misuse of predictive power},
  year      = {2010},
  volume    = {10},
  number    = {4},
  month     = {oct},
  pages     = {311--317},
  doi       = {10.1002/pst.467},
  url       = {http://dx.doi.org/10.1002/pst.467},
  groups    = {Bayesian},
  journal   = {Pharmaceut. Statist.},
  publisher = {Wiley-Blackwell},
}

@Article{Kirchner2015,
  author    = {Marietta Kirchner and Meinhard Kieser and Heiko Götte and Armin Schüler},
  title     = {Utility-based optimization of phase {II}/{III} programs},
  year      = {2015},
  volume    = {35},
  number    = {2},
  month     = {aug},
  pages     = {305--316},
  doi       = {10.1002/sim.6624},
  url       = {http://dx.doi.org/10.1002/sim.6624},
  groups    = {Bayesian, Decision theory},
  journal   = {Statist. Med.},
  publisher = {Wiley-Blackwell},
}

@Article{Shieh2016,
  author    = {Gwowen Shieh},
  title     = {The equivalence of two approaches to incorporating variance uncertainty in sample size calculations for linear statistical models},
  year      = {2016},
  month     = {may},
  pages     = {1--17},
  doi       = {10.1080/02664763.2016.1158797},
  url       = {http://dx.doi.org/10.1080/02664763.2016.1158797},
  groups    = {Bayesian, Pilot/feasibility},
  journal   = {Journal of Applied Statistics},
  publisher = {Informa {UK} Limited},
}

@Article{Chen1998,
  author    = {T. Timothy Chen and Tie-Hua Ng},
  title     = {Optimal flexible designs in phase {II} clinical trials},
  journal   = {Statist. Med.},
  year      = {1998},
  volume    = {17},
  number    = {20},
  month     = {oct},
  pages     = {2301--2312},
  doi       = {10.1002/(sici)1097-0258(19981030)17:20<2301::aid-sim927>3.0.co;2-x},
  url       = {http://dx.doi.org/10.1002/(SICI)1097-0258(19981030)17:20<2301::AID-SIM927>3.0.CO;2-X},
  comment   = {Only considers response. Focusses on extending Simon's design to the case where the actual sample size at each stage will have some randomness rather than being fixed.},
  groups    = {Multiple endpoints, Phase II},
  publisher = {Wiley-Blackwell},
}

@Article{Ratain2009,
  author    = {Mark J. Ratain and Daniel J. Sargent},
  title     = {Optimising the design of phase {II} oncology trials: The importance of randomisation},
  year      = {2009},
  volume    = {45},
  number    = {2},
  month     = {jan},
  pages     = {275--280},
  doi       = {10.1016/j.ejca.2008.10.029},
  url       = {http://dx.doi.org/10.1016/j.ejca.2008.10.029},
  groups    = {Phase II},
  journal   = {European Journal of Cancer},
  publisher = {Elsevier {BV}},
}

@Article{Thall2002,
  author    = {Peter F Thall and Hsi-Guang Sung and Elihu H Estey},
  title     = {Selecting Therapeutic Strategies Based on Efficacy and Death in Multicourse Clinical Trials},
  year      = {2002},
  volume    = {97},
  number    = {457},
  month     = {mar},
  pages     = {29--39},
  doi       = {10.1198/016214502753479202},
  url       = {http://dx.doi.org/10.1198/016214502753479202},
  journal   = {Journal of the American Statistical Association},
  publisher = {Informa {UK} Limited},
}

@Article{Thall2003,
  author    = {Peter F. Thall and J. Kyle Wathen and B. Nebiyou Bekele and Richard E. Champlin and Laurence H. Baker and Robert S. Benjamin},
  title     = {Hierarchical Bayesian approaches to phase {II} trials in diseases with multiple subtypes},
  year      = {2003},
  volume    = {22},
  number    = {5},
  pages     = {763--780},
  doi       = {10.1002/sim.1399},
  url       = {http://dx.doi.org/10.1002/sim.1399},
  groups    = {Bayesian, Phase II},
  journal   = {Statist. Med.},
  publisher = {Wiley-Blackwell},
}

@Article{Cheung2002,
  author    = {Ying Kuen Cheung and Peter F. Thall},
  title     = {Monitoring the Rates of Composite Events with Censored Data in Phase {II} Clinical Trials},
  year      = {2002},
  volume    = {58},
  number    = {1},
  month     = {mar},
  pages     = {89--97},
  doi       = {10.1111/j.0006-341x.2002.00089.x},
  url       = {http://dx.doi.org/10.1111/j.0006-341X.2002.00089.x},
  groups    = {Phase II, Multiple endpoints},
  journal   = {Biometrics},
  publisher = {Wiley-Blackwell},
}

@Article{Panageas2002,
  author    = {Katherine S Panageas and Alex Smith and Mithat Gönen and Paul B Chapman},
  title     = {An optimal two-stage phase {II} design utilizing complete and partial response information separately},
  year      = {2002},
  volume    = {23},
  number    = {4},
  month     = {aug},
  pages     = {367--379},
  doi       = {10.1016/s0197-2456(02)00217-9},
  url       = {http://dx.doi.org/10.1016/S0197-2456(02)00217-9},
  groups    = {Phase II},
  journal   = {Controlled Clinical Trials},
  publisher = {Elsevier {BV}},
}

@Article{Jin2007,
  author    = {Hua Jin},
  title     = {Alternative designs of phase {II} trials considering response and toxicity},
  journal   = {Contemporary Clinical Trials},
  year      = {2007},
  volume    = {28},
  number    = {4},
  month     = {jul},
  pages     = {525--531},
  doi       = {10.1016/j.cct.2007.03.003},
  url       = {http://dx.doi.org/10.1016/j.cct.2007.03.003},
  comment   = {Just Bryant \& Day again - claims that the contribution is allowing preferences between toxicity and efficacy to be stated through setting different associated type I erro rates.},
  groups    = {Multiple endpoints, Phase II},
  publisher = {Elsevier {BV}},
}

@Article{Thall2005,
  author    = {Peter F. Thall and Leiko H. Wooten and Elizabeth J. Shpall},
  title     = {A Geometric Approach to Comparing Treatments for Rapidly Fatal Diseases},
  year      = {2005},
  volume    = {62},
  number    = {1},
  month     = {sep},
  pages     = {193--201},
  doi       = {10.1111/j.1541-0420.2005.00434.x},
  url       = {http://dx.doi.org/10.1111/j.1541-0420.2005.00434.x},
  journal   = {Biometrics},
  publisher = {Wiley-Blackwell},
}

@Article{Hong2007,
  author    = {Shengyan Hong and Yanping Wang},
  title     = {A three-outcome design for randomized comparative phase {II} clinical trials},
  year      = {2007},
  volume    = {26},
  number    = {19},
  pages     = {3525--3534},
  doi       = {10.1002/sim.2824},
  url       = {http://dx.doi.org/10.1002/sim.2824},
  groups    = {Phase II},
  journal   = {Statist. Med.},
  publisher = {Wiley-Blackwell},
}

@Article{Brutti2011,
  author    = {P. Brutti and S. Gubbiotti and V. Sambucini},
  title     = {An extension of the single threshold design for monitoring efficacy and safety in phase {II} clinical trials},
  year      = {2011},
  volume    = {30},
  number    = {14},
  month     = {apr},
  pages     = {1648--1664},
  doi       = {10.1002/sim.4229},
  url       = {http://dx.doi.org/10.1002/sim.4229},
  groups    = {Phase II, Multiple endpoints, Bayesian},
  journal   = {Statist. Med.},
  publisher = {Wiley-Blackwell},
}

@Article{Letierce2003,
  author    = {Alexia Letierce and Pascale Tubert-Bitter and Andrew Kramar and Jean Maccario},
  title     = {Two-treatment comparison based on joint toxicity and efficacy ordered alternatives in cancer trials},
  journal   = {Statist. Med.},
  year      = {2003},
  volume    = {22},
  number    = {6},
  pages     = {859--868},
  doi       = {10.1002/sim.1446},
  url       = {http://dx.doi.org/10.1002/sim.1446},
  comment   = {Considers toxicity and efficacy, and a situation where the notification of one outcome can come before the other.  Outcomes are composed to give a 5 outcome multinomial model.  Two notions of ordering treatments (based on these five probabilities) are given, and tests using these defined. Since we have a composite outcome, we now just have a single type I error rate and a single type II error rate.},
  groups    = {Multiple endpoints},
  publisher = {Wiley-Blackwell},
}

@Article{Wu2007,
  author    = {Chengqing Wu and Aiyi Liu},
  title     = {An adaptive approach for bivariate phase {II} clinical trial designs},
  journal   = {Contemporary Clinical Trials},
  year      = {2007},
  volume    = {28},
  number    = {4},
  month     = {jul},
  pages     = {482--486},
  doi       = {10.1016/j.cct.2007.01.005},
  url       = {http://dx.doi.org/10.1016/j.cct.2007.01.005},
  comment   = {Considers toxicity and efficacy and notes that designs will be locally optimal with respect to the correlation between the two endpoints (expressed here as an odds ratio), so proposes an adaptive extension to the~\cite{Conaway1995} design, where the odds ratio is re-estimated at the interim point. Note that~\cite{Tournoux2007} compared the~\cite{Conaway1995} and the~\cite{Bryant1995} designs and showed that~\cite{Conaway1995} is sensitive to this nuisance parameter but~\cite{Bryant1995} is not.  So this may not be a useful design.},
  groups    = {Multiple endpoints, Phase II},
  publisher = {Elsevier {BV}},
}

@Article{Chang2009,
  author    = {Myron Chang},
  title     = {Estimation of Multiple Response Rates in Phase {II} Clinical Trials with Missing Observations},
  journal   = {Journal of Biopharmaceutical Statistics},
  year      = {2009},
  volume    = {19},
  number    = {5},
  month     = {aug},
  pages     = {791--802},
  doi       = {10.1080/10543400903105182},
  url       = {http://dx.doi.org/10.1080/10543400903105182},
  comment   = {Focusses on analysis rather than design. Considers multiple correlated binary response endpoints (actually two, but says extension is simple). One response is of interest, and will be used in the hypothesis test, but will have missing data. Paper explores how data on the other endpoint(s) can be used to help estimate the primary response rate under an assumption of missing at random. Evaluates the asymptotic properties of their proposed maximum likelihood estimator and then the small sample properties through simulation. Find their estimator is more efficient than the conventional.},
  groups    = {Multiple endpoints, Phase II},
  publisher = {Informa {UK} Limited},
}

@Article{Ray2012,
  author    = {H.E. Ray and S.N. Rai},
  title     = {Operating characteristics of a Simon two-stage phase {II} clinical trial design incorporating continuous toxicity monitoring},
  journal   = {Pharmaceut. Statist.},
  year      = {2012},
  volume    = {11},
  number    = {2},
  month     = {jan},
  pages     = {170--176},
  doi       = {10.1002/pst.510},
  url       = {http://dx.doi.org/10.1002/pst.510},
  comment   = {Binary response and toxicity. Response is tested in a Simon type two-stage design, while toxicity in continuously monitored using a Pocock boundary. Focusses on comparing expected sample sizes, error rates etc. for two specific designs/scenarios, doesn't discuss choosing designs.},
  groups    = {Multiple endpoints, Phase II},
  publisher = {Wiley-Blackwell},
}

@Article{Chen2011,
  author    = {Chia-Min Chen and Yunchan Chi},
  title     = {Curtailed two-stage designs with two dependent binary endpoints},
  year      = {2011},
  volume    = {11},
  number    = {1},
  month     = {dec},
  pages     = {57--62},
  doi       = {10.1002/pst.496},
  url       = {http://dx.doi.org/10.1002/pst.496},
  groups    = {Multiple endpoints, Phase II},
  journal   = {Pharmaceut. Statist.},
  publisher = {Wiley-Blackwell},
}

@Article{Teramukai2015,
  author    = {Satoshi Teramukai and Takashi Daimon and Sarah Zohar},
  title     = {An extension of Bayesian predictive sample size selection designs for monitoring efficacy and safety},
  year      = {2015},
  volume    = {34},
  number    = {22},
  month     = {jun},
  pages     = {3029--3039},
  doi       = {10.1002/sim.6550},
  url       = {http://dx.doi.org/10.1002/sim.6550},
  groups    = {Multiple endpoints, Phase II, Bayesian},
  journal   = {Statistics in Medicine},
  publisher = {Wiley-Blackwell},
}

@Article{Kunz2015,
  author    = {C. U. Kunz and J. M. Wason and M. Kieser},
  title     = {Two-stage phase {II} oncology designs using short-term endpoints for early stopping},
  year      = {2015},
  month     = {jun},
  doi       = {10.1177/0962280215585819},
  url       = {http://dx.doi.org/10.1177/0962280215585819},
  groups    = {Phase II},
  journal   = {Statistical Methods in Medical Research},
  publisher = {{SAGE} Publications},
}

@Article{Cook2009,
  author       = {John D. Cook and Jairo A. Fúquene and Luis R. Pericchi},
  title        = {A Case for Robust Bayesian Priors with Applications to Clinical Trials},
  journaltitle = {Bayesian Analysis},
  year         = {2009},
  volume       = {4},
  pages        = {817--846},
  doi          = {10.1214/09-ba431},
  url          = {http://dx.doi.org/10.1214/09-BA431},
  groups       = {Bayesian},
  journal      = {Bayesian Analysis},
  publisher    = {Institute of Mathematical Statistics},
}

@Article{Wathen2008,
  author    = {J. Kyle Wathen and Peter F. Thall and John D. Cook and Elihu H. Estey},
  title     = {Accounting for patient heterogeneity in phase {II} clinical trials},
  year      = {2008},
  volume    = {27},
  number    = {15},
  pages     = {2802--2815},
  doi       = {10.1002/sim.3109},
  url       = {http://dx.doi.org/10.1002/sim.3109},
  groups    = {Phase II, Bayesian},
  journal   = {Statistics in Medicine},
  publisher = {Wiley-Blackwell},
}

@Article{Chi2008,
  author    = {Yunchan Chi and Chia-Min Chen},
  title     = {Curtailed two-stage designs in Phase {II} clinical trials},
  year      = {2008},
  volume    = {27},
  number    = {29},
  month     = {dec},
  pages     = {6175--6189},
  doi       = {10.1002/sim.3424},
  url       = {http://dx.doi.org/10.1002/sim.3424},
  groups    = {Phase II},
  journal   = {Statistics in Medicine},
  publisher = {Wiley-Blackwell},
}

@Article{Shih2006,
  author    = {Weichung Joe Shih},
  title     = {Group sequential, sample size re-estimation and two-stage adaptive designs in clinical trials: a comparison},
  year      = {2006},
  volume    = {25},
  number    = {6},
  pages     = {933--941},
  doi       = {10.1002/sim.2252},
  url       = {http://dx.doi.org/10.1002/sim.2252},
  groups    = {SSR},
  journal   = {Statistics in Medicine},
  publisher = {Wiley-Blackwell},
}

@Report{Gosling2014,
  author      = {John Paul Gosling},
  title       = {Methods for eliciting expert opinion to inform health technology assessment},
  type        = {Vignette},
  institution = {MRC},
  year        = {2014},
  date        = {04/11/14},
  url         = {https://www.mrc.ac.uk/documents/pdf/methods-for-eliciting-expert-opinion-gosling-2014/},
  groups      = {Elicitation},
}

@Article{Dmitrienko2003,
  author    = {Alexei Dmitrienko and Walter W. Offen and Peter H. Westfall},
  title     = {Gatekeeping strategies for clinical trials that do not require all primary effects to be significant},
  journal   = {Statistics in Medicine},
  year      = {2003},
  volume    = {22},
  number    = {15},
  pages     = {2387--2400},
  doi       = {10.1002/sim.1526},
  url       = {http://dx.doi.org/10.1002/sim.1526},
  comment   = {'Gatekeeping' strategy for testing in a phase III trial with multiple endpoints grouped as primary and secondary. We test the primary endpoints (not adjusting for the secondary endpoints) and go on to test the secondary if any one of the primary hypotheses is rejected. Develops testing strategies that control the FWER in each group of endpoints. Uses closed testing procedures - we reject a hypothesis if all intersecting hypotheses are rejected. Looks at several types of test, including weighted Bonferroni (which ignores correlation between test statistics, and is conservative when that is large). Also weighted Simes test.},
  groups    = {Multiple endpoints},
  publisher = {Wiley-Blackwell},
}

@Article{10.2307/2676721,
  author    = {Sandra J. Lee, Marvin Zelen},
  title     = {Clinical Trials and Sample Size Considerations: Another Perspective},
  year      = {2000},
  volume    = {15},
  number    = {2},
  pages     = {95-103},
  issn      = {08834237},
  url       = {http://www.jstor.org/stable/2676721},
  abstract  = {We propose a Bayesian formulation of the sample size problem for planning clinical trials. The frequentist paradigm for calculating sample sizes for clinical trials is to prespecify the type I and II error probabilities. These error probabilities are conditional on the true hypotheses. Instead we propose prespecifying posterior probabilities which are conditional on the outcome of the trial. Our method is easy to implement and has intuitive interpretations. We illustrate an application of our method to the planning of cancer clinical trials for the Eastern Cooperative Oncology Group (ECOG).},
  groups    = {Bayesian},
  journal   = {Statistical Science},
  publisher = {Institute of Mathematical Statistics},
}

@Article{Bryant2000,
  author       = {John Bryant and Roger Day},
  title        = {Comment on Clinical Trials and Sample Size Considerations: Another Perspective},
  journaltitle = {Statistical Science},
  year         = {2000},
  volume       = {15},
  number       = {2},
  pages        = {106-108},
  url          = {http://www.jstor.org/stable/2676723},
  groups       = {Bayesian},
}

@Article{Whitehead1986,
  author    = {John Whitehead},
  title     = {Sample sizes for phase {II} and phase {III} clinical trials: An integrated approach},
  year      = {1986},
  volume    = {5},
  number    = {5},
  month     = {sep},
  pages     = {459--464},
  doi       = {10.1002/sim.4780050510},
  url       = {http://dx.doi.org/10.1002/sim.4780050510},
  groups    = {Phase II, Bayesian},
  journal   = {Statist. Med.},
  publisher = {Wiley-Blackwell},
}

@Article{Whitehead1985,
  author    = {John Whitehead},
  title     = {Designing Phase {II} Studies in the Context of a Programme of Clinical Research},
  year      = {1985},
  volume    = {41},
  number    = {2},
  month     = {jun},
  pages     = {373},
  doi       = {10.2307/2530863},
  url       = {http://dx.doi.org/10.2307/2530863},
  journal   = {Biometrics},
  publisher = {{JSTOR}},
}

@Article{Walwyn2015,
  author    = {Rebecca E. A. Walwyn and Amy M. Russell and Louise D. Bryant and Amanda J. Farrin and Alexandra M. Wright-Hughes and Elizabeth H. Graham and Claire Hulme and John L. O'Dwyer and Gary J. Latchford and Alison J. Stansfield and Dinesh Nagi and Ramzi A. Ajjan and Allan O. House},
  title     = {Supported self-management for adults with type 2 diabetes and a learning disability ({OK}-Diabetes): study protocol for a randomised controlled feasibility trial},
  year      = {2015},
  volume    = {16},
  number    = {1},
  month     = {aug},
  doi       = {10.1186/s13063-015-0832-9},
  url       = {http://dx.doi.org/10.1186/s13063-015-0832-9},
  journal   = {Trials},
  publisher = {Springer Nature},
}

@Article{Bacchetti2011,
  author    = {P. Bacchetti and S. G. Deeks and J. M. McCune},
  title     = {Breaking Free of Sample Size Dogma to Perform Innovative Translational Research},
  year      = {2011},
  volume    = {3},
  number    = {87},
  month     = {jun},
  pages     = {87ps24--87ps24},
  doi       = {10.1126/scitranslmed.3001628},
  url       = {http://dx.doi.org/10.1126/scitranslmed.3001628},
  journal   = {Science Translational Medicine},
  publisher = {American Association for the Advancement of Science ({AAAS})},
}

@Article{Bacchetti2008,
  author    = {Peter Bacchetti and Charles E. McCulloch and Mark R. Segal},
  title     = {Simple, Defensible Sample Sizes Based on Cost Efficiency},
  journal   = {Biometrics},
  year      = {2008},
  volume    = {64},
  number    = {2},
  month     = {jun},
  pages     = {577--585},
  doi       = {10.1111/j.1541-0420.2008.01004\_1.x},
  url       = {http://dx.doi.org/10.1111/j.1541-0420.2008.01004_1.x},
  publisher = {Wiley-Blackwell},
}

@Article{Green1992,
  author    = {Stephanie J. Green and Steve Dahlberg},
  title     = {Planned versus attained design in phase {II} clinical trials},
  year      = {1992},
  volume    = {11},
  number    = {7},
  pages     = {853--862},
  doi       = {10.1002/sim.4780110703},
  url       = {http://dx.doi.org/10.1002/sim.4780110703},
  groups    = {Phase II},
  journal   = {Statist. Med.},
  publisher = {Wiley-Blackwell},
}

@Article{Kunz2011,
  author       = {C. U. Kunz and M. Kieser},
  title        = {Optimal two-stage designs for single-arm phase II oncology trials with two binary endpoints},
  journaltitle = {Methodsof Information in Medicine},
  year         = {2011},
  volume       = {50},
  pages        = {372–377},
  url          = {http://dx.doi.org/10.3414/ME10-01-0037},
}

@Article{Wu2012,
  author    = {Sheng Wu and Catherine M. Crespi and Weng Kee Wong},
  title     = {Comparison of methods for estimating the intraclass correlation coefficient for binary responses in cancer prevention cluster randomized trials},
  year      = {2012},
  volume    = {33},
  number    = {5},
  month     = {sep},
  pages     = {869--880},
  doi       = {10.1016/j.cct.2012.05.004},
  url       = {http://dx.doi.org/10.1016/j.cct.2012.05.004},
  journal   = {Contemporary Clinical Trials},
  publisher = {Elsevier {BV}},
}

@Article{Eldridge2009,
  author    = {Sandra M. Eldridge and Obioha C. Ukoumunne and John B. Carlin},
  title     = {The Intra-Cluster Correlation Coefficient in Cluster Randomized Trials: A Review of Definitions},
  year      = {2009},
  volume    = {77},
  number    = {3},
  month     = {dec},
  pages     = {378--394},
  doi       = {10.1111/j.1751-5823.2009.00092.x},
  url       = {http://dx.doi.org/10.1111/j.1751-5823.2009.00092.x},
  journal   = {International Statistical Review},
  publisher = {Wiley-Blackwell},
}

@Article{Hade2010,
  author    = {E. M. Hade and D. M. Murray and M. L. Pennell and D. Rhoda and E. D. Paskett and V. L. Champion and B. F. Crabtree and A. Dietrich and M. B. Dignan and M. Farmer and J. J. Fenton and S. Flocke and R. A. Hiatt and S. V. Hudson and M. Mitchell and P. Monahan and S. Shariff-Marco and S. L. Slone and K. Stange and S. L. Stewart and P. A. Ohman Strickland},
  title     = {Intraclass Correlation Estimates for Cancer Screening Outcomes: Estimates and Applications in the Design of Group-Randomized Cancer Screening Studies},
  year      = {2010},
  volume    = {2010},
  number    = {40},
  month     = {apr},
  pages     = {97--103},
  doi       = {10.1093/jncimonographs/lgq011},
  url       = {http://dx.doi.org/10.1093/jncimonographs/lgq011},
  groups    = {Multilevel, Bayesian},
  journal   = {{JNCI} Monographs},
  publisher = {Oxford University Press ({OUP})},
}

@Article{Eldridge2016,
  author    = {Sandra M. Eldridge and Gillian A. Lancaster and Michael J. Campbell and Lehana Thabane and Sally Hopewell and Claire L. Coleman and Christine M. Bond},
  title     = {Defining Feasibility and Pilot Studies in Preparation for Randomised Controlled Trials: Development of a Conceptual Framework},
  year      = {2016},
  editor    = {Chiara Lazzeri},
  volume    = {11},
  number    = {3},
  month     = {mar},
  pages     = {e0150205},
  doi       = {10.1371/journal.pone.0150205},
  url       = {http://dx.doi.org/10.1371/journal.pone.0150205},
  groups    = {Pilot/feasibility},
  journal   = {{PLOS} {ONE}},
  publisher = {Public Library of Science ({PLoS})},
}

@Article{Kontopantelis2016,
  author    = {Evangelos Kontopantelis and David A Springate and Rosa Parisi and David Reeves},
  title     = {Simulation-Based Power Calculations for Mixed Effects Modeling: ipdpower in Stata},
  year      = {2016},
  volume    = {74},
  number    = {12},
  doi       = {10.18637/jss.v074.i12},
  url       = {http://dx.doi.org/10.18637/jss.v074.i12},
  groups    = {multi-D / sim SS},
  journal   = {Journal of Statistical Software},
  publisher = {Foundation for Open Access Statistic},
}

@Article{Grieve2016,
  author    = {Andrew P. Grieve and Shah-Jalal Sarker},
  title     = {Simulation-based sample-sizing and power calculations in logistic regression with partial prior information},
  year      = {2016},
  volume    = {15},
  number    = {6},
  month     = {sep},
  pages     = {507--516},
  doi       = {10.1002/pst.1773},
  url       = {http://dx.doi.org/10.1002/pst.1773},
  groups    = {multi-D / sim SS},
  journal   = {Pharmaceutical Statistics},
  publisher = {Wiley-Blackwell},
}

@Article{Hooper2016,
  author    = {Richard Hooper and Steven Teerenstra and Esther de Hoop and Sandra Eldridge},
  title     = {Sample size calculation for stepped wedge and other longitudinal cluster randomised trials},
  year      = {2016},
  volume    = {35},
  number    = {26},
  month     = {jun},
  pages     = {4718--4728},
  doi       = {10.1002/sim.7028},
  url       = {http://dx.doi.org/10.1002/sim.7028},
  groups    = {multi-D / sim SS},
  journal   = {Statistics in Medicine},
  publisher = {Wiley-Blackwell},
}

@Article{Baio2015,
  author    = {Gianluca Baio and Andrew Copas and Gareth Ambler and James Hargreaves and Emma Beard and Rumana Z Omar},
  title     = {Sample size calculation for a stepped wedge trial},
  year      = {2015},
  volume    = {16},
  number    = {1},
  month     = {aug},
  doi       = {10.1186/s13063-015-0840-9},
  url       = {http://dx.doi.org/10.1186/s13063-015-0840-9},
  groups    = {multi-D / sim SS},
  journal   = {Trials},
  publisher = {Springer Nature},
}

@Article{Schoenfeld2005,
  author    = {David A. Schoenfeld and Michael Borenstein},
  title     = {Calculating the power or sample size for the logistic and proportional hazards models},
  year      = {2005},
  volume    = {75},
  number    = {10},
  month     = {oct},
  pages     = {771--785},
  doi       = {10.1080/00949650410001729445},
  url       = {http://dx.doi.org/10.1080/00949650410001729445},
  groups    = {multi-D / sim SS},
  journal   = {Journal of Statistical Computation and Simulation},
  publisher = {Informa {UK} Limited},
}

@Article{Feiveson2002,
  author       = {A. H. Feiveson},
  title        = {Power by Simulation},
  journaltitle = {The Stata Journal},
  year         = {2002},
  volume       = {2},
  pages        = {107-124},
  url          = {http://ageconsearch.umn.edu/bitstream/115955/2/sjart_st0010.pdf},
  groups       = {multi-D / sim SS},
}

@Article{Arnold2011,
  author    = {Benjamin F Arnold and Daniel R Hogan and John M Colford and Alan E Hubbard},
  title     = {Simulation methods to estimate design power: an overview for applied research},
  journal   = {{BMC} Med Res Methodol},
  year      = {2011},
  volume    = {11},
  number    = {1},
  month     = {6},
  doi       = {10.1186/1471-2288-11-94},
  url       = {http://dx.doi.org/10.1186/1471-2288-11-94},
  groups    = {multi-D / sim SS},
  publisher = {Springer Nature},
}

@Article{Gastanaga2006,
  author    = {Victor M. Gasta{\~{n}}aga and Christine E. McLaren and Ralph J. Delfino},
  title     = {Power calculations for generalized linear models in observational longitudinal studies: A simulation approach in {SAS}},
  year      = {2006},
  volume    = {84},
  number    = {1},
  month     = {oct},
  pages     = {27--33},
  doi       = {10.1016/j.cmpb.2006.07.011},
  url       = {http://dx.doi.org/10.1016/j.cmpb.2006.07.011},
  groups    = {multi-D / sim SS},
  journal   = {Computer Methods and Programs in Biomedicine},
  publisher = {Elsevier {BV}},
}

@Article{Chang1999,
  author    = {Myron N. Chang and Jonathan J. Shuster and James L. Kepner},
  title     = {Group Sequential Designs for Phase {II} Trials With Historical Controls},
  year      = {1999},
  volume    = {20},
  number    = {4},
  month     = {aug},
  pages     = {353--364},
  doi       = {10.1016/s0197-2456(99)00008-2},
  url       = {http://dx.doi.org/10.1016/S0197-2456(99)00008-2},
  groups    = {Phase II},
  journal   = {Controlled Clinical Trials},
  publisher = {Elsevier {BV}},
}

@Book{Rasmussen2006,
  author    = {Carl Edward Rasmussen and Christopher K. I. Williams},
  title     = {Gaussian Processes for Machine Learning},
  year      = {2006},
  publisher = {MIT Press},
  groups    = {Optimisation},
}

@Article{Deb2002,
  author    = {K. Deb and A. Pratap and S. Agarwal and T. Meyarivan},
  title     = {A fast and elitist multiobjective genetic algorithm: {NSGA}-{II}},
  year      = {2002},
  volume    = {6},
  number    = {2},
  month     = {apr},
  pages     = {182--197},
  doi       = {10.1109/4235.996017},
  url       = {http://dx.doi.org/10.1109/4235.996017},
  journal   = {{IEEE} Transactions on Evolutionary Computation},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Manual{Mersmann2014,
  author = {Olaf Mersmann},
  title  = {mco: Multiple Criteria Optimization Algorithms and Related Functions},
  year   = {2014},
  note   = {R package version 1.0-15.1},
  url    = {https://CRAN.R-project.org/package=mco},
}

@Manual{Carnell2016,
  author = {Rob Carnell},
  title  = {lhs: Latin Hypercube Samples},
  year   = {2016},
  note   = {R package version 0.14},
  url    = {https://CRAN.R-project.org/package=lhs},
}

@InProceedings{Balling2003,
  author    = {Richard Balling},
  title     = {The Maximin Fitness Function; Multi-objective City and Regional Planning},
  booktitle = {Evolutionary Multi-Criterion Optimization},
  year      = {2003},
  editor    = {Fonseca, C.M. and Fleming, P.J. and Zitzler, E. and Deb, K. and Thiele, L.},
  publisher = {Springer},
  pages     = {1 - 15},
}

@Article{Picheny2013,
  author    = {Victor Picheny and David Ginsbourger and Yann Richet and Gregory Caplin},
  title     = {Quantile-Based Optimization of Noisy Computer Experiments With Tunable Precision},
  year      = {2013},
  volume    = {55},
  number    = {1},
  month     = {feb},
  pages     = {2--13},
  doi       = {10.1080/00401706.2012.707580},
  url       = {http://dx.doi.org/10.1080/00401706.2012.707580},
  groups    = {Optimisation},
  journal   = {Technometrics},
  publisher = {Informa {UK} Limited},
}

@Article{Raudenbush2000,
  author    = {Stephen W. Raudenbush and Xiaofeng Liu},
  title     = {Statistical power and optimal design for multisite randomized trials.},
  year      = {2000},
  volume    = {5},
  number    = {2},
  pages     = {199--213},
  doi       = {10.1037/1082-989x.5.2.199},
  url       = {http://dx.doi.org/10.1037/1082-989X.5.2.199},
  groups    = {Multilevel},
  journal   = {Psychological Methods},
  publisher = {American Psychological Association ({APA})},
}

@Article{Shanyinde2011,
  author    = {Milensu Shanyinde and Ruth M Pickering and Mark Weatherall},
  title     = {Questions asked and answered in pilot and feasibility randomized controlled trials},
  year      = {2011},
  volume    = {11},
  number    = {1},
  month     = {aug},
  doi       = {10.1186/1471-2288-11-117},
  url       = {http://dx.doi.org/10.1186/1471-2288-11-117},
  groups    = {Pilot/feasibility},
  journal   = {{BMC} Medical Research Methodology},
  publisher = {Springer Nature},
}

@Article{Sim2012,
  author    = {Julius Sim and Martyn Lewis},
  title     = {The size of a pilot study for a clinical trial should be calculated in relation to considerations of precision and efficiency},
  year      = {2012},
  volume    = {65},
  number    = {3},
  month     = {mar},
  pages     = {301--308},
  doi       = {10.1016/j.jclinepi.2011.07.011},
  url       = {http://dx.doi.org/10.1016/j.jclinepi.2011.07.011},
  groups    = {Pilot/feasibility},
  journal   = {Journal of Clinical Epidemiology},
  publisher = {Elsevier {BV}},
}

@Article{Teerenstra2008,
  author    = {S. Teerenstra and M. Moerbeek and T. van Achterberg and B. J. Pelzer and G. F. Borm},
  title     = {Sample size calculations for 3-level cluster randomized trials},
  year      = {2008},
  volume    = {5},
  number    = {5},
  month     = {sep},
  pages     = {486--495},
  doi       = {10.1177/1740774508096476},
  url       = {http://dx.doi.org/10.1177/1740774508096476},
  groups    = {multi-D / sim SS},
  journal   = {Clinical Trials},
  publisher = {{SAGE} Publications},
}

@Article{Wason2011,
  author    = {James M.S. Wason and Adrian P Mander and Simon G. Thompson},
  title     = {Optimal multistage designs for randomised clinical trials with continuous outcomes},
  year      = {2011},
  volume    = {31},
  number    = {4},
  month     = {dec},
  pages     = {301--312},
  doi       = {10.1002/sim.4421},
  url       = {http://dx.doi.org/10.1002/sim.4421},
  groups    = {multi-D / sim SS},
  journal   = {Statistics in Medicine},
  publisher = {Wiley-Blackwell},
}

@Article{Wason2012,
  author    = {James M. S. Wason and Thomas Jaki},
  title     = {Optimal design of multi-arm multi-stage trials},
  year      = {2012},
  volume    = {31},
  number    = {30},
  month     = {jul},
  pages     = {4269--4279},
  doi       = {10.1002/sim.5513},
  url       = {http://dx.doi.org/10.1002/sim.5513},
  groups    = {multi-D / sim SS},
  journal   = {Statistics in Medicine},
  publisher = {Wiley-Blackwell},
}

@Article{Shao2007,
  author    = {Jun Shao and Huaibao Feng},
  title     = {Group sequential t-test for clinical trials with small sample sizes across stages},
  year      = {2007},
  volume    = {28},
  number    = {5},
  month     = {sep},
  pages     = {563--571},
  doi       = {10.1016/j.cct.2007.02.006},
  url       = {http://dx.doi.org/10.1016/j.cct.2007.02.006},
  groups    = {multi-D / sim SS},
  journal   = {Contemporary Clinical Trials},
  publisher = {Elsevier {BV}},
}

@Article{Eldridge2016a,
  author    = {Sandra M Eldridge and Claire L Chan and Michael J Campbell and Christine M Bond and Sally Hopewell and Lehana Thabane and Gillian A Lancaster},
  title     = {{CONSORT} 2010~{s}tatement: extension to randomised pilot and feasibility trials},
  year      = {2016},
  month     = {oct},
  pages     = {i5239},
  doi       = {10.1136/bmj.i5239},
  url       = {http://dx.doi.org/10.1136/bmj.i5239},
  groups    = {Pilot/feasibility},
  journal   = {{BMJ}},
  publisher = {{BMJ}},
}

@Article{Sackett2007,
  author    = {D. L. Sackett},
  title     = {Commentary: Measuring the success of blinding in {RCTs}: don{\textquotesingle}t, must, can{\textquotesingle}t or needn{\textquotesingle}t?},
  year      = {2007},
  volume    = {36},
  number    = {3},
  month     = {jun},
  pages     = {664--665},
  doi       = {10.1093/ije/dym088},
  url       = {http://dx.doi.org/10.1093/ije/dym088},
  journal   = {International Journal of Epidemiology},
  publisher = {Oxford University Press ({OUP})},
}

@Article{Jung2004,
  author    = {Sin-Ho Jung and Taiyeong Lee and Kyung Mann Kim and Stephen L. George},
  title     = {Admissible two-stage designs for phase {II} cancer clinical trials},
  journal   = {Statistics in Medicine},
  year      = {2004},
  volume    = {23},
  number    = {4},
  pages     = {561--569},
  doi       = {10.1002/sim.1600},
  url       = {http://dx.doi.org/10.1002/sim.1600},
  groups    = {Phase II, multi-D / sim SS},
  publisher = {Wiley-Blackwell},
}

@Article{Wason2012a,
  author    = {James M. S. Wason and Adrian P. Mander},
  title     = {Minimizing the Maximum Expected Sample Size in Two-Stage Phase {II} Clinical Trials with Continuous Outcomes},
  year      = {2012},
  volume    = {22},
  number    = {4},
  month     = {jul},
  pages     = {836--852},
  doi       = {10.1080/10543406.2010.528104},
  url       = {http://dx.doi.org/10.1080/10543406.2010.528104},
  journal   = {Journal of Biopharmaceutical Statistics},
  publisher = {Informa {UK} Limited},
}

@Article{Teerenstra2012,
  author    = {Steven Teerenstra and Sandra Eldridge and Maud Graff and Esther Hoop and George F. Borm},
  title     = {A simple sample size formula for analysis of covariance in cluster randomized trials},
  year      = {2012},
  volume    = {31},
  number    = {20},
  month     = {apr},
  pages     = {2169--2178},
  doi       = {10.1002/sim.5352},
  url       = {http://dx.doi.org/10.1002/sim.5352},
  journal   = {Statistics in Medicine},
  publisher = {Wiley-Blackwell},
}

@Article{Rotondi2009,
  author    = {Michael A. Rotondi and Allan Donner},
  title     = {Sample Size Estimation in Cluster Randomized Educational Trials: An Empirical Bayes Approach},
  year      = {2009},
  volume    = {34},
  number    = {2},
  pages     = {229-237},
  issn      = {10769986, 19351054},
  url       = {http://www.jstor.org/stable/40263527},
  abstract  = {The educational field has now accumulated an extensive literature reporting on values of the intraclass correlation coefficient, a parameter essential to determining the required size of a planned cluster randomized trial. We propose here a simple simulation-based approach including all relevant information that can facilitate this task. An example and corresponding computer code is attached.},
  journal   = {Journal of Educational and Behavioral Statistics},
  publisher = {[American Educational Research Association, Sage Publications, Inc., American Statistical Association]},
}

@Article{Sahu2006,
  author    = {S. K. Sahu and T. M. F. Smith},
  title     = {A Bayesian method of sample size determination with practical applications},
  year      = {2006},
  volume    = {169},
  number    = {2},
  month     = {mar},
  pages     = {235--253},
  doi       = {10.1111/j.1467-985x.2006.00408.x},
  url       = {http://dx.doi.org/10.1111/j.1467-985X.2006.00408.x},
  groups    = {Decision theory, Bayesian},
  journal   = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  publisher = {Wiley-Blackwell},
}

@Article{Julious2006,
  author    = {Steven A. Julious and Roger J. Owen},
  title     = {Sample size calculations for clinical studies allowing for uncertainty about the variance},
  year      = {2006},
  volume    = {5},
  number    = {1},
  month     = {jan},
  pages     = {29--37},
  doi       = {10.1002/pst.197},
  url       = {http://dx.doi.org/10.1002/pst.197},
  groups    = {Pilot/feasibility},
  journal   = {Pharmaceutical Statistics},
  publisher = {Wiley-Blackwell},
}

@Article{Donkers2016,
  author    = {H.W. Donkers and M.J.L. Graff and M.J. Vernooij-Dassen and M.W.G. Nijhuis- van der Sanden and S. Teerenstra},
  title     = {Reducing sample size by combining superiority and non-inferiority for two primary endpoints in the Social Fitness study},
  year      = {2016},
  month     = {sep},
  doi       = {10.1016/j.jclinepi.2016.09.007},
  url       = {http://dx.doi.org/10.1016/j.jclinepi.2016.09.007},
  groups    = {multi-D / sim SS, Multiple endpoints, Bayesian},
  journal   = {Journal of Clinical Epidemiology},
  publisher = {Elsevier {BV}},
}

@Article{Shih2004,
  author    = {Weichung Joseph Shih and Pamela A. Ohman-Strickland and Yong Lin},
  title     = {Analysis of pilot and early phase studies with small sample sizes},
  year      = {2004},
  volume    = {23},
  number    = {12},
  month     = {jun},
  pages     = {1827--1842},
  doi       = {10.1002/sim.1807},
  url       = {http://dx.doi.org/10.1002/sim.1807},
  groups    = {Pilot/feasibility},
  journal   = {Statistics in Medicine},
  publisher = {Wiley-Blackwell},
}

@Article{Hertzog2008,
  author    = {Melody A. Hertzog},
  title     = {Considerations in determining sample size for pilot studies},
  year      = {2008},
  volume    = {31},
  number    = {2},
  pages     = {180--191},
  doi       = {10.1002/nur.20247},
  url       = {http://dx.doi.org/10.1002/nur.20247},
  groups    = {Pilot/feasibility},
  journal   = {Research in Nursing {\&} Health},
  publisher = {Wiley-Blackwell},
}

@Article{Schoenfeld1980,
  author    = {David Schoenfeld},
  title     = {Statistical considerations for pilot studies},
  year      = {1980},
  volume    = {6},
  number    = {3},
  month     = {mar},
  pages     = {371--374},
  doi       = {10.1016/0360-3016(80)90153-4},
  url       = {http://dx.doi.org/10.1016/0360-3016(80)90153-4},
  groups    = {Pilot/feasibility},
  journal   = {International Journal of Radiation Oncology{\ast}Biology{\ast}Physics},
  publisher = {Elsevier {BV}},
}

@Article{Jiang2014,
  author    = {Yu Jiang and Steve Simon and Matthew S. Mayo and Byron J. Gajewski},
  title     = {Modeling and validating Bayesian accrual models on clinical data and simulations using adaptive priors},
  year      = {2014},
  volume    = {34},
  number    = {4},
  month     = {nov},
  pages     = {613--629},
  doi       = {10.1002/sim.6359},
  url       = {https://doi.org/10.1002%2Fsim.6359},
  groups    = {Pilot/feasibility},
  journal   = {Statistics in Medicine},
  publisher = {Wiley-Blackwell},
}

@Article{Gajewski2008,
  author    = {Byron J. Gajewski and Stephen D. Simon and Susan E. Carlson},
  title     = {Predicting accrual in clinical trials with Bayesian posterior predictive distributions},
  year      = {2008},
  volume    = {27},
  number    = {13},
  pages     = {2328--2340},
  doi       = {10.1002/sim.3128},
  url       = {https://doi.org/10.1002%2Fsim.3128},
  groups    = {Pilot/feasibility},
  journal   = {Statistics in Medicine},
  publisher = {Wiley-Blackwell},
}

@Article{Barnard2010,
  author    = {Katharine D Barnard and Louise Dent and Andrew Cook},
  title     = {A systematic review of models to predict recruitment to multicentre clinical trials},
  year      = {2010},
  volume    = {10},
  number    = {1},
  month     = {jul},
  doi       = {10.1186/1471-2288-10-63},
  url       = {https://doi.org/10.1186%2F1471-2288-10-63},
  groups    = {Pilot/feasibility},
  journal   = {{BMC} Medical Research Methodology},
  publisher = {Springer Nature},
}

@Article{Senn1998,
  author    = {Stephen Senn},
  title     = {Some controversies in planning and analysing multi-centre trials},
  year      = {1998},
  volume    = {17},
  number    = {15-16},
  month     = {aug},
  pages     = {1753--1765},
  doi       = {10.1002/(sici)1097-0258(19980815/30)17:15/16<1753::aid-sim977>3.0.co;2-x},
  url       = {https://doi.org/10.1002%2F%28sici%291097-0258%2819980815%2F30%2917%3A15%2F16%3C1753%3A%3Aaid-sim977%3E3.0.co%3B2-x},
  groups    = {Pilot/feasibility},
  journal   = {Statistics in Medicine},
  publisher = {Wiley-Blackwell},
}

@Article{Feliot2016,
  author    = {Paul Feliot and Julien Bect and Emmanuel Vazquez},
  title     = {A Bayesian approach to constrained single- and multi-objective optimization},
  year      = {2016},
  month     = {apr},
  doi       = {10.1007/s10898-016-0427-3},
  url       = {https://doi.org/10.1007%2Fs10898-016-0427-3},
  groups    = {Optimisation, multi-D / sim SS},
  journal   = {Journal of Global Optimization},
  publisher = {Springer Nature},
}

@Article{Ren2013,
  author    = {Shijie Ren and Jeremy E. Oakley},
  title     = {Assurance calculations for planning clinical trials with time-to-event outcomes},
  year      = {2013},
  volume    = {33},
  number    = {1},
  month     = {jul},
  pages     = {31--45},
  doi       = {10.1002/sim.5916},
  url       = {https://doi.org/10.1002%2Fsim.5916},
  groups    = {Bayesian},
  journal   = {Statistics in Medicine},
  publisher = {Wiley-Blackwell},
}

@Article{Longford2012,
  author    = {Nicholas T. Longford},
  title     = {Screening as an application of decision theory},
  year      = {2012},
  volume    = {32},
  number    = {5},
  month     = {aug},
  pages     = {849--863},
  doi       = {10.1002/sim.5554},
  url       = {https://doi.org/10.1002%2Fsim.5554},
  groups    = {Bayesian, Decision theory},
  journal   = {Statistics in Medicine},
  publisher = {Wiley-Blackwell},
}

@Article{Strong2014,
  author    = {Mark Strong and Jeremy E. Oakley and Alan Brennan},
  title     = {Estimating Multiparameter Partial Expected Value of Perfect Information from a Probabilistic Sensitivity Analysis Sample},
  year      = {2014},
  volume    = {34},
  number    = {3},
  month     = {apr},
  pages     = {311--326},
  doi       = {10.1177/0272989x13505910},
  url       = {https://doi.org/10.1177%2F0272989x13505910},
  groups    = {Decision theory},
  journal   = {Medical Decision Making},
  publisher = {{SAGE} Publications},
}

@Article{Albert2012,
  author    = {Isabelle Albert and Sophie Donnet and Chantal Guihenneuc-Jouyaux and Samantha Low-Choy and Kerrie Mengersen and Judith Rousseau},
  title     = {Combining Expert Opinions in Prior Elicitation},
  year      = {2012},
  volume    = {7},
  number    = {3},
  month     = {sep},
  pages     = {503--532},
  doi       = {10.1214/12-ba717},
  url       = {https://doi.org/10.1214%2F12-ba717},
  groups    = {Elicitation, Bayesian},
  journal   = {Bayesian Analysis},
  publisher = {Institute of Mathematical Statistics},
}

@Article{Viechtbauer2015,
  author    = {Wolfgang Viechtbauer and Luc Smits and Daniel Kotz and Luc Bud{\'{e}} and Mark Spigt and Jan Serroyen and Rik Crutzen},
  title     = {A simple formula for the calculation of sample size in pilot studies},
  year      = {2015},
  volume    = {68},
  number    = {11},
  month     = {nov},
  pages     = {1375--1379},
  doi       = {10.1016/j.jclinepi.2015.04.014},
  url       = {https://doi.org/10.1016%2Fj.jclinepi.2015.04.014},
  groups    = {Pilot/feasibility},
  journal   = {Journal of Clinical Epidemiology},
  publisher = {Elsevier {BV}},
}

@Article{Goette2015,
  author    = {Heiko Götte and Armin Schüler and Marietta Kirchner and Meinhard Kieser},
  title     = {Sample size planning for phase {II} trials based on success probabilities for phase {III}},
  year      = {2015},
  volume    = {14},
  number    = {6},
  month     = {sep},
  pages     = {515--524},
  doi       = {10.1002/pst.1717},
  url       = {https://doi.org/10.1002%2Fpst.1717},
  groups    = {Phase II},
  journal   = {Pharmaceutical Statistics},
  publisher = {Wiley-Blackwell},
}

@Article{Stallard2016,
  author    = {Nigel Stallard and Frank Miller and Simon Day and Siew Wan Hee and Jason Madan and Sarah Zohar and Martin Posch},
  title     = {Determination of the optimal sample size for a clinical trial accounting for the population size},
  year      = {2016},
  month     = {may},
  doi       = {10.1002/bimj.201500228},
  url       = {https://doi.org/10.1002%2Fbimj.201500228},
  groups    = {Phase II, Decision theory},
  journal   = {Biometrical Journal},
  publisher = {Wiley-Blackwell},
}

@Article{Freiman1978,
  author    = {Jennie A. Freiman and Thomas C. Chalmers and Harry Smith and Roy R. Kuebler},
  title     = {The Importance of Beta, the Type {II} Error and Sample Size in the Design and Interpretation of the Randomized Control Trial},
  year      = {1978},
  volume    = {299},
  number    = {13},
  month     = {sep},
  pages     = {690--694},
  doi       = {10.1056/nejm197809282991304},
  url       = {https://doi.org/10.1056%2Fnejm197809282991304},
  journal   = {New England Journal of Medicine},
  publisher = {New England Journal of Medicine ({NEJM}/{MMS})},
}

@Article{Bretz2006,
  author    = {Frank Bretz and Heinz Schmidli and Franz König and Amy Racine and Willi Maurer},
  title     = {Confirmatory Seamless Phase {II}/{III} Clinical Trials with Hypotheses Selection at Interim: General Concepts},
  year      = {2006},
  volume    = {48},
  number    = {4},
  month     = {aug},
  pages     = {623--634},
  doi       = {10.1002/bimj.200510232},
  url       = {https://doi.org/10.1002%2Fbimj.200510232},
  groups    = {Phase II, Multiple endpoints},
  journal   = {Biometrical Journal},
  publisher = {Wiley-Blackwell},
}

@Article{Jennison2006,
  author    = {Christopher Jennison and Bruce W. Turnbull},
  title     = {Confirmatory Seamless Phase {II}/{III} Clinical Trials with Hypotheses Selection at Interim: Opportunities and Limitations},
  year      = {2006},
  volume    = {48},
  number    = {4},
  month     = {aug},
  pages     = {650--655},
  doi       = {10.1002/bimj.200610248},
  url       = {https://doi.org/10.1002%2Fbimj.200610248},
  groups    = {Multiple endpoints, Phase II},
  journal   = {Biometrical Journal},
  publisher = {Wiley-Blackwell},
}

@Article{Bedrick1996,
  author  = {Edward J. Bedrick and Ronald Christensen and Wesley Johnson},
  title   = {A New Perspective on Priors for Generalized Linear Models},
  year    = {1996},
  volume  = {91},
  number  = {436},
  pages   = {1450-1460},
  doi     = {10.1080/01621459.1996.10476713},
  eprint  = { http://amstat.tandfonline.com/doi/pdf/10.1080/01621459.1996.10476713 },
  url     = { 
        http://amstat.tandfonline.com/doi/abs/10.1080/01621459.1996.10476713
    
},
  groups  = {Bayesian, Elicitation},
  journal = {Journal of the American Statistical Association},
}

@Article{Herndon1998,
  author    = {James E.Herndon II},
  title     = {A Design Alternative for Two-Stage, Phase {II}, Multicenter Cancer Clinical Trials},
  year      = {1998},
  volume    = {19},
  number    = {5},
  month     = {oct},
  pages     = {440--450},
  doi       = {10.1016/s0197-2456(98)00012-9},
  url       = {https://doi.org/10.1016%2Fs0197-2456%2898%2900012-9},
  journal   = {Controlled Clinical Trials},
  publisher = {Elsevier {BV}},
}

@Article{Sklar1973,
  author       = {Abe Sklar},
  title        = {Random variables, joint distribution functions, and copulas},
  journaltitle = {Kybernetika},
  year         = {1973},
  volume       = {9},
  number       = {6},
  pages        = {449-460},
  url          = {http://dml.cz/bitstream/handle/10338.dmlcz/125838/Kybernetika_09-1973-6_2.pdf},
}

@Article{ACCRDSG2008,
  author    = {{The Action to Control Cardiovascular Risk in Diabetes Study Group}},
  title     = {Effects of Intensive Glucose Lowering in Type 2 Diabetes},
  year      = {2008},
  volume    = {358},
  number    = {24},
  month     = {jun},
  pages     = {2545--2559},
  doi       = {10.1056/nejmoa0802743},
  url       = {https://doi.org/10.1056%2Fnejmoa0802743},
  journal   = {New England Journal of Medicine},
  publisher = {New England Journal of Medicine ({NEJM}/{MMS})},
}

@Article{Jennison1997,
  author    = {Christopher Jennison and Bruce W. Turnbull},
  title     = {Distribution theory of group sequential t,$\upchi$2and F-tests for general linear models},
  year      = {1997},
  volume    = {16},
  number    = {4},
  month     = {jan},
  pages     = {295--317},
  doi       = {10.1080/07474949708836390},
  url       = {https://doi.org/10.1080%2F07474949708836390},
  groups    = {multi-D / sim SS},
  journal   = {Sequential Analysis},
  publisher = {Informa {UK} Limited},
}

@Article{Mander2010,
  author    = {A.P. Mander and S.G. Thompson},
  title     = {Two-stage designs optimal under the alternative hypothesis for phase {II} cancer clinical trials},
  year      = {2010},
  volume    = {31},
  number    = {6},
  month     = {nov},
  pages     = {572--578},
  doi       = {10.1016/j.cct.2010.07.008},
  url       = {https://doi.org/10.1016%2Fj.cct.2010.07.008},
  groups    = {multi-D / sim SS, Phase II},
  journal   = {Contemporary Clinical Trials},
  publisher = {Elsevier {BV}},
}

@Article{Whitehead1983,
  author    = {John Whitehead and Irene Stratton},
  title     = {Group Sequential Clinical Trials with Triangular Continuation Regions},
  year      = {1983},
  volume    = {39},
  number    = {1},
  month     = {mar},
  pages     = {227},
  doi       = {10.2307/2530822},
  url       = {https://doi.org/10.2307%2F2530822},
  groups    = {multi-D / sim SS},
  journal   = {Biometrics},
  publisher = {{JSTOR}},
}

@Article{Wason2016,
  author    = {James Wason and Dominic Magirr and Martin Law and Thomas Jaki},
  title     = {Some recommendations for multi-arm multi-stage trials},
  year      = {2016},
  volume    = {25},
  number    = {2},
  month     = {apr},
  pages     = {716--727},
  doi       = {10.1177/0962280212465498},
  url       = {https://doi.org/10.1177%2F0962280212465498},
  groups    = {multi-D / sim SS},
  journal   = {Statistical Methods in Medical Research},
  publisher = {{SAGE} Publications},
}

@Article{Moser1989,
  author    = {Barry K. Moser and Gary R. Stevens and Christian L. Watts},
  title     = {The two-sample t test versus satterthwaite{\textquotesingle}s approximate f test},
  year      = {1989},
  volume    = {18},
  number    = {11},
  month     = {jan},
  pages     = {3963--3975},
  doi       = {10.1080/03610928908830135},
  url       = {https://doi.org/10.1080%2F03610928908830135},
  file      = {:U\:\\Literature\\Papers\\The two-sample t test versus satterthwaites approximate f test.pdf:PDF},
  journal   = {Communications in Statistics - Theory and Methods},
  publisher = {Informa {UK} Limited},
}

@Article{Batistatou2014,
  author       = {Evridiki Batistatou and Chris Roberts and Steve Roberts},
  title        = {Sample size and power calculations for trials and quasi-experimental studies with clustering},
  journaltitle = {The Stata Journal},
  year         = {2014},
  volume       = {14},
  issue        = {1},
  url          = {http://www.econ.uzh.ch/dam/jcr:00000000-5766-84b0-0000-000011c494e2/sj14-1.pdf#page=165},
  abstract     = {This article considers the estimation of power and sample size in experimental
and quasi-experimental intervention studies, where there is clustering of
subjects within one or both intervention arms, for both continuous and binary outcomes.
A new command, clsampsi, which has a wide range of options, calculates
the power and sample size needed (that is, the number of clusters and cluster size)
by using the noncentral F distribution as described by Moser, Stevens, and Watts
(1989, Communications in Statistics—Theory and Methods 18: 3963–3975). For
comparative purposes, this command can also produce power and sample-size estimates
on the basis of existing methods that use a normal approximation.},
}

@Book{Pinheiro2000,
  author    = {José C. Pinheiro and Douglas M. Bates},
  title     = {Mixed-Effects Models in S and S-PLUS},
  year      = {2000},
  publisher = {Springer},
  file      = {:U\:\\Literature\\Books\\Mixed Effects Models in S and S-Plus.pdf:PDF},
}

@Report{McConnell2015,
  author      = {McConnell, Brendon and Vera-Hernández, Marcos},
  title       = {Going beyond simple sample size calculations: A practitioner's guide},
  type        = {Working Paper},
  institution = {Institute for Fiscal Studies},
  year        = {2015},
  number      = {W15/17},
  doi         = {10.1920/wp.ifs.2015.1517},
  groups      = {multi-D / sim SS},
}

@Article{Bloom2007,
  author    = {H. S. Bloom and L. Richburg-Hayes and A. R. Black},
  title     = {Using Covariates to Improve Precision for Studies That Randomize Schools to Evaluate Educational Interventions},
  year      = {2007},
  volume    = {29},
  number    = {1},
  month     = {mar},
  pages     = {30--59},
  doi       = {10.3102/0162373707299550},
  url       = {https://doi.org/10.3102%2F0162373707299550},
  groups    = {Multilevel, multi-D / sim SS},
  journal   = {Educational Evaluation and Policy Analysis},
  publisher = {American Educational Research Association ({AERA})},
}

@Article{Picheny2014,
  author    = {Victor Picheny and David Ginsbourger},
  title     = {Noisy kriging-based optimization methods: A unified implementation within the {DiceOptim} package},
  year      = {2014},
  volume    = {71},
  month     = {mar},
  pages     = {1035--1053},
  doi       = {10.1016/j.csda.2013.03.018},
  url       = {https://doi.org/10.1016%2Fj.csda.2013.03.018},
  groups    = {Optimisation},
  journal   = {Computational Statistics {\&} Data Analysis},
  publisher = {Elsevier {BV}},
}

@Article{Dupuy2015,
  author    = {Delphine Dupuy and C{\'{e}}line Helbert and Jessica Franco},
  title     = {{DiceDesign} and {DiceEval} : Two R Packages for Design and Analysis of Computer Experiments},
  year      = {2015},
  volume    = {65},
  number    = {11},
  doi       = {10.18637/jss.v065.i11},
  url       = {https://doi.org/10.18637%2Fjss.v065.i11},
  groups    = {Optimisation},
  journal   = {Journal of Statistical Software},
  publisher = {Foundation for Open Access Statistic},
}

@Article{Knowles2006,
  author    = {J. Knowles},
  title     = {{ParEGO}: a hybrid algorithm with on-line landscape approximation for expensive multiobjective optimization problems},
  year      = {2006},
  volume    = {10},
  number    = {1},
  month     = {feb},
  pages     = {50--66},
  doi       = {10.1109/tevc.2005.851274},
  url       = {https://doi.org/10.1109%2Ftevc.2005.851274},
  groups    = {Optimisation},
  journal   = {{IEEE} Transactions on Evolutionary Computation},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Sasena2002,
  author    = {Michael J. Sasena and Panos Papalambros and Pierre Goovaerts},
  title     = {Exploration of Metamodeling Sampling Criteria for Constrained Global Optimization},
  year      = {2002},
  volume    = {34},
  number    = {3},
  month     = {jan},
  pages     = {263--278},
  doi       = {10.1080/03052150211751},
  url       = {https://doi.org/10.1080%2F03052150211751},
  groups    = {Optimisation},
  journal   = {Engineering Optimization},
  publisher = {Informa {UK} Limited},
}

@Article{Jo2002,
  author    = {Booil Jo},
  title     = {Statistical power in randomized intervention studies with noncompliance.},
  journal   = {Psychological Methods},
  year      = {2002},
  volume    = {7},
  number    = {2},
  pages     = {178--193},
  doi       = {10.1037/1082-989x.7.2.178},
  url       = {https://doi.org/10.1037%2F1082-989x.7.2.178},
  file      = {:U\:\\Literature\\Papers\\Jo02.pdf:PDF},
  groups    = {Complex Interventions},
  publisher = {American Psychological Association ({APA})},
}

@Article{Forster2017,
  author    = {Anne Forster and and Jennifer Airlie and Karen Birch and Robert Cicero and Bonnie Cundill and Alison Ellwood and Mary Godfrey and Liz Graham and John Green and Claire Hulme and Rebecca Lawton and Vicki McLellan and Nicola McMaster and Amanda Farrin},
  title     = {Research Exploring Physical Activity in Care Homes ({REACH}): study protocol for a randomised controlled trial},
  journal   = {Trials},
  year      = {2017},
  volume    = {18},
  number    = {1},
  month     = {apr},
  doi       = {10.1186/s13063-017-1921-8},
  url       = {https://doi.org/10.1186%2Fs13063-017-1921-8},
  publisher = {Springer Nature},
}

@Article{Ambrosius2010,
  author    = {Walter T. Ambrosius and Jonathan D. Mahnken},
  title     = {Power for studies with random group sizes},
  year      = {2010},
  pages     = {n/a--n/a},
  doi       = {10.1002/sim.3873},
  url       = {https://doi.org/10.1002%2Fsim.3873},
  groups    = {Bayesian design},
  journal   = {Statistics in Medicine},
  publisher = {Wiley-Blackwell},
}

@Article{Avery2017,
  author    = {Kerry N L Avery and Paula R Williamson and Carrol Gamble and Elaine O'Connell Francischetto and Chris Metcalfe and Peter Davidson and Hywel Williams and Jane M Blazeby},
  title     = {Informing efficient randomised controlled trials: exploration of challenges in developing progression criteria for internal pilot studies},
  journal   = {{BMJ} Open},
  year      = {2017},
  volume    = {7},
  number    = {2},
  month     = {feb},
  pages     = {e013537},
  doi       = {10.1136/bmjopen-2016-013537},
  url       = {https://doi.org/10.1136%2Fbmjopen-2016-013537},
  groups    = {Other},
  publisher = {{BMJ}},
}

@Article{Baio2015a,
  author    = {Baio, Gianluca and Dawid, A Philip},
  title     = {Probabilistic sensitivity analysis in health economics},
  journal   = {Statistical methods in medical research},
  year      = {2015},
  volume    = {24},
  number    = {6},
  pages     = {615--634},
  groups    = {Bayesian design},
  publisher = {SAGE Publications Sage UK: London, England},
}

@Article{Bakhshi2013,
  author    = {Bakhshi, Andisheh and Senn, Stephen and Phillips, Alan},
  title     = {Some issues in predicting patient recruitment in multi-centre clinical trials},
  year      = {2013},
  volume    = {32},
  number    = {30},
  pages     = {5458--5468},
  doi       = {10.1002/sim.5979},
  groups    = {Recruitment},
  journal   = {Statistics in medicine},
  publisher = {Wiley Online Library},
}

@Article{Berger1987,
  author    = {Berger, James O and Sellke, Thomas},
  title     = {Testing a point null hypothesis: The irreconcilability of p values and evidence},
  year      = {1987},
  volume    = {82},
  number    = {397},
  pages     = {112--122},
  groups    = {Other},
  journal   = {Journal of the American statistical Association},
  publisher = {Taylor \& Francis},
}

@Article{Bryant2000a,
  author    = {John Bryant and Roger Day},
  title     = {[Clinical Trials and Sample Size Considerations: Another Perspective]: Comment},
  journal   = {Statistical Science},
  year      = {2000},
  volume    = {15},
  number    = {2},
  pages     = {106-108},
  issn      = {08834237},
  url       = {http://www.jstor.org/stable/2676723},
  groups    = {Bayesian design},
  publisher = {Institute of Mathematical Statistics},
}

@Article{Cellamare2014,
  author    = {Matteo Cellamare and Valeria Sambucini},
  title     = {A randomized two-stage design for phase {II} clinical trials based on a Bayesian predictive approach},
  year      = {2014},
  volume    = {34},
  number    = {6},
  month     = {dec},
  pages     = {1059--1078},
  doi       = {10.1002/sim.6396},
  url       = {https://doi.org/10.1002%2Fsim.6396},
  file      = {:Cellamare2014 - A randomized two-stage design for phase II clinical trials based on a Bayesian predictive approach.pdf:PDF},
  groups    = {Bayesian design},
  journal   = {Statistics in Medicine},
  publisher = {Wiley-Blackwell},
}

@Article{Chen2009,
  author    = {Cong Chen and Robert A. Beckman},
  title     = {Optimal Cost-Effective Go-No Go Decisions in Late-Stage Oncology Drug Development},
  year      = {2009},
  volume    = {1},
  number    = {2},
  month     = {may},
  pages     = {159--169},
  doi       = {10.1198/sbr.2009.0027},
  url       = {https://doi.org/10.1198%2Fsbr.2009.0027},
  groups    = {Bayesian design},
  journal   = {Statistics in Biopharmaceutical Research},
  publisher = {Informa {UK} Limited},
}

@Article{Chen2011a,
  author    = {Ming-Hui Chen and Joseph G. Ibrahim and Peter Lam and Alan Yu and Yuanye Zhang},
  title     = {Bayesian Design of Noninferiority Trials for Medical Devices Using Historical Data},
  journal   = {Biometrics},
  year      = {2011},
  volume    = {67},
  number    = {3},
  month     = {mar},
  pages     = {1163--1170},
  doi       = {10.1111/j.1541-0420.2011.01561.x},
  url       = {https://doi.org/10.1111%2Fj.1541-0420.2011.01561.x},
  groups    = {Bayesian design},
  publisher = {Wiley-Blackwell},
}

@Article{Chuang-Stein2006,
  author    = {Christy Chuang-Stein},
  title     = {Sample size and the probability of a successful trial},
  year      = {2006},
  volume    = {5},
  number    = {4},
  pages     = {305--309},
  doi       = {10.1002/pst.232},
  url       = {https://doi.org/10.1002%2Fpst.232},
  groups    = {Bayesian design},
  journal   = {Pharmaceutical Statistics},
  publisher = {Wiley-Blackwell},
}

@Article{Chuang-Stein2011,
  author    = {C. Chuang-Stein and S. Kirby and J. French and K. Kowalski and S. Marshall and M. K. Smith and P. Bycott and M. Beltangady},
  title     = {A Quantitative Approach for Making Go/No-Go Decisions in Drug Development},
  year      = {2011},
  volume    = {45},
  number    = {2},
  month     = {mar},
  pages     = {187--202},
  doi       = {10.1177/009286151104500213},
  url       = {https://doi.org/10.1177%2F009286151104500213},
  groups    = {Bayesian design},
  journal   = {Therapeutic Innovation {\&} Regulatory Science},
  publisher = {{SAGE} Publications},
}

@Article{Chuang-Stein2011a,
  author    = {Chuang-Stein, Christy and Kirby, Simon and Hirsch, Ian and Atkinson, Gary},
  title     = {The role of the minimum clinically important difference and its impact on designing a trial},
  year      = {2011},
  volume    = {10},
  number    = {3},
  pages     = {250--256},
  groups    = {Other},
  journal   = {Pharmaceutical statistics},
  publisher = {Wiley Online Library},
}

@Article{Cook2015,
  author   = {Cook, Jonathan A. and Hislop, Jenni and Altman, Douglas G. and Fayers, Peter and Briggs, Andrew H. and Ramsay, Craig R. and Norrie, John D. and Harvey, Ian M. and Buckley, Brian and Fergusson, Dean and Ford, Ian and Vale, Luke D.},
  title    = {Specifying the target difference in the primary outcome for a randomised controlled trial: guidance for researchers},
  year     = {2015},
  volume   = {16},
  number   = {1},
  pages    = {12},
  issn     = {1745-6215},
  doi      = {10.1186/s13063-014-0526-8},
  url      = {http://dx.doi.org/10.1186/s13063-014-0526-8},
  abstract = {Central to the design of a randomised controlled trial is the calculation of the number of participants needed. This is typically achieved by specifying a target difference and calculating the corresponding sample size, which provides reassurance that the trial will have the required statistical power (at the planned statistical significance level) to identify whether a difference of a particular magnitude exists. Beyond pure statistical or scientific concerns, it is ethically imperative that an appropriate number of participants should be recruited. Despite the critical role of the target difference for the primary outcome in the design of randomised controlled trials, its determination has received surprisingly little attention. This article provides guidance on the specification of the target difference for the primary outcome in a sample size calculation for a two parallel group randomised controlled trial with a superiority question.},
  groups   = {Other},
  journal  = {Trials},
}

@Article{DERSIMONIAN1996,
  author    = {DERSIMONIAN, REBECCA},
  title     = {META-ANALYSIS IN THE DESIGN AND MONITORING OF CLINICAL TRIALS},
  year      = {1996},
  volume    = {15},
  number    = {12},
  pages     = {1237--1248},
  doi       = {10.1002/(SICI)1097-0258(19960630)15:12<1237::AID-SIM301>3.0.CO;2-N},
  journal   = {Statistics in medicine},
  publisher = {Wiley Online Library},
}

@Article{Dignam1998,
  author    = {Dignam, James J and Bryant, John and Wieand, H Samuel and Fisher, Bernard and Wolmark, Norman},
  title     = {Early stopping of a clinical trial when there is evidence of no treatment benefit: protocol B-14 of the National Surgical Adjuvant Breast and Bowel Project},
  year      = {1998},
  volume    = {19},
  number    = {6},
  pages     = {575--588},
  doi       = {10.1016/S0197-2456(98)00041-5},
  journal   = {Controlled clinical trials},
  publisher = {Elsevier},
}

@Article{Eaton2013,
  author    = {Morris L. Eaton and Robb J. Muirhead and Adina I. Soaita},
  title     = {On the Limiting Behavior of the {\textquotedblleft}Probability of Claiming Superiority{\textquotedblright} in a Bayesian Context},
  year      = {2013},
  volume    = {8},
  number    = {1},
  month     = {mar},
  pages     = {221--232},
  doi       = {10.1214/13-ba809},
  url       = {https://doi.org/10.1214%2F13-ba809},
  groups    = {Bayesian design},
  journal   = {Bayesian Analysis},
  publisher = {Institute of Mathematical Statistics},
}

@Article{Fay2006,
  author    = {Michael P. Fay and M. Elizabeth Halloran and Dean A. Follmann},
  title     = {Accounting for Variability in Sample Size Estimation with Applications to Nonadherence and Estimation of Variance and Effect Size},
  year      = {2006},
  volume    = {63},
  number    = {2},
  month     = {dec},
  pages     = {465--474},
  doi       = {10.1111/j.1541-0420.2006.00703.x},
  url       = {https://doi.org/10.1111%2Fj.1541-0420.2006.00703.x},
  groups    = {Bayesian design},
  journal   = {Biometrics},
  publisher = {Wiley-Blackwell},
}

@Article{Fletcher1993,
  author    = {Fletcher, Astrid and Spiegelhalter, David and Staessen, Jan and Thijs, Lutgarde and Bulpitt, Christopher},
  title     = {Implications for trials in progress of publication of positive results},
  year      = {1993},
  volume    = {342},
  number    = {8872},
  pages     = {653--657},
  doi       = {10.1016/0140-6736(93)91762-B},
  journal   = {The Lancet},
  publisher = {Elsevier},
}

@Article{Freedman1994,
  author    = {Freedman, Laurence S and Spiegelhalter, David J and Parmar, Mahesh KB},
  title     = {The what, why and how of Bayesian clinical trials monitoring},
  year      = {1994},
  volume    = {13},
  number    = {13-14},
  pages     = {1371--1383},
  doi       = {10.1002/sim.4780131312},
  groups    = {Bayesian design},
  journal   = {Statistics in medicine},
  publisher = {Wiley Online Library},
}

@Article{Heitjan1997,
  author    = {Heitjan, Daniel F},
  title     = {Bayesian interim analysis of phase II cancer clinical trials},
  year      = {1997},
  volume    = {16},
  number    = {16},
  pages     = {1791--1802},
  doi       = {10.1002/(SICI)1097-0258(19970830)16:16<1791::AID-SIM609>3.0.CO;2-E},
  journal   = {Statistics in medicine},
  publisher = {Wiley Online Library},
}

@Article{Heitjan2015,
  author    = {Heitjan, Daniel F and Ge, Zhiyun and Ying, Gui-shuang},
  title     = {Real-time prediction of clinical trial enrollment and event counts: A review},
  year      = {2015},
  volume    = {45},
  pages     = {26--33},
  doi       = {10.1016/j.cct.2015.07.010},
  groups    = {Recruitment},
  journal   = {Contemporary clinical trials},
  publisher = {Elsevier},
}

@Article{Hong2012,
  author    = {Shengyan Hong and Li Shi},
  title     = {Predictive power to assist phase 3 go/no go decision based on phase 2 data on a different endpoint},
  year      = {2012},
  volume    = {31},
  number    = {9},
  month     = {feb},
  pages     = {831--843},
  doi       = {10.1002/sim.4476},
  url       = {https://doi.org/10.1002%2Fsim.4476},
  groups    = {Bayesian design},
  journal   = {Statistics in Medicine},
  publisher = {Wiley-Blackwell},
}

@Article{Ibrahim2014,
  author    = {Joseph G. Ibrahim and Ming-Hui Chen and Mani Lakshminarayanan and Guanghan F. Liu and Joseph F. Heyse},
  title     = {Bayesian probability of success for clinical trials using historical data},
  year      = {2014},
  volume    = {34},
  number    = {2},
  month     = {oct},
  pages     = {249--264},
  doi       = {10.1002/sim.6339},
  url       = {https://doi.org/10.1002%2Fsim.6339},
  groups    = {Bayesian design},
  journal   = {Statistics in Medicine},
  publisher = {Wiley-Blackwell},
}

@Article{Ioannidis2013,
  author   = {John P.A. Ioannidis and Iztok Hozo and Benjamin Djulbegovic},
  title    = {Optimal type I and type \{II\} error pairs when the available sample size is fixed},
  year     = {2013},
  volume   = {66},
  number   = {8},
  pages    = {903 - 910.e2},
  issn     = {0895-4356},
  doi      = {https://doi.org/10.1016/j.jclinepi.2013.03.002},
  url      = {http://www.sciencedirect.com/science/article/pii/S0895435613000887},
  groups   = {Other},
  journal  = {Journal of Clinical Epidemiology},
  keywords = {Type I error, Type \{II\} error, Power calculations, Optimization, False positive, False negative },
}

@Article{Jakobsen2014,
  author    = {Janus Christian Jakobsen and Christian Gluud and Per Winkel and Theis Lange and J{\o}rn Wetterslev},
  title     = {The thresholds for statistical and clinical significance {\textendash} a five-step procedure for evaluation of intervention effects in randomised clinical trials},
  year      = {2014},
  volume    = {14},
  number    = {1},
  month     = {mar},
  doi       = {10.1186/1471-2288-14-34},
  url       = {https://doi.org/10.1186%2F1471-2288-14-34},
  groups    = {Bayesian design},
  journal   = {{BMC} Medical Research Methodology},
  publisher = {Springer Nature},
}

@Article{Jiang2011,
  author    = {Kaihong Jiang},
  title     = {Optimal Sample Sizes and Go/No-Go Decisions for Phase {II}/{III} Development Programs Based on Probability of Success},
  year      = {2011},
  volume    = {3},
  number    = {3},
  month     = {aug},
  pages     = {463--475},
  doi       = {10.1198/sbr.2011.10068},
  url       = {https://doi.org/10.1198%2Fsbr.2011.10068},
  groups    = {Bayesian design},
  journal   = {Statistics in Biopharmaceutical Research},
  publisher = {Informa {UK} Limited},
}

@Article{Kadane1998,
  author    = {Kadane, Joseph and Wolfson, Lara J},
  title     = {Experiences in elicitation},
  year      = {1998},
  volume    = {47},
  number    = {1},
  pages     = {3--19},
  groups    = {Other},
  journal   = {Journal of the Royal Statistical Society: Series D (The Statistician)},
  publisher = {Wiley Online Library},
}

@Article{Kirby2012,
  author    = {Kirby, S and Burke, J and Chuang-Stein, C and Sin, C},
  title     = {Discounting phase 2 results when planning phase 3 clinical trials},
  journal   = {Pharmaceutical statistics},
  year      = {2012},
  volume    = {11},
  number    = {5},
  pages     = {373--385},
  doi       = {10.1002/pst.1521},
  groups    = {Bayesian design},
  publisher = {Wiley Online Library},
}

@Article{MLan2008,
  author    = {Cyr E. M{\textquotesingle}Lan and Lawrence Joseph and David B. Wolfson},
  title     = {Bayesian sample size determination for binomial proportions},
  year      = {2008},
  volume    = {3},
  number    = {2},
  month     = {jun},
  pages     = {269--296},
  doi       = {10.1214/08-ba310},
  url       = {https://doi.org/10.1214%2F08-ba310},
  groups    = {Bayesian design},
  journal   = {Bayesian Analysis},
  publisher = {Institute of Mathematical Statistics},
}

@Article{Maxwell2008,
  author    = {Scott E. Maxwell and Ken Kelley and Joseph R. Rausch},
  title     = {Sample Size Planning for Statistical Power and Accuracy in Parameter Estimation},
  year      = {2008},
  volume    = {59},
  number    = {1},
  month     = {jan},
  pages     = {537--563},
  doi       = {10.1146/annurev.psych.59.103006.093735},
  url       = {https://doi.org/10.1146%2Fannurev.psych.59.103006.093735},
  groups    = {Bayesian design},
  journal   = {Annual Review of Psychology},
  publisher = {Annual Reviews},
}

@Article{Mijoule2012,
  author    = {Mijoule, Guillaume and Savy, St{\'e}phanie and Savy, Nicolas},
  title     = {Models for patients' recruitment in clinical trials and sensitivity analysis},
  year      = {2012},
  volume    = {31},
  number    = {16},
  pages     = {1655--1674},
  doi       = {10.1002/sim.4495},
  groups    = {Recruitment},
  journal   = {Statistics in medicine},
  publisher = {Wiley Online Library},
}

@Article{Parmar1994,
  author    = {Parmar, Mahesh KB and Spiegelhalter, David J and Freedman, Laurence S},
  title     = {The CHART trials: Bayesian design and monitoring in practice},
  year      = {1994},
  volume    = {13},
  number    = {13-14},
  pages     = {1297--1312},
  doi       = {10.1002/sim.4780131304},
  journal   = {Statistics in medicine},
  publisher = {Wiley Online Library},
}

@Article{Patel2013,
  author    = {Patel, Nitin R and Ankolekar, Suresh and Antonijevic, Zoran and Rajicic, Natasa},
  title     = {A mathematical model for maximizing the value of phase 3 drug development portfolios incorporating budget constraints and risk},
  year      = {2013},
  volume    = {32},
  number    = {10},
  pages     = {1763--1777},
  doi       = {10.1002/sim.5731},
  groups    = {Bayesian design},
  journal   = {Statistics in medicine},
  publisher = {Wiley Online Library},
}

@Article{Roloff2012,
  author    = {Verena Roloff and Julian P.T. Higgins and Alex J. Sutton},
  title     = {Planning future studies based on the conditional power of a meta-analysis},
  year      = {2012},
  volume    = {32},
  number    = {1},
  month     = {jul},
  pages     = {11--24},
  doi       = {10.1002/sim.5524},
  url       = {https://doi.org/10.1002%2Fsim.5524},
  groups    = {Bayesian design},
  journal   = {Statistics in Medicine},
  publisher = {Wiley-Blackwell},
}

@Article{Sabin2014,
  author    = {Tony Sabin and James Matcham and Sarah Bray and Andrew Copas and Mahesh K. B. Parmar},
  title     = {A Quantitative Process for Enhancing End of Phase 2 Decisions},
  year      = {2014},
  volume    = {6},
  number    = {1},
  month     = {jan},
  pages     = {67--77},
  doi       = {10.1080/19466315.2013.852617},
  url       = {https://doi.org/10.1080%2F19466315.2013.852617},
  groups    = {Bayesian design},
  journal   = {Statistics in Biopharmaceutical Research},
  publisher = {Informa {UK} Limited},
}

@Article{Santis2007,
  author    = {Fulvio De Santis},
  title     = {Using historical data for Bayesian sample size determination},
  year      = {2007},
  volume    = {170},
  number    = {1},
  month     = {jan},
  pages     = {95--113},
  doi       = {10.1111/j.1467-985x.2006.00438.x},
  url       = {https://doi.org/10.1111%2Fj.1467-985x.2006.00438.x},
  groups    = {Bayesian design},
  journal   = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  publisher = {Wiley-Blackwell},
}

@Article{Staquet1979,
  author  = {Staquet, MJ and Rozencweig, Marcel and Von Hoff, DD and Muggia, FM},
  title   = {The delta and epsilon errors in the assessment of cancer clinical trials.},
  year    = {1979},
  volume  = {63},
  number  = {11-12},
  pages   = {1917--1921},
  groups  = {Other},
  journal = {Cancer treatment reports},
}

@Article{Sutton2007,
  author    = {Alexander J. Sutton and Nicola J. Cooper and David R. Jones and Paul C. Lambert and John R. Thompson and Keith R. Abrams},
  title     = {Evidence-based sample size calculations based upon updated meta-analysis},
  year      = {2007},
  volume    = {26},
  number    = {12},
  pages     = {2479--2500},
  doi       = {10.1002/sim.2704},
  url       = {https://doi.org/10.1002%2Fsim.2704},
  groups    = {Bayesian design},
  journal   = {Statistics in Medicine},
  publisher = {Wiley-Blackwell},
}

@Article{Tan2002a,
  author    = {Tan, SB and Machin, D and Tai, BC and Foo, KF and Tan, EH},
  title     = {A Bayesian re-assessment of two Phase II trials of gemcitabine in metastatic nasopharyngeal cancer},
  journal   = {British journal of cancer},
  year      = {2002},
  volume    = {86},
  number    = {6},
  pages     = {843--850},
  doi       = {10.1038/sj.bjc.6600199},
  publisher = {Nature Publishing Group},
}

@Article{Tang2012,
  author    = {Tang, Gong and Kong, Yuan and Chang, Chung-Chou Ho and Kong, Lan and Costantino, Joseph P},
  title     = {Prediction of accrual closure date in multi-center clinical trials with discrete-time Poisson process models},
  year      = {2012},
  volume    = {11},
  number    = {5},
  pages     = {351--356},
  doi       = {10.1002/pst.1506},
  groups    = {Recruitment},
  journal   = {Pharmaceutical statistics},
  publisher = {Wiley Online Library},
}

@Article{Vail2001,
  author    = {Vail, A and Hornbuckle, J and Spiegelhalter, DJ and Thornton, JG},
  title     = {Prospective application of Bayesian monitoring and analysis in an ‘open’randomized clinical trial},
  journal   = {Statistics in medicine},
  year      = {2001},
  subtitle  = {fd},
  volume    = {20},
  number    = {24},
  pages     = {3777--3787},
  doi       = {10.1002/sim.1171},
  publisher = {Wiley Online Library},
}

@Article{Wang2006,
  author    = {Wang, Sue-Jane and Hung, HM and O'Neill, Robert T},
  title     = {Adapting the sample size planning of a phase III trial based on phase II data},
  year      = {2006},
  volume    = {5},
  number    = {2},
  pages     = {85--97},
  doi       = {10.1002/pst.217},
  groups    = {Bayesian design},
  journal   = {Pharmaceutical Statistics},
  publisher = {Wiley Online Library},
}

@Article{Wang2013,
  author    = {Yanping Wang and Haoda Fu and Pandurang Kulkarni and Christopher Kaiser},
  title     = {Evaluating and utilizing probability of study success in clinical development},
  year      = {2013},
  volume    = {10},
  number    = {3},
  month     = {jun},
  pages     = {407--413},
  doi       = {10.1177/1740774513478229},
  url       = {https://doi.org/10.1177%2F1740774513478229},
  groups    = {Bayesian design},
  journal   = {Clinical Trials},
  publisher = {{SAGE} Publications},
}

@Article{Whitehead2008,
  author    = {John Whitehead and Elsa Vald{\'{e}}s-M{\'{a}}rquez and Patrick Johnson and Gordon Graham},
  title     = {Bayesian sample size for exploratory clinical trials incorporating historical data},
  year      = {2008},
  volume    = {27},
  number    = {13},
  pages     = {2307--2327},
  doi       = {10.1002/sim.3140},
  url       = {https://doi.org/10.1002%2Fsim.3140},
  groups    = {Bayesian design},
  journal   = {Statistics in Medicine},
  publisher = {Wiley-Blackwell},
}

@Article{Wittes2002,
  author    = {Wittes, Janet},
  title     = {Sample size calculations for randomized controlled trials},
  year      = {2002},
  volume    = {24},
  number    = {1},
  pages     = {39--53},
  url       = {https://pdfs.semanticscholar.org/7f7a/5347088aa8e01f4258d15a672ec9cb8d80df.pdf},
  groups    = {Bayesian design, Other},
  journal   = {Epidemiologic Reviews},
  publisher = {Oxford University Press},
}

@Article{Zhang2013,
  author    = {Jianliang Zhang and Jenny J. Zhang},
  title     = {Joint probability of statistical success of multiple phase {III} trials},
  year      = {2013},
  volume    = {12},
  number    = {6},
  month     = {sep},
  pages     = {358--365},
  doi       = {10.1002/pst.1597},
  url       = {https://doi.org/10.1002%2Fpst.1597},
  groups    = {Bayesian design},
  journal   = {Pharmaceutical Statistics},
  publisher = {Wiley-Blackwell},
}

@Article{Zhang2012,
  author    = {Zhang, Xiaoxi and Long, Qi},
  title     = {Modeling and prediction of subject accrual and event times in clinical trials: a systematic review},
  year      = {2012},
  volume    = {9},
  number    = {6},
  pages     = {681--688},
  doi       = {10.1177/1740774512447996},
  groups    = {Recruitment},
  journal   = {Clinical Trials},
  publisher = {SAGE Publications Sage UK: London, England},
}

@Article{Zhang2010,
  author    = {Zhang, Xiaoxi and Long, Qi},
  title     = {Stochastic modeling and prediction for accrual in clinical trials},
  year      = {2010},
  volume    = {29},
  number    = {6},
  pages     = {649--658},
  doi       = {10.1002/sim.3847},
  groups    = {Recruitment},
  journal   = {Statistics in Medicine},
  publisher = {Wiley Online Library},
}

@Article{Ventz2015,
  author   = {Ventz, Steffen and Trippa, Lorenzo},
  title    = {Bayesian designs and the control of frequentist characteristics: A practical solution},
  journal  = {Biometrics},
  year     = {2015},
  volume   = {71},
  number   = {1},
  pages    = {218--226},
  issn     = {1541-0420},
  doi      = {10.1111/biom.12226},
  url      = {http://dx.doi.org/10.1111/biom.12226},
  abstract = {
Frequentist concepts, such as the control of the type I error or the false discovery rate, are well established in the medical literature and often required by regulators. Most Bayesian designs are defined without explicit considerations of frequentist characteristics. Once the Bayesian design is structured, statisticians use simulations and adjust tuning parameters to comply with a set of targeted operating characteristics. These adjustments affect the use of prior information and utility functions. Here we consider a Bayesian decision theoretic approach for experimental designs with explicit frequentist requisites. We define optimal designs under a set of constraints required by a regulator. Our approach combines the use of interpretable utility functions with frequentist criteria, and selects an optimal design that satisfies a set of required operating characteristics. We illustrate the approach using a group-sequential multi-arm Phase II trial and a bridging trial.
},
  keywords = {Bayesian design, Clinical trials, Decision theory, Frequentist constraints},
}

@Misc{FDA2017,
  author = {FDA},
  title  = {Multiple Endpoints in Clinical Trials: Guidance for Industry},
  year   = {2017},
  url    = {https://www.fda.gov/downloads/Drugs/GuidanceComplianceRegulatoryInformation/Guidances/UCM536750.pdf},
}

@Article{Moore2011,
  author    = {Moore, Charity G. and Carter, Rickey E. and Nietert, Paul J. and Stewart, Paul W.},
  title     = {Recommendations for Planning Pilot Studies in Clinical and Translational Research},
  journal   = {Clinical and Translational Science},
  year      = {2011},
  volume    = {4},
  number    = {5},
  pages     = {332--337},
  issn      = {1752-8062},
  doi       = {10.1111/j.1752-8062.2011.00347.x},
  url       = {http://dx.doi.org/10.1111/j.1752-8062.2011.00347.x},
  groups    = {Pilot/feasibility},
  keywords  = {pilot studies, pilot study design, sample size, power calculations, confidence intervals},
  publisher = {Blackwell Publishing Inc},
}

@Article{Hampson2017,
  author   = {Lisa V Hampson and Paula R Williamson and Martin J Wilby and Thomas Jaki},
  title    = {A framework for prospectively defining progression rules for internal pilot studies monitoring recruitment},
  journal  = {Statistical Methods in Medical Research},
  year     = {2017},
  volume   = {0},
  number   = {0},
  pages    = {0962280217708906},
  note     = {PMID: 28589752},
  doi      = {10.1177/0962280217708906},
  eprint   = {http://dx.doi.org/10.1177/0962280217708906},
  url      = { 
        http://dx.doi.org/10.1177/0962280217708906
    
},
  abstract = { Just over half of publicly funded trials recruit their target sample size within the planned study duration. When recruitment targets are missed, the funder of a trial is faced with the decision of either committing further resources to the study or risk that a worthwhile treatment effect may be missed by an underpowered final analysis. To avoid this challenging situation, when there is insufficient prior evidence to support predicted recruitment rates, funders now require feasibility assessments to be performed in the early stages of trials. Progression criteria are usually specified and agreed with the funder ahead of time. To date, however, the progression rules used are typically ad hoc. In addition, rules routinely permit adaptations to recruitment strategies but do not stipulate criteria for evaluating their effectiveness. In this paper, we develop a framework for planning and designing internal pilot studies which permit a trial to be stopped early if recruitment is disappointing or to continue to full recruitment if enrolment during the feasibility phase is adequate. This framework enables a progression rule to be pre-specified and agreed upon prior to starting a trial. The novel two-stage designs stipulate that if neither of these situations arises, adaptations to recruitment should be made and subsequently evaluated to establish whether they have been successful. We derive optimal progression rules for internal pilot studies which minimise the expected trial overrun and maintain a high probability of completing the study when the recruitment rate is adequate. The advantages of this procedure are illustrated using a real trial example. },
}

@Book{DeGroot1970,
  author = {Morris H. DeGroot},
  title  = {Optimal Statistical Decisions},
  year   = {1970},
}

@Article{Strong2015,
  author    = {Strong, Mark and Oakley, Jeremy E and Brennan, Alan and Breeze, Penny},
  title     = {Estimating the expected value of sample information using the probabilistic sensitivity analysis sample: a fast, nonparametric regression-based method},
  journal   = {Medical Decision Making},
  year      = {2015},
  volume    = {35},
  number    = {5},
  pages     = {570--583},
  doi       = {10.1177/0272989x15575286},
  publisher = {SAGE Publications Sage CA: Los Angeles, CA},
}

@Article{Francischetto2013,
  author    = {Francischetto, Elaine O'Connell and Avery, Kerry and Metcalfe, Chris and Williamson, Paula and Gamble, Carrol and Blazeby, Jane},
  title     = {Optimising the design and evaluation of pilot work to inform the main trial: a review of current evidence and consideration of future practices},
  journal   = {Trials},
  year      = {2013},
  volume    = {14},
  number    = {1},
  pages     = {O17},
  doi       = {10.1186/1745-6215-14-s1-o17},
  publisher = {BioMed Central},
}

@InBook{Muirhead2013,
  author    = {Muirhead, Robb J. and ?oaita, Adina I.},
  title     = {On an Approach to Bayesian Sample Sizing in Clinical Trials},
  booktitle = {Advances in Modern Statistical Theory and Applications: A Festschrift in honor of Morris L. Eaton},
  year      = {2013},
  volume    = {Volume 10},
  series    = {Collections},
  publisher = {Institute of Mathematical Statistics},
  pages     = {126--137},
  doi       = {10.1214/12-IMSCOLL1007},
  url       = {http://dx.doi.org/10.1214/12-IMSCOLL1007},
  address   = {Beachwood, Ohio, USA},
}

@Article{Lindley1997a,
  author    = {Lindley, Dennis V.},
  title     = {The choice of sample size—a reply to the discussion},
  journal   = {Journal of the Royal Statistical Society: Series D (The Statistician)},
  year      = {1997},
  volume    = {46},
  number    = {2},
  pages     = {163--166},
  issn      = {1467-9884},
  doi       = {10.1111/1467-9884.00073},
  url       = {http://dx.doi.org/10.1111/1467-9884.00073},
  publisher = {Blackwell Publishers Ltd},
}

@Article{Whitehead1993,
  author    = {Whitehead, John},
  title     = {The case for frequentism in clinical trials},
  journal   = {Statistics in Medicine},
  year      = {1993},
  volume    = {12},
  number    = {15-16},
  pages     = {1405--1413},
  issn      = {1097-0258},
  doi       = {10.1002/sim.4780121506},
  url       = {http://dx.doi.org/10.1002/sim.4780121506},
  publisher = {Wiley Subscription Services, Inc., A Wiley Company},
}

@Article{Kinnersley2013,
  author    = {Kinnersley, Nelson and Day, Simon},
  title     = {Structured approach to the elicitation of expert beliefs for a Bayesian-designed clinical?trial: a case study},
  journal   = {Pharmaceutical Statistics},
  year      = {2013},
  volume    = {12},
  number    = {2},
  pages     = {104--113},
  issn      = {1539-1612},
  doi       = {10.1002/pst.1552},
  url       = {http://dx.doi.org/10.1002/pst.1552},
  keywords  = {Bayesian, elicitation, beliefs, expert opinion, feasibility},
  publisher = {John Wiley \& Sons, Ltd},
}

@Article{Teramukai2012,
  author   = {Teramukai, Satoshi and Daimon, Takashi and Zohar, Sarah},
  title    = {A Bayesian predictive sample size selection design for single-arm exploratory clinical trials},
  journal  = {Statistics in Medicine},
  year     = {2012},
  volume   = {31},
  number   = {30},
  pages    = {4243--4254},
  issn     = {1097-0258},
  doi      = {10.1002/sim.5505},
  url      = {http://dx.doi.org/10.1002/sim.5505},
  keywords = {Bayesian approach, adaptive design, analysis and design priors, prior predictive distributions, interim monitoring},
}

@Article{Cotterill2015,
  author   = {Cotterill, Amy and Whitehead, John},
  title    = {Bayesian methods for setting sample sizes and choosing allocation ratios in phase II clinical trials with time-to-event endpoints},
  journal  = {Statistics in Medicine},
  year     = {2015},
  volume   = {34},
  number   = {11},
  pages    = {1889--1903},
  note     = {sim.6426},
  issn     = {1097-0258},
  doi      = {10.1002/sim.6426},
  url      = {http://dx.doi.org/10.1002/sim.6426},
  keywords = {Phase II trial, proportional hazards model, sample size calculation, time-to-event endpoint, Bayesian framework},
}

@Article{Berry1993,
  author    = {Berry, Donald A.},
  title     = {A case for bayesianism in clinical trials},
  journal   = {Statistics in Medicine},
  year      = {1993},
  volume    = {12},
  number    = {15-16},
  pages     = {1377--1393},
  issn      = {1097-0258},
  doi       = {10.1002/sim.4780121504},
  url       = {http://dx.doi.org/10.1002/sim.4780121504},
  publisher = {Wiley Subscription Services, Inc., A Wiley Company},
}

@Article{Parmar2016,
  author   = {Parmar, Mahesh K. B. and Sydes, Matthew R. and Morris, Tim P.},
  title    = {How do you design randomised trials for smaller populations? A framework},
  journal  = {BMC Medicine},
  year     = {2016},
  volume   = {14},
  number   = {1},
  month    = {Nov},
  pages    = {183},
  issn     = {1741-7015},
  doi      = {10.1186/s12916-016-0722-3},
  url      = {https://doi.org/10.1186/s12916-016-0722-3},
  abstract = {How should we approach trial design when we can get some, but not all, of the way to the numbers required for a randomised phase III trial?},
  day      = {25},
}

@Article{Beavers2012,
  author   = {Daniel P. Beavers and James D. Stamey},
  title    = {Bayesian sample size determination for binary regression with a misclassified covariate and no gold standard},
  journal  = {Computational Statistics \& Data Analysis},
  year     = {2012},
  volume   = {56},
  number   = {8},
  pages    = {2574 - 2582},
  issn     = {0167-9473},
  doi      = {http://dx.doi.org/10.1016/j.csda.2012.02.014},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167947312000965},
  abstract = {Covariate misclassification is a common problem in epidemiology, genetics, and other biomedical areas. Because this form of misclassification is known to bias estimators, accounting for it at the design stage is of high importance. In this paper, we extend on previous work applied to response misclassification by developing a Bayesian approach to sample size determination for a covariate misclassification model with no gold standard. Our procedure considers both conditionally independent tests and tests in which dependence exists between classifiers. We specifically consider a Bayesian power criterion for the sample size determination scheme, and we demonstrate the improvement in model power for our dual classifier approach compared to a naïve single classifier approach.},
  keywords = {Logistic regression, Misclassification, Bias, Sample size, Covariate},
}

@Article{Lee2012,
  author    = {Lee, J. Jack and Chu, Caleb T.},
  title     = {Bayesian clinical trials in action},
  journal   = {Statistics in Medicine},
  year      = {2012},
  volume    = {31},
  number    = {25},
  pages     = {2955--2972},
  issn      = {1097-0258},
  doi       = {10.1002/sim.5404},
  url       = {http://dx.doi.org/10.1002/sim.5404},
  keywords  = {adaptive trial design, Bayesian paradigm, clinical trial conduct, frequentist paradigm, trial efficiency, trial ethics},
  publisher = {John Wiley \& Sons, Ltd},
}

@Article{Zaslavsky2013,
  author   = {Zaslavsky, Boris G.},
  title    = {Bayesian Hypothesis Testing in Two-Arm Trials with Dichotomous Outcomes},
  journal  = {Biometrics},
  year     = {2013},
  volume   = {69},
  number   = {1},
  pages    = {157--163},
  issn     = {1541-0420},
  doi      = {10.1111/j.1541-0420.2012.01806.x},
  url      = {http://dx.doi.org/10.1111/j.1541-0420.2012.01806.x},
  keywords = {Credible limits, Hypothesis testing, Markov Chain Monte Carlo method, Posterior distribution, p-Value},
}

@Article{Lewis2007,
  author   = {Roger J Lewis and Ari M Lipsky and Donald A Berry},
  title    = {Bayesian decision-theoretic group sequential clinical trial design based on a quadratic loss function: a frequentist evaluation},
  journal  = {Clinical Trials},
  year     = {2007},
  volume   = {4},
  number   = {1},
  pages    = {5-14},
  note     = {PMID: 17327241},
  doi      = {10.1177/1740774506075764},
  eprint   = {http://dx.doi.org/10.1177/1740774506075764},
  url      = { 
        http://dx.doi.org/10.1177/1740774506075764
    
},
  abstract = { The decision to terminate a controlled clinical trial at the time of an interim analysis is perhaps best made by weighing the value of the likely additional information to be gained if further subjects are enrolled against the various costs of that further enrollment. The most commonly used statistical plans for interim analysis (eg, O'Brien–Fleming), however, are based on a frequentist approach that makes no such comparison. A two-armed Bayesian decision-theoretic clinical trial design is developed for a disease with two possible outcomes, incorporating a quadratic decision loss function and using backward induction to quantify the cost of future enrollment. Monte Carlo simulation is used to compare frequentist error rates and mean required sample sizes for these Bayesian designs with the two-tailed frequentist group-sequential designs of, O'Brien–Fleming and Pocock. When the terminal decision loss function is chosen to yield typical frequentist error rates, the mean sample sizes required by the Bayesian designs are smaller than those of the corresponding O'Brien–Fleming frequentist designs, largely due to the more frequent interim analyses typically used with the Bayesian designs and the ability of the Bayesian designs to terminate early and conclude equivalence. Adding stochastic curtailment to the frequentist designs and using the same number of interim analyses results in largely equivalent trials. An example of a Bayesian design for the data safety monitoring of a clinical trial is given. Our design assumes independence of the probabilities of success in the two trial arms. Additionally, we have chosen non-informative priors and selected loss functions to produce trials with appealing frequentist error rates, rather than choosing priors that reflect realistic prior information and loss functions that reflect true costs. Our Bayesian designs allow interpretation of the final results along either Bayesian or frequentist lines. For the Bayesian, they minimize the total cost and allow the direct calculation of the probability density function for the difference in efficacy. For the frequentist, they have well-characterized type I and II error rates and in some cases lead to a reduction in the mean sample size. },
}

@Article{Johnson2013,
  author   = {Johnson, Valen E.},
  title    = {Revised standards for statistical evidence},
  journal  = {Proceedings of the National Academy of Sciences},
  year     = {2013},
  volume   = {110},
  number   = {48},
  pages    = {19313-19317},
  doi      = {10.1073/pnas.1313476110},
  eprint   = {http://www.pnas.org/content/110/48/19313.full.pdf},
  url      = {http://www.pnas.org/content/110/48/19313.abstract},
  abstract = {Recent advances in Bayesian hypothesis testing have led to the development of uniformly most powerful Bayesian tests, which represent an objective, default class of Bayesian hypothesis tests that have the same rejection regions as classical significance tests. Based on the correspondence between these two classes of tests, it is possible to equate the size of classical hypothesis tests with evidence thresholds in Bayesian tests, and to equate P values with Bayes factors. An examination of these connections suggest that recent concerns over the lack of reproducibility of scientific studies can be attributed largely to the conduct of significance tests at unjustifiably high levels of significance. To correct this problem, evidence thresholds required for the declaration of a significant finding should be increased to 25–50:1, and to 100–200:1 for the declaration of a highly significant finding. In terms of classical hypothesis tests, these evidence standards mandate the conduct of tests at the 0.005 or 0.001 level of significance.},
}

@Article{Bernardo1980,
  author   = {Bernardo, Jos{\'e} M.},
  title    = {A Bayesian analysis of classical hypothesis testing},
  journal  = {Trabajos de Estadistica Y de Investigacion Operativa},
  year     = {1980},
  volume   = {31},
  number   = {1},
  month    = {Feb},
  pages    = {605--647},
  issn     = {0041-0241},
  doi      = {10.1007/BF02888370},
  url      = {https://doi.org/10.1007/BF02888370},
  abstract = {The procedure of maximizing the missing information is applied to derive reference posterior probabilities for null hypotheses. The results shed further light on Lindley's paradox and suggest that a Bayesian interpretation of classical hypothesis testing is possible by providing a one-to-one approximate relationship between significance levels and posterior probabilities.},
  day      = {01},
}

@Article{DelPaggio2017,
  author  = {Del Paggio, J. C. and Azariah, B. and Sullivan, R. and Hopman, W. M. and James, F. V. and Roshni, S. and Tannock, I. F. and Booth, C. M.},
  title   = {Do Contemporary Randomized Controlled Trials Meet ESMO Thresholds for Meaningful Clinical Benefit?},
  journal = {Annals of Oncology},
  year    = {2017},
  volume  = {28},
  number  = {1},
  pages   = {157-162},
  doi     = {10.1093/annonc/mdw538},
  eprint  = {/oup/backfile/content_public/journal/annonc/28/1/10.1093_annonc_mdw538/2/mdw538.pdf},
  url     = { + http://dx.doi.org/10.1093/annonc/mdw538},
}

@Article{Cohen1994,
  author  = {Jacob Cohen},
  title   = {The Earth is round (p < .05)},
  journal = {American Psychologist},
  year    = {1994},
  volume  = {49},
  number  = {12},
  pages   = {997-1003},
  url     = {http://ist-socrates.berkeley.edu/~maccoun/PP279_Cohen1.pdf},
}

@Article{Bates2015,
  author  = {Douglas Bates and Martin M{\"a}chler and Ben Bolker and Steve Walker},
  title   = {Fitting Linear Mixed-Effects Models Using {lme4}},
  journal = {Journal of Statistical Software},
  year    = {2015},
  volume  = {67},
  number  = {1},
  pages   = {1--48},
  doi     = {10.18637/jss.v067.i01},
}

@Article{Dong2012,
  author    = {Dong, Gaohong and Shih, Weichung Joe and Moore, Dirk and Quan, Hui and Marcella, Stephen},
  title     = {A Bayesian–frequentist two-stage single-arm phase II clinical trial design},
  journal   = {Statistics in Medicine},
  year      = {2012},
  volume    = {31},
  number    = {19},
  pages     = {2055--2067},
  issn      = {1097-0258},
  doi       = {10.1002/sim.5330},
  url       = {http://dx.doi.org/10.1002/sim.5330},
  keywords  = {Bayesian Type I and Type II error rates, frequentist Type I and Type II error rates, predictive probability, probability of early termination, expected sample size},
  publisher = {John Wiley \& Sons, Ltd},
}

@Article{Haynes2016,
  author    = {Mary E. Haynes and Roy T. Sabo and N. Rao Chaganty},
  title     = {Simulating dependent binary variables through multinomial sampling},
  journal   = {Journal of Statistical Computation and Simulation},
  year      = {2016},
  volume    = {86},
  number    = {3},
  pages     = {510-523},
  doi       = {10.1080/00949655.2015.1020313},
  eprint    = {http://dx.doi.org/10.1080/00949655.2015.1020313},
  url       = { 
        http://dx.doi.org/10.1080/00949655.2015.1020313
    
},
  abstract  = { A new method for simulating dependent binary data is presented. This methodology does not require specification of a parametric joint distribution, and instead uses desired dependence levels and marginal probabilities to construct a cumulative distribution function for the multinomial distribution of all possible combinations of binary outcomes. This method is simple to use and compute, and through simulation studies is found to be comparable with the gold-standard approach by Emrich and Piedmonte [A method for generating high-dimensional multivariate binary variates. Amer Statist. 1991;45(4):302–304]. },
  publisher = {Taylor \& Francis},
}

@Article{Chaganty2006,
  author  = {Chaganty, N. Rao and Joe, Harry},
  title   = {Range of correlation matrices for dependent Bernoulli random variables},
  journal = {Biometrika},
  year    = {2006},
  volume  = {93},
  number  = {1},
  pages   = {197-206},
  doi     = {10.1093/biomet/93.1.197},
  eprint  = {/oup/backfile/content_public/journal/biomet/93/1/10.1093/biomet/93.1.197/2/931197.pdf},
  url     = { + http://dx.doi.org/10.1093/biomet/93.1.197},
}

@Article{Binois2015,
  author   = {M. Binois and D. Ginsbourger and O. Roustant},
  title    = {Quantifying uncertainty on Pareto fronts with Gaussian process conditional simulations},
  journal  = {European Journal of Operational Research},
  year     = {2015},
  volume   = {243},
  number   = {2},
  pages    = {386 - 394},
  issn     = {0377-2217},
  doi      = {https://doi.org/10.1016/j.ejor.2014.07.032},
  url      = {http://www.sciencedirect.com/science/article/pii/S0377221714005980},
  abstract = {Abstract Multi-objective optimization algorithms aim at finding Pareto-optimal solutions. Recovering Pareto fronts or Pareto sets from a limited number of function evaluations are challenging problems. A popular approach in the case of expensive-to-evaluate functions is to appeal to metamodels. Kriging has been shown efficient as a base for sequential multi-objective optimization, notably through infill sampling criteria balancing exploitation and exploration such as the Expected Hypervolume Improvement. Here we consider Kriging metamodels not only for selecting new points, but as a tool for estimating the whole Pareto front and quantifying how much uncertainty remains on it at any stage of Kriging-based multi-objective optimization algorithms. Our approach relies on the Gaussian process interpretation of Kriging, and bases upon conditional simulations. Using concepts from random set theory, we propose to adapt the Vorob’ev expectation and deviation to capture the variability of the set of non-dominated points. Numerical experiments illustrate the potential of the proposed workflow, and it is shown on examples how Gaussian process simulations and the estimated Vorob’ev deviation can be used to monitor the ability of Kriging-based multi-objective optimization algorithms to accurately learn the Pareto front.},
  keywords = {Multi-objective optimization, Attainment function, Vorob’ev expectation, Expected Hypervolume Improvement, Kriging},
}

@InProceedings{Emmerich2011,
  author       = {Emmerich, Michael TM and Deutz, Andr{\'e} H and Klinkenberg, Jan Willem},
  title        = {Hypervolume-based expected improvement: Monotonicity properties and exact computation},
  booktitle    = {Evolutionary Computation (CEC), 2011 IEEE Congress on},
  year         = {2011},
  organization = {IEEE},
  pages        = {2147--2154},
  doi          = {10.1109/cec.2011.5949880},
}

@Article{Hoorde2015,
  author   = {K. Van Hoorde and S. Van Huffel and D. Timmerman and T. Bourne and B. Van Calster},
  title    = {A spline-based tool to assess and visualize the calibration of multiclass risk predictions},
  journal  = {Journal of Biomedical Informatics},
  year     = {2015},
  volume   = {54},
  number   = {Supplement C},
  pages    = {283 - 293},
  issn     = {1532-0464},
  doi      = {https://doi.org/10.1016/j.jbi.2014.12.016},
  url      = {http://www.sciencedirect.com/science/article/pii/S1532046415000027},
  abstract = {Abstract When validating risk models (or probabilistic classifiers), calibration is often overlooked. Calibration refers to the reliability of the predicted risks, i.e. whether the predicted risks correspond to observed probabilities. In medical applications this is important because treatment decisions often rely on the estimated risk of disease. The aim of this paper is to present generic tools to assess the calibration of multiclass risk models. We describe a calibration framework based on a vector spline multinomial logistic regression model. This framework can be used to generate calibration plots and calculate the estimated calibration index (ECI) to quantify lack of calibration. We illustrate these tools in relation to risk models used to characterize ovarian tumors. The outcome of the study is the surgical stage of the tumor when relevant and the final histological outcome, which is divided into five classes: benign, borderline malignant, stage I, stage II–IV, and secondary metastatic cancer. The 5909 patients included in the study are randomly split into equally large training and test sets. We developed and tested models using the following algorithms: logistic regression, support vector machines, k nearest neighbors, random forest, naive Bayes and nearest shrunken centroids. Multiclass calibration plots are interesting as an approach to visualizing the reliability of predicted risks. The ECI is a convenient tool for comparing models, but is less informative and interpretable than calibration plots. In our case study, logistic regression and random forest showed the highest degree of calibration, and the naive Bayes the lowest.},
  keywords = {Risk models, Probability estimation, Machine learning, Logistic regression, Calibration, Multiclass},
}

@Article{VanHoorde2014,
  author   = {Van Hoorde, Kirsten and Vergouwe, Yvonne and Timmerman, Dirk and Van Huffel, Sabine and Steyerberg, Ewout W. and Van Calster, Ben},
  title    = {Assessing calibration of multinomial risk prediction models},
  journal  = {Statistics in Medicine},
  year     = {2014},
  volume   = {33},
  number   = {15},
  pages    = {2585--2596},
  issn     = {1097-0258},
  doi      = {10.1002/sim.6114},
  url      = {http://dx.doi.org/10.1002/sim.6114},
  keywords = {risk prediction model, polytomous diagnosis, calibration, calibration plot},
}

@Article{Steyerberg2004,
  author    = {Steyerberg, Ewout W. and Borsboom, Gerard J. J. M. and van Houwelingen, Hans C. and Eijkemans, Marinus J. C. and Habbema, J. Dik F.},
  title     = {Validation and updating of predictive logistic regression models: a study on sample size and shrinkage},
  journal   = {Statistics in Medicine},
  year      = {2004},
  volume    = {23},
  number    = {16},
  pages     = {2567--2586},
  issn      = {1097-0258},
  doi       = {10.1002/sim.1844},
  url       = {http://dx.doi.org/10.1002/sim.1844},
  keywords  = {logistic regression, validation, updating, shrinkage},
  publisher = {John Wiley \& Sons, Ltd.},
}

@Article{Debray2015,
  author   = {Thomas P.A. Debray and Yvonne Vergouwe and Hendrik Koffijberg and Daan Nieboer and Ewout W. Steyerberg and Karel G.M. Moons},
  title    = {A new framework to enhance the interpretation of external validation studies of clinical prediction models},
  journal  = {Journal of Clinical Epidemiology},
  year     = {2015},
  volume   = {68},
  number   = {3},
  pages    = {279 - 289},
  issn     = {0895-4356},
  doi      = {https://doi.org/10.1016/j.jclinepi.2014.06.018},
  url      = {http://www.sciencedirect.com/science/article/pii/S0895435614002753},
  abstract = {Abstract It is widely acknowledged that the performance of diagnostic and prognostic prediction models should be assessed in external validation studies with independent data from “different but related” samples as compared with that of the development sample. We developed a framework of methodological steps and statistical methods for analyzing and enhancing the interpretation of results from external validation studies of prediction models. We propose to quantify the degree of relatedness between development and validation samples on a scale ranging from reproducibility to transportability by evaluating their corresponding case-mix differences. We subsequently assess the models' performance in the validation sample and interpret the performance in view of the case-mix differences. Finally, we may adjust the model to the validation setting. We illustrate this three-step framework with a prediction model for diagnosing deep venous thrombosis using three validation samples with varying case mix. While one external validation sample merely assessed the model's reproducibility, two other samples rather assessed model transportability. The performance in all validation samples was adequate, and the model did not require extensive updating to correct for miscalibration or poor fit to the validation settings. The proposed framework enhances the interpretation of findings at external validation of prediction models.},
  keywords = {Case mix, Reproducibility, Transportability, Generalizability, Prediction model, Validation},
}

@Article{VanCalster2017,
  author   = {Van Calster, Ben and Van Hoorde, Kirsten and Vergouwe, Yvonne and Bobdiwala, Shabnam and Condous, George and Kirk, Emma and Bourne, Tom and Steyerberg, Ewout W.},
  title    = {Validation and updating of risk models based on multinomial logistic regression},
  journal  = {Diagnostic and Prognostic Research},
  year     = {2017},
  volume   = {1},
  number   = {1},
  month    = {Feb},
  pages    = {2},
  issn     = {2397-7523},
  doi      = {10.1186/s41512-016-0002-x},
  url      = {https://doi.org/10.1186/s41512-016-0002-x},
  abstract = {Risk models often perform poorly at external validation in terms of discrimination or calibration. Updating methods are needed to improve performance of multinomial logistic regression models for risk prediction.},
  day      = {08},
}

@Article{Genders2011,
  author  = {Genders, Tessa S.S. and Steyerberg, Ewout W. and Alkadhi, Hatem and Leschka, Sebastian and Desbiolles, Lotus and Nieman, Koen and Galema, Tjebbe W. and Meijboom, W. Bob and Mollet, Nico R. and de Feyter, Pim J. and Cademartiri, Filippo and Maffei, Erica and Dewey, Marc and Zimmermann, Elke and Laule, Michael and Pugliese, Francesca and Barbagallo, Rossella and Sinitsyn, Valentin and Bogaert, Jan and Goetschalckx, Kaatje and Schoepf, U. Joseph and Rowe, Garrett W. and Schuijf, Joanne D. and Bax, Jeroen J. and de Graaf, Fleur R. and Knuuti, Juhani and Kajander, Sami and van Mieghem, Carlos A.G. and Meijs, Matthijs F.L. and Cramer, Maarten J. and Gopalan, Deepa and Feuchtner, Gudrun and Friedrich, Guy and Krestin, Gabriel P. and Hunink, M.G. Myriam and ,},
  title   = {A clinical prediction rule for the diagnosis of coronary artery disease: validation, updating, and extension},
  journal = {European Heart Journal},
  year    = {2011},
  volume  = {32},
  number  = {11},
  pages   = {1316-1330},
  doi     = {10.1093/eurheartj/ehr014},
  eprint  = {/oup/backfile/content_public/journal/eurheartj/32/11/10.1093_eurheartj_ehr014/2/ehr014.pdf},
  url     = {http://dx.doi.org/10.1093/eurheartj/ehr014},
}

@Article{Genders2012,
  author    = {Genders, Tessa S S and Steyerberg, Ewout W and Hunink, M G Myriam and Nieman, Koen and Galema, Tjebbe W and Mollet, Nico R and Feyter, Pim J de and Krestin, Gabriel P and Alkadhi, Hatem and Leschka, Sebastian and Desbiolles, Lotus and Meijs, Matthijs F L and Cramer, Maarten J and Knuuti, Juhani and Kajander, Sami and Bogaert, Jan and Goetschalckx, Kaatje and Cademartiri, Filippo and Maffei, Erica and Martini, Chiara and Seitun, Sara and Aldrovandi, Annachiara and Wildermuth, Simon and Stinn, Bj{\"o}rn and Fornaro, J{\"u}rgen and Feuchtner, Gudrun and De Zordo, Tobias and Auer, Thomas and Plank, Fabian and Friedrich, Guy and Pugliese, Francesca and Petersen, Steffen E and Davies, L Ceri and Schoepf, U Joseph and Rowe, Garrett W and van Mieghem, Carlos A G and van Driessche, Luc and Sinitsyn, Valentin and Gopalan, Deepa and Nikolaou, Konstantin and Bamberg, Fabian and Cury, Ricardo C and Battle, Juan and Maurovich-Horvat, P{\'a}l and Bartykowszki, Andrea and Merkely, Bela and Becker, D{\'a}vid and Hadamitzky, Martin and Hausleiter, J{\"o}rg and Dewey, Marc and Zimmermann, Elke and Laule, Michael},
  title     = {Prediction model to estimate presence of coronary artery disease: retrospective pooled analysis of existing cohorts},
  journal   = {BMJ},
  year      = {2012},
  volume    = {344},
  doi       = {10.1136/bmj.e3485},
  eprint    = {http://www.bmj.com/content/344/bmj.e3485.full.pdf},
  url       = {http://www.bmj.com/content/344/bmj.e3485},
  abstract  = {Objectives To develop prediction models that better estimate the pretest probability of coronary artery disease in low prevalence populations.Design Retrospective pooled analysis of individual patient data.Setting 18 hospitals in Europe and the United States.Participants Patients with stable chest pain without evidence for previous coronary artery disease, if they were referred for computed tomography (CT) based coronary angiography or catheter based coronary angiography (indicated as low and high prevalence settings, respectively).Main outcome measures Obstructive coronary artery disease (>=50\% diameter stenosis in at least one vessel found on catheter based coronary angiography). Multiple imputation accounted for missing predictors and outcomes, exploiting strong correlation between the two angiography procedures. Predictive models included a basic model (age, sex, symptoms, and setting), clinical model (basic model factors and diabetes, hypertension, dyslipidaemia, and smoking), and extended model (clinical model factors and use of the CT based coronary calcium score). We assessed discrimination (c statistic), calibration, and continuous net reclassification improvement by cross validation for the four largest low prevalence datasets separately and the smaller remaining low prevalence datasets combined.Results We included 5677 patients (3283 men, 2394 women), of whom 1634 had obstructive coronary artery disease found on catheter based coronary angiography. All potential predictors were significantly associated with the presence of disease in univariable and multivariable analyses. The clinical model improved the prediction, compared with the basic model (cross validated c statistic improvement from 0.77 to 0.79, net reclassification improvement 35\%); the coronary calcium score in the extended model was a major predictor (0.79 to 0.88, 102\%). Calibration for low prevalence datasets was satisfactory.Conclusions Updated prediction models including age, sex, symptoms, and cardiovascular risk factors allow for accurate estimation of the pretest probability of coronary artery disease in low prevalence populations. Addition of coronary calcium scores to the prediction models improves the estimates.},
  publisher = {BMJ Publishing Group Ltd},
}

@Article{Austin2014,
  author   = {Austin, Peter C. and Steyerberg, Ewout W.},
  title    = {Graphical assessment of internal and external calibration of logistic regression models by using loess smoothers},
  journal  = {Statistics in Medicine},
  year     = {2014},
  volume   = {33},
  number   = {3},
  pages    = {517--535},
  issn     = {1097-0258},
  doi      = {10.1002/sim.5941},
  url      = {http://dx.doi.org/10.1002/sim.5941},
  keywords = {logistic regression, prediction, calibration, graphical methods, prediction models},
}

@Article{Li1991,
  author    = {Ker-Chau Li},
  title     = {Sliced Inverse Regression for Dimension Reduction},
  journal   = {Journal of the American Statistical Association},
  year      = {1991},
  volume    = {86},
  number    = {414},
  pages     = {316-327},
  issn      = {01621459},
  url       = {http://www.jstor.org/stable/2290563},
  abstract  = {Modern advances in computing power have greatly widened scientists' scope in gathering and investigating information from many variables, information which might have been ignored in the past. Yet to effectively scan a large pool of variables is not an easy task, although our ability to interact with data has been much enhanced by recent innovations in dynamic graphics. In this article, we propose a novel data-analytic tool, sliced inverse regression (SIR), for reducing the dimension of the input variable x without going through any parametric or nonparametric model-fitting process. This method explores the simplicity of the inverse view of regression; that is, instead of regressing the univariate output variable y against the multivariate x, we regress x against y. Forward regression and inverse regression are connected by a theorem that motivates this method. The theoretical properties of SIR are investigated under a model of the form, y = f(?1x, ..., ?Kx, ?), where the ?k's are the unknown row vectors. This model looks like a nonlinear regression, except for the crucial difference that the functional form of f is completely unknown. For effectively reducing the dimension, we need only to estimate the space [effective dimension reduction (e.d.r.) space] generated by the ?k's. This makes our goal different from the usual one in regression analysis, the estimation of all the regression coefficients. In fact, the ?k's themselves are not identifiable without a specific structural form on f. Our main theorem shows that under a suitable condition, if the distribution of x has been standardized to have the zero mean and the identity covariance, the inverse regression curve, E(x ? y), will fall into the e.d.r. space. Hence a principal component analysis on the covariance matrix for the estimated inverse regression curve can be conducted to locate its main orientation, yielding our estimates for e.d.r. directions. Furthermore, we use a simple step function to estimate the inverse regression curve. No complicated smoothing is needed. SIR can be easily implemented on personal computers. By simulation, we demonstrate how SIR can effectively reduce the dimension of the input variable from, say, 10 to K = 2 for a data set with 400 observations. The spin-plot of y against the two projected variables obtained by SIR is found to mimic the spin-plot of y against the true directions very well. A chi-squared statistic is proposed to address the issue of whether or not a direction found by SIR is spurious.},
  publisher = {[American Statistical Association, Taylor \& Francis, Ltd.]},
}

@Article{Bacchetti2010,
  author   = {Bacchetti, Peter},
  title    = {Current sample size conventions: Flaws, harms, and alternatives},
  journal  = {BMC Medicine},
  year     = {2010},
  volume   = {8},
  number   = {1},
  month    = {Mar},
  pages    = {17},
  issn     = {1741-7015},
  doi      = {10.1186/1741-7015-8-17},
  url      = {https://doi.org/10.1186/1741-7015-8-17},
  abstract = {The belief remains widespread that medical research studies must have statistical power of at least 80{\%} in order to be scientifically sound, and peer reviewers often question whether power is high enough.},
  day      = {22},
}

@Article{Bacchetti2002,
  author  = {Bacchetti, P.},
  title   = {Peer review of statistics in medical research: the other problem},
  journal = {British Medical Journal},
  year    = {2002},
  volume  = {324},
  doi     = {10.1136/bmj.324.7348.1271},
  url     = {https://doi.org/10.1136/bmj.324.7348.1271},
}

@Article{Bacchetti2002a,
  author  = {Bacchetti, P.},
  title   = {Peer review of statistics in medical research - Author's thoughts on power calculations},
  journal = {British Medical Journal},
  year    = {2002},
  volume  = {325},
}

@Article{Bacchetti2008a,
  author  = {Bacchetti, P. and McCulloch, C. E. and Segal, M. R.},
  title   = {Simple, defensible sample sizes based on cost efficiency - Rejoinder},
  journal = {Biometrics},
  year    = {2008},
  volume  = {64},
  doi     = {10.1111/j.1541-0420.2008.01004_5.x},
  url     = {https://doi.org/10.1111/j.1541-0420.2008.01004_5.x},
}

@Article{Bacchetti2005,
  author  = {Bacchetti, P. and Wolf, L. E. and Segal, M. R. and McCulloch, C. E.},
  title   = {Bacchetti et al. Respond to ``Ethics and sample size - Another view''},
  journal = {American Journal of Epidemiology},
  year    = {2005},
  volume  = {161},
  doi     = {10.1093/aje/kwi016},
  url     = {https://doi.org/10.1093/aje/kwi016},
}

@Article{Bacchetti2005a,
  author  = {Bacchetti, P. and Wolf, L. E. and Segal, M. R. and McCulloch, C. E.},
  title   = {Ethics and sample size},
  journal = {American Journal of Epidemiology},
  year    = {2005},
  volume  = {161},
  doi     = {10.1093/aje/kwi014},
  url     = {https://doi.org/10.1093/aje/kwi014},
}

@Article{Bacchetti2005b,
  author  = {Bacchetti, P. and Wolf, L. E. and Segal, M. R. and McCulloch, C. E.},
  title   = {Re: ``Ethics and sample size'' - Reply},
  journal = {American Journal of Epidemiology},
  year    = {2005},
  volume  = {162},
  doi     = {10.1093/aje/kwi177},
  url     = {https://doi.org/10.1093/aje/kwi177},
}

@Article{Thomson2009,
  author    = {Thomson, Andrew and Hayes, Richard and Cousens, Simon},
  title     = {Measures of between-cluster variability in cluster randomized trials with binary outcomes},
  journal   = {Statistics in Medicine},
  year      = {2009},
  volume    = {28},
  number    = {12},
  pages     = {1739--1751},
  issn      = {1097-0258},
  doi       = {10.1002/sim.3582},
  url       = {http://dx.doi.org/10.1002/sim.3582},
  keywords  = {cluster randomized trials, sample size, binary outcomes},
  publisher = {John Wiley \& Sons, Ltd.},
}

@Article{Hemming2017,
  author    = {Hemming, K and Eldridge, S and Forbes, G and Weijer, C and Taljaard, M},
  title     = {How to design efficient cluster randomised trials},
  journal   = {BMJ},
  year      = {2017},
  volume    = {358},
  issn      = {0959-8138},
  doi       = {10.1136/bmj.j3064},
  eprint    = {http://www.bmj.com/content/358/bmj.j3064.full.pdf},
  url       = {http://www.bmj.com/content/358/bmj.j3064},
  publisher = {BMJ Publishing Group Ltd},
}

@Article{Rotondi2012,
  author   = {Michael Rotondi and Allan Donner},
  title    = {Sample size estimation in cluster randomized trials: An evidence-based perspective},
  journal  = {Computational Statistics \& Data Analysis},
  year     = {2012},
  volume   = {56},
  number   = {5},
  pages    = {1174 - 1187},
  note     = {Second Issue for COMPUTATIONAL STATISTICS FOR CLINICAL RESEARCH},
  issn     = {0167-9473},
  doi      = {https://doi.org/10.1016/j.csda.2010.12.010},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167947310004780},
  abstract = {Abstract The evidence-based perspective to sample size estimation determines appropriate trial size by examining its potential impact on the literature. This approach is extended to determine the appropriate size of a planned cluster randomized trial by considering the role of the planned trial on a future meta-analysis (including current literature and the proposed study). A simulation-based algorithm allows consideration of variable cluster sizes and intracluster correlation coefficient values in conjunction with three approaches to sample size estimation, namely the power-based, variance reduction and non-inferiority perspectives. Two examples employing the sample size estimation techniques described are discussed in detail, while appropriate code is provided in the accompanying R package CRTSize.},
  keywords = {Sample size estimation, Cluster randomized trials, Intracluster correlation coefficient, Meta-analysis, Design of experiments, Sequential methods},
}

@Article{Donner2004,
  author   = {Allan Donner and Neil Klar},
  title    = {Pitfalls of and Controversies in Cluster Randomization Trials},
  journal  = {American Journal of Public Health},
  year     = {2004},
  volume   = {94},
  number   = {3},
  pages    = {416-422},
  note     = {PMID: 14998805},
  doi      = {10.2105/AJPH.94.3.416},
  eprint   = {https://doi.org/10.2105/AJPH.94.3.416},
  url      = { 
        https://doi.org/10.2105/AJPH.94.3.416
    
},
  abstract = { It is now well known that standard statistical procedures become invalidated when applied to cluster randomized trials in which the unit of inference is the individual. A resulting consequence is that researchers conducting such trials are faced with a multitude of design choices, including selection of the primary unit of inference, the degree to which clusters should be matched or stratified by prognostic factors at baseline, and decisions related to cluster subsampling. Moreover, application of ethical principles developed for individually randomized trials may also require modification. We discuss several topics related to these issues, with emphasis on the choices that must be made in the planning stages of a trial and on some potential pitfalls to be avoided. },
}

@Article{Pudlo2016,
  author  = {Pudlo, Pierre and Marin, Jean-Michel and Estoup, Arnaud and Cornuet, Jean-Marie and Gautier, Mathieu and Robert, Christian P.},
  title   = {Reliable ABC model choice via random forests},
  journal = {Bioinformatics},
  year    = {2016},
  volume  = {32},
  number  = {6},
  pages   = {859-866},
  doi     = {10.1093/bioinformatics/btv684},
  url     = { + http://dx.doi.org/10.1093/bioinformatics/btv684},
}

@Article{Oakley2010,
  author   = {Jeremy E. Oakley and Alan Brennan and Paul Tappenden and Jim Chilcott},
  title    = {Simulation sample sizes for Monte Carlo partial EVPI calculations},
  journal  = {Journal of Health Economics},
  year     = {2010},
  volume   = {29},
  number   = {3},
  pages    = {468 - 477},
  issn     = {0167-6296},
  doi      = {https://doi.org/10.1016/j.jhealeco.2010.03.006},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167629610000470},
  abstract = {Abstract Partial expected value of perfect information (EVPI) quantifies the value of removing uncertainty about unknown parameters in a decision model. EVPIs can be computed via Monte Carlo methods. An outer loop samples values of the parameters of interest, and an inner loop samples the remaining parameters from their conditional distribution. This nested Monte Carlo approach can result in biased estimates if small numbers of inner samples are used and can require a large number of model runs for accurate partial EVPI estimates. We present a simple algorithm to estimate the EVPI bias and confidence interval width for a specified number of inner and outer samples. The algorithm uses a relatively small number of model runs (we suggest approximately 600), is quick to compute, and can help determine how many outer and inner iterations are needed for a desired level of accuracy. We test our algorithm using three case studies.},
  keywords = {Economic model, Expected value of perfect information, Monte Carlo estimation, Bayesian decision theory},
}

@Article{Birnbaum1962,
  author  = {Allan Birnbaum},
  title   = {On the foundations of Statistical Inference},
  journal = {Journal of the American Statistical Association},
  year    = {1962},
  volume  = {57},
  pages   = {269 - 306},
  url     = {http://www.phil.vt.edu/dmayo/personal_website/Birnbaum_1962_Foundations_Statistical_Inference.pdf},
}

@Article{Savage1962,
  author    = {L. J. Savage and George Barnard and Jerome Cornfield and Irwin Bross and George E. P. Box and I. J. Good and D. V. Lindley and C. W. Clunies-Ross and John W. Pratt and Howard Levene and Thomas Goldman and A. P. Dempster and Oscar Kempthorne and Allan Birnbaum},
  title     = {On the Foundations of Statistical Inference: Discussion},
  journal   = {Journal of the American Statistical Association},
  year      = {1962},
  volume    = {57},
  number    = {298},
  pages     = {307-326},
  issn      = {01621459},
  url       = {http://www.jstor.org/stable/2281641},
  publisher = {[American Statistical Association, Taylor \& Francis, Ltd.]},
}

@Article{Mayo2014,
  author    = {Mayo, Deborah G.},
  title     = {On the Birnbaum Argument for the Strong Likelihood Principle},
  journal   = {Statist. Sci.},
  year      = {2014},
  volume    = {29},
  number    = {2},
  month     = {05},
  pages     = {227--239},
  doi       = {10.1214/13-STS457},
  url       = {https://doi.org/10.1214/13-STS457},
  fjournal  = {Statistical Science},
  publisher = {The Institute of Mathematical Statistics},
}

@Article{Reich2012,
  author    = {Reich, Nicholas G. AND Myers, Jessica A. AND Obeng, Daniel AND Milstone, Aaron M. AND Perl, Trish M.},
  title     = {Empirical Power and Sample Size Calculations for Cluster-Randomized and Cluster-Randomized Crossover Studies},
  journal   = {PLOS ONE},
  year      = {2012},
  volume    = {7},
  number    = {4},
  month     = {04},
  pages     = {1-7},
  doi       = {10.1371/journal.pone.0035564},
  url       = {https://doi.org/10.1371/journal.pone.0035564},
  abstract  = {In recent years, the number of studies using a cluster-randomized design has grown dramatically. In addition, the cluster-randomized crossover design has been touted as a methodological advance that can increase efficiency of cluster-randomized studies in certain situations. While the cluster-randomized crossover trial has become a popular tool, standards of design, analysis, reporting and implementation have not been established for this emergent design. We address one particular aspect of cluster-randomized and cluster-randomized crossover trial design: estimating statistical power. We present a general framework for estimating power via simulation in cluster-randomized studies with or without one or more crossover periods. We have implemented this framework in the clusterPower software package for R, freely available online from the Comprehensive R Archive Network. Our simulation framework is easy to implement and users may customize the methods used for data analysis. We give four examples of using the software in practice. The clusterPower package could play an important role in the design of future cluster-randomized and cluster-randomized crossover studies. This work is the first to establish a universal method for calculating power for both cluster-randomized and cluster-randomized clinical trials. More research is needed to develop standardized and recommended methodology for cluster-randomized crossover studies.},
  publisher = {Public Library of Science},
}

@Article{Harrison2004,
  author  = {David A. Harrison and Anthony R. Brady},
  title   = {Sample size and power calculations using the noncentral t-distribution},
  journal = {The Stata Journal},
  year    = {2004},
  volume  = {4},
  number  = {2},
  pages   = {142-153},
  url     = {http://ageconsearch.umn.edu/bitstream/116234/2/sjart_st0062.pdf},
}

@Article{Beran1986,
  author    = {Rudolf Beran},
  title     = {Simulated Power Functions},
  journal   = {The Annals of Statistics},
  year      = {1986},
  volume    = {14},
  number    = {1},
  pages     = {151-173},
  issn      = {00905364},
  url       = {http://www.jstor.org/stable/2241272},
  abstract  = {Tests for a null hypothesis whose specification involves an unknown nuisance parameter may be obtained by inverting a bootstrap confidence region for the parameter being tested or by constructing a simulated null distribution for the test statistic. The power of either test against certain alternatives involving the same unknown nuisance parameter can itself be estimated by simulation.},
  publisher = {Institute of Mathematical Statistics},
}

@Article{Kerry1998,
  author    = {Kerry, Sally M and Bland, J Martin},
  title     = {The intracluster correlation coefficient in cluster randomisation},
  journal   = {BMJ},
  year      = {1998},
  volume    = {316},
  number    = {7142},
  pages     = {1455--1460},
  issn      = {0959-8138},
  doi       = {10.1136/bmj.316.7142.1455},
  eprint    = {http://www.bmj.com/content/316/7142/1455.1.full.pdf},
  url       = {http://www.bmj.com/content/316/7142/1455.1},
  publisher = {BMJ Publishing Group Ltd},
}

@Article{Guittet2006,
  author   = {Guittet, Lydia and Ravaud, Philippe and Giraudeau, Bruno},
  title    = {Planning a cluster randomized trial with unequal cluster sizes: practical issues involving continuous outcomes},
  journal  = {BMC Medical Research Methodology},
  year     = {2006},
  volume   = {6},
  number   = {1},
  month    = {Apr},
  pages    = {17},
  issn     = {1471-2288},
  doi      = {10.1186/1471-2288-6-17},
  url      = {https://doi.org/10.1186/1471-2288-6-17},
  abstract = {Cluster randomization design is increasingly used for the evaluation of health-care, screeening or educational interventions. At the planning stage, sample size calculations usually consider an average cluster size without taking into account any potential imbalance in cluster size. However, there may exist high discrepancies in cluster sizes.},
  day      = {12},
}

@Article{Breukelen2007,
  author    = {van Breukelen, Gerard J. P. and Candel, Math J. J. M. and Berger, Martijn P. F.},
  title     = {Relative efficiency of unequal versus equal cluster sizes in cluster randomized and multicentre trials},
  journal   = {Statistics in Medicine},
  year      = {2007},
  volume    = {26},
  number    = {13},
  pages     = {2589--2603},
  issn      = {1097-0258},
  doi       = {10.1002/sim.2740},
  url       = {http://dx.doi.org/10.1002/sim.2740},
  keywords  = {cluster randomized trials, mixed regression, multicentre trials, optimal design, relative efficiency, sample size, power},
  publisher = {John Wiley \& Sons, Ltd.},
}

@Article{Fedorov2005,
  author   = {Valerii Fedorov and Byron Jones},
  title    = {The design of multicentre trials},
  journal  = {Statistical Methods in Medical Research},
  year     = {2005},
  volume   = {14},
  number   = {3},
  pages    = {205-248},
  note     = {PMID: 15969302},
  doi      = {10.1191/0962280205sm399oa},
  eprint   = {https://doi.org/10.1191/0962280205sm399oa},
  url      = { 
        https://doi.org/10.1191/0962280205sm399oa
    
},
  abstract = { The analysis of data collected in multicentre trials offers challenges because the data from the individual centres must be combined in some way to give an overall evaluation of the differences between the treatments in the trial. We propose that the combined response to treatment (CRT) be used as this overall measure. The definition and estimation of the CRT can be derived from either a fixed-effects or a random-effects model. For the latter we introduce the ECRT - the expected combined response to treatment. We describe and compare both types of model and express our preference for the random-effects model. We stress that the number of patients enrolled at a centre is a random variable and show that this source of randomness inflates the variance of the estimated ECRT. Variability in enrolment rates over the centres further inflates this variance. A simple conclusion from our results is that if variability in the treatment and centre effects, in the enrolment time, in the number of patients enrolled at a centre and in the enrolment rates is not properly accounted for, then an underpowered trial may result. Using properties of estimators generated by the random-effects model we propose methods for determining the optimal number of centres and total number of patients to enrol in a trial to minimize a loss function that accounts for centre and patient costs and loss of revenue. We discuss variants of the loss function and corresponding optimization problems for different types of enrolment. We end the paper with brief generalizations of the developed techniques to the case where the response is binary. },
}

@Manual{Bendtsen2012,
  author = {Claus Bendtsen},
  title  = {pso: Particle Swarm Optimization},
  year   = {2012},
  note   = {R package version 1.0.3},
  url    = {https://CRAN.R-project.org/package=pso},
}

@Article{Kerry54,
  author    = {Kerry, Sally M and Bland, J Martin},
  title     = {Analysis of a trial randomised in clusters},
  journal   = {BMJ},
  year      = {1998},
  volume    = {316},
  number    = {7124},
  pages     = {54},
  issn      = {0959-8138},
  doi       = {10.1136/bmj.316.7124.54},
  eprint    = {http://www.bmj.com/content/316/7124/54.full.pdf},
  url       = {http://www.bmj.com/content/316/7124/54},
  publisher = {BMJ Publishing Group Ltd},
}

@Article{Donner1998,
  author    = {Donner, Allan},
  title     = {Some aspects of the design and analysis of cluster randomization trials},
  journal   = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  year      = {1998},
  volume    = {47},
  number    = {1},
  pages     = {95--113},
  issn      = {1467-9876},
  doi       = {10.1111/1467-9876.00100},
  url       = {http://dx.doi.org/10.1111/1467-9876.00100},
  keywords  = {Antenatal care, Clinical trials, Correlated binary data, Group randomization, Sample size estimation},
  publisher = {Blackwell Publishers Ltd.},
}

@Article{Feng1992,
  author    = {Feng, Ziding and Grizzle, James E.},
  title     = {Correlated binomial variates: Properties of estimator of intraclass correlation and its effect on sample size calculation},
  journal   = {Statistics in Medicine},
  year      = {1992},
  volume    = {11},
  number    = {12},
  pages     = {1607--1614},
  issn      = {1097-0258},
  doi       = {10.1002/sim.4780111208},
  url       = {http://dx.doi.org/10.1002/sim.4780111208},
  publisher = {Wiley Subscription Services, Inc., A Wiley Company},
}

@Article{Freidlin2017,
  author   = {Boris Freidlin and Edward L Korn},
  title    = {Sample size adjustment designs with time-to-event outcomes: A caution},
  journal  = {Clinical Trials},
  year     = {2017},
  volume   = {14},
  number   = {6},
  pages    = {597-604},
  note     = {PMID: 28795844},
  doi      = {10.1177/1740774517724746},
  eprint   = {https://doi.org/10.1177/1740774517724746},
  url      = { 
        https://doi.org/10.1177/1740774517724746
    
},
  abstract = { Background:Sample size adjustment designs, which allow increasing the study sample size based on interim analysis of outcome data from a randomized clinical trial, have been increasingly promoted in the biostatistical literature. Although it is recognized that group sequential designs can be at least as efficient as sample size adjustment designs, many authors argue that a key advantage of these designs is their flexibility; interim sample size adjustment decisions can incorporate information and business interests external to the trial. Recently, Chen et al. (Clinical Trials 2015) considered sample size adjustment applications in the time-to-event setting using a design (CDL) that limits adjustments to situations where the interim results are promising. The authors demonstrated that while CDL provides little gain in unconditional power (versus fixed-sample-size designs), there is a considerable increase in conditional power for trials in which the sample size is adjusted.Methods:In time-to-event settings, sample size adjustment allows an increase in the number of events required for the final analysis. This can be achieved by either (a) following the original study population until the additional events are observed thus focusing on the tail of the survival curves or (b) enrolling a potentially large number of additional patients thus focusing on the early differences in survival curves. We use the CDL approach to investigate performance of sample size adjustment designs in time-to-event trials.Results:Through simulations, we demonstrate that when the magnitude of the true treatment effect changes over time, interim information on the shape of the survival curves can be used to enrich the final analysis with events from the time period with the strongest treatment effect. In particular, interested parties have the ability to make the end-of-trial treatment effect larger (on average) based on decisions using interim outcome data. Furthermore, in “clinical null” cases where there is no benefit due to crossing survival curves, the sample size adjustment design is shown to increase the probability of recommending an ineffective therapy.Conclusion:Access to interim information on the shape of the survival curves may jeopardize the perceived integrity of trials using sample size adjustment designs. Therefore, given the lack of efficiency advantage over group sequential designs, sample size adjustment designs in time-to-event settings remain unjustified. },
}

@Article{Kennedy2000,
  author  = {Kennedy, MC and O'Hagan, A},
  title   = {Predicting the output from a complex computer code when fast approximations are available},
  journal = {Biometrika},
  year    = {2000},
  volume  = {87},
  number  = {1},
  pages   = {1-13},
  doi     = {10.1093/biomet/87.1.1},
  eprint  = {/oup/backfile/content_public/journal/biomet/87/1/10.1093/biomet/87.1.1/2/870001.pdf},
  url     = { + http://dx.doi.org/10.1093/biomet/87.1.1},
}

@Article{Sacks1989,
  author    = {Jerome Sacks and William J. Welch and Toby J. Mitchell and Henry P. Wynn},
  title     = {Design and Analysis of Computer Experiments},
  journal   = {Statistical Science},
  year      = {1989},
  volume    = {4},
  number    = {4},
  pages     = {409-423},
  issn      = {08834237},
  url       = {http://www.jstor.org/stable/2245858},
  abstract  = {Many scientific phenomena are now investigated by complex computer models or codes. A computer experiment is a number of runs of the code with various inputs. A feature of many computer experiments is that the output is deterministic--rerunning the code with the same inputs gives identical observations. Often, the codes are computationally expensive to run, and a common objective of an experiment is to fit a cheaper predictor of the output to the data. Our approach is to model the deterministic output as the realization of a stochastic process, thereby providing a statistical basis for designing experiments (choosing the inputs) for efficient prediction. With this model, estimates of uncertainty of predictions are also available. Recent work in this area is reviewed, a number of applications are discussed, and we demonstrate our methodology with an example.},
  publisher = {Institute of Mathematical Statistics},
}

@Article{Berry1997,
  author    = {Berry, Scott M. and Kadane, Joseph B.},
  title     = {Optimal Bayesian Randomization},
  journal   = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  year      = {1997},
  volume    = {59},
  number    = {4},
  pages     = {813--819},
  issn      = {1467-9868},
  doi       = {10.1111/1467-9868.00098},
  url       = {http://dx.doi.org/10.1111/1467-9868.00098},
  keywords  = {Clinical trial, Covariate, Ethics, Multiperson model},
  publisher = {Blackwell Publishers Ltd.},
}

@Article{Brown1987,
  author   = {Barry W. Brown and Jay Herson and E. Neely Atkinson and M. Elizabeth Rozell},
  title    = {Projection from previous studies: A Bayesian and frequentist compromise},
  journal  = {Controlled Clinical Trials},
  year     = {1987},
  volume   = {8},
  number   = {1},
  pages    = {29 - 44},
  issn     = {0197-2456},
  doi      = {https://doi.org/10.1016/0197-2456(87)90023-7},
  url      = {http://www.sciencedirect.com/science/article/pii/0197245687900237},
  abstract = {Abstract We present methods that use the results of a previous study to predict the outcome of a specified comparative trial in which each subject's outcome is categorized dichotomously (e.g., response or no reponse). The methods can be generalized to other two-sample cases for which power can be calculated (e.g., exponential survival), and to one-sample cases such as demonstrating a minimal response rate or demonstrating superiority to a historic control. Bayesian methods are used on the results of the preliminary study to obtain a posterior distribution representing the state of knowledge of the parameters of interest. This distribution provides the probability that the experimental regimen is superior to the standard by any particular amount. The probability that a future study will demonstrate the superiority of the experimental treatment is obtained by using the posterior distribution to average the power of the rest over the parameter space.},
  keywords = {study planning, sample size, Bayesian methods},
}

@Article{Lindley1991,
  author    = {Dennis V. Lindley and Nozer D. Singpurwalla},
  title     = {On the Evidence Needed to Reach Agreed Action between Adversaries, with Application to Acceptance Sampling},
  journal   = {Journal of the American Statistical Association},
  year      = {1991},
  volume    = {86},
  number    = {416},
  pages     = {933-937},
  doi       = {10.1080/01621459.1991.10475136},
  eprint    = {http://amstat.tandfonline.com/doi/pdf/10.1080/01621459.1991.10475136},
  url       = { 
        http://amstat.tandfonline.com/doi/abs/10.1080/01621459.1991.10475136
    
},
  abstract  = { Abstract Two decision makers disagree about a quantity of interest to them both. One of them, the “consumer,” has a choice of two decisions that are affected by the quantity. The other, the “manufacturer,” offers to perform an agreed type of experiment that it is hoped will change the consumer's view of the quantity and hence the decision. This article is devoted to the evaluation of how much experimentation should be done. Binomial, Poisson, and normal likelihoods, together with their conjugate utilities and probabilities, are considered and illustrated by numerical cases. The scenario considered here arises in applications to quality control, bidding, drug testing, marketing, and sales. },
  publisher = {Taylor \& Francis},
}

@Article{Senn2013,
  author   = {Senn, Stephen},
  title    = {Seven myths of randomisation in clinical trials},
  journal  = {Statistics in Medicine},
  year     = {2013},
  volume   = {32},
  number   = {9},
  pages    = {1439--1450},
  issn     = {1097-0258},
  doi      = {10.1002/sim.5713},
  url      = {http://dx.doi.org/10.1002/sim.5713},
  keywords = {randomisation, blinding, conditioning, covariates},
}

@Article{Chuang-Stein2007,
  author    = {Chuang-Stein, Christy and Stryszak, Paul and Dmitrienko, Alex and Offen, Walter},
  title     = {Challenge of multiple co-primary endpoints: a new approach},
  journal   = {Statistics in Medicine},
  year      = {2007},
  volume    = {26},
  number    = {6},
  pages     = {1181--1192},
  issn      = {1097-0258},
  doi       = {10.1002/sim.2604},
  url       = {http://dx.doi.org/10.1002/sim.2604},
  keywords  = {average type I error rate, intersection–union test, prior distribution, restricted null space, reverse multiplicity},
  publisher = {John Wiley \& Sons, Ltd.},
}

@Article{Offen2007,
  author   = {Walter Offen and Christy Chuang-Stein and Alex Dmitrienko and Gary Littman and Jeff Maca and Laura Meyerson and Robb Muirhead and Paul Stryszak and Alex Baddy and Kun Chen and Kati Copley-Merriman and Willard Dere and Sam Givens and David Hall and David Henry and Joseph D. Jackson and Alok Krishen and Thomas Liu and Steve Ryder and A. J. Sankoh and Julia Wang and Chyon-Hwa Yeh},
  title    = {Multiple Co-primary Endpoints: Medical and Statistical Solutions: A Report from the Multiple Endpoints Expert Team of the Pharmaceutical Research and Manufacturers of America},
  journal  = {Drug Information Journal},
  year     = {2007},
  volume   = {41},
  number   = {1},
  pages    = {31-46},
  doi      = {10.1177/009286150704100105},
  eprint   = {https://doi.org/10.1177/009286150704100105},
  url      = { 
        https://doi.org/10.1177/009286150704100105
    
},
  abstract = { There are quite a few disorders for which regulatory agencies have required a treatment to demonstrate a statistically significant effect on multiple endpoints, each at the one-sided 2.5\% level, before accepting the treatment's efficacy for the disorders. Depending on the correlation among the endpoints, this requirement could lead to a substantial reduction in the study's power to conclude the efficacy of a treatment. To investigate the prevalence of this requirement and propose possible solutions, a multiple-disciplinary Multiple Endpoints Expert Team sponsored by Pharmaceutical Research and Manufacturers of America was formed in November 2003. The team recognized early that many researchers were not fully aware of the implications of requiring multiple co-primary endpoints. The team proposes possible solutions from both the medical and the statistical perspectives. The optimal solution is to reduce the number of multiple co-primary endpoints. If after careful considerations, multiple co-primary endpoints remain a scientific requirement, the team proposes statistical solutions and encourages that regulatory agencies be receptive to approaches that adopt modest upward adjustments of the nominal significance levels for testing individual endpoints. Finally, the team hopes that this report will draw more attention to the problem of multiple co-primary endpoints and stimulate further research. },
}

@Article{Berger1982,
  author    = {Roger L. Berger},
  title     = {Multiparameter Hypothesis Testing and Acceptance Sampling},
  journal   = {Technometrics},
  year      = {1982},
  volume    = {24},
  number    = {4},
  pages     = {295-300},
  doi       = {10.1080/00401706.1982.10487790},
  eprint    = {http://www.tandfonline.com/doi/pdf/10.1080/00401706.1982.10487790},
  url       = { 
        http://www.tandfonline.com/doi/abs/10.1080/00401706.1982.10487790
    
},
  abstract  = { The quality of a product might be determined by several parameters, each of which must meet certain standards before the product is acceptable. In this article, a method of determining whether all the parameters meet their respective standards is proposed. The method consists of testing each parameter individually and deciding that the product is acceptable only if each parameter passes its test. This simple method has some optimal properties including attaining exactly a prespecified consumer's risk and uniformly minimizing the producer's risk. These results are obtained from more general hypothesis-testing results concerning null hypotheses consisting of the unions of sets. },
  publisher = {Taylor \& Francis},
}

@Article{Laska1989,
  author    = {Eugene M. Laska and Morris J. Meisner},
  title     = {Testing Whether an Identified Treatment Is Best},
  journal   = {Biometrics},
  year      = {1989},
  volume    = {45},
  number    = {4},
  pages     = {1139-1151},
  issn      = {0006341X, 15410420},
  url       = {http://www.jstor.org/stable/2531766},
  abstract  = {We consider the problem of testing whether an identified treatment is better than each of K treatments. Suppose there are univariate test statistics Si that contrast the identified treatment with treatment i for i = 1, 2,..., K. The min test is defined to be the alpha-level procedure that rejects the null hypothesis that the identified treatment is not best when, for all i, Si rejects the one-sided hypothesis, at the alpha-level, that the identified treatment is not better than the ith treatment. In the normal case where Si are t statistics the min test is the likelihood ratio test. For distributions satisfying mild regularity conditions, if attention is restricted to test statistics that are monotone nondecreasing functions of Si, then regardless of their covariance structure the min test is an optimal alpha-level test. Tables of the sample size needed to achieve power .5, .8, .90, and .95 are given for the min test when the Si are Student's t and Wilcoxon.},
  publisher = {[Wiley, International Biometric Society]},
}

@Article{Sozu2010,
  author    = {Sozu, Takashi and Sugimoto, Tomoyuki and Hamasaki, Toshimitsu},
  title     = {Sample size determination in clinical trials with multiple co-primary binary endpoints},
  journal   = {Statistics in Medicine},
  year      = {2010},
  volume    = {29},
  number    = {21},
  pages     = {2169--2179},
  issn      = {1097-0258},
  doi       = {10.1002/sim.3972},
  url       = {http://dx.doi.org/10.1002/sim.3972},
  keywords  = {correlated endpoints, multivariate Bernoulli, association measures, continuity correction, arcsine transformation, Fisher's exact method},
  publisher = {John Wiley \& Sons, Ltd.},
}

@Article{Chuang-Stein2017,
  author   = {Chuang-Stein, Christy and Li, Jianjun (David)},
  title    = {Changes are still needed on multiple co-primary endpoints},
  journal  = {Statistics in Medicine},
  year     = {2017},
  volume   = {36},
  number   = {28},
  pages    = {4427--4436},
  note     = {SIM-17-0194},
  issn     = {1097-0258},
  doi      = {10.1002/sim.7383},
  url      = {http://dx.doi.org/10.1002/sim.7383},
  keywords = {average Type I error rate, balanced adjustment method, min test, non-monotone test, strong Type I error rate control},
}

@Article{Jones2017,
  author   = {Terry A. Jones and Timothy S. Olds and David C. Currow and Marie T. Williams},
  title    = {Feasibility and Pilot Studies in Palliative Care Research: A Systematic Review},
  journal  = {Journal of Pain and Symptom Management},
  year     = {2017},
  volume   = {54},
  number   = {1},
  pages    = {139 - 151.e4},
  issn     = {0885-3924},
  doi      = {https://doi.org/10.1016/j.jpainsymman.2017.02.015},
  url      = {http://www.sciencedirect.com/science/article/pii/S0885392417301604},
  abstract = {Abstract Feasibility and pilot study designs are common in palliative care research. Finding standard guidelines on the structure and reporting of these study types is difficult. In feasibility and pilot studies in palliative care research, to determine 1) how commonly a priori feasibility are criteria reported and whether results are subsequently reported against these criteria? and 2) how commonly are participants' views on acceptability of burden of the study protocol assessed? Four databases (OVID Medline, EMBASE, CINAHL, and PubMed via caresearch.com.au.) were searched. Search terms included palliative care, terminal care, advance care planning, hospice, pilot, feasibility, with a publication date between January 1, 2012 and December 31, 2013. Articles were selected and appraised by two independent reviewers. Fifty-six feasibility and/or pilot studies were included in this review. Only three studies had clear a priori criteria to measure success. Sixteen studies reported participant acceptability or burden with measures. Forty-eight studies concluded feasibility. The terms “feasibility” and “pilot” are used synonymously in palliative care research when describing studies that test for feasibility. Few studies in palliative care research outline clear criteria for success. The assessment of participant acceptability and burden is uncommon. A gold standard for feasibility study design in palliative care research that includes both clear criteria for success and testing of the study protocol for participant acceptability and burden is needed. Such a standard would assist with consistency in the design, conduct and reporting of feasibility and pilot studies.},
  keywords = {Pilot, feasibility, palliative, criteria, acceptability, participants' study burden, clinical trials},
}

@Article{Kordzakhia2010,
  author    = {Kordzakhia, George and Siddiqui, Ohidul and Huque, Mohammad F.},
  title     = {Method of balanced adjustment in testing co-primary endpoints},
  journal   = {Statistics in Medicine},
  year      = {2010},
  volume    = {29},
  number    = {19},
  pages     = {2055--2066},
  issn      = {1097-0258},
  doi       = {10.1002/sim.3950},
  url       = {http://dx.doi.org/10.1002/sim.3950},
  keywords  = {co-primary endpoints, clinical trials, multiplicity},
  publisher = {John Wiley \& Sons, Ltd.},
}

@Article{Deaton2017,
  author   = {Angus Deaton and Nancy Cartwright},
  title    = {Understanding and misunderstanding randomized controlled trials},
  journal  = {Social Science \& Medicine},
  year     = {2017},
  issn     = {0277-9536},
  doi      = {https://doi.org/10.1016/j.socscimed.2017.12.005},
  url      = {http://www.sciencedirect.com/science/article/pii/S0277953617307359},
  abstract = {Abstract Randomized Controlled Trials (RCTs) are increasingly popular in the social sciences, not only in medicine. We argue that the lay public, and sometimes researchers, put too much trust in RCTs over other methods of investigation. Contrary to frequent claims in the applied literature, randomization does not equalize everything other than the treatment in the treatment and control groups, it does not automatically deliver a precise estimate of the average treatment effect (ATE), and it does not relieve us of the need to think about (observed or unobserved) covariates. Finding out whether an estimate was generated by chance is more difficult than commonly believed. At best, an RCT yields an unbiased estimate, but this property is of limited practical value. Even then, estimates apply only to the sample selected for the trial, often no more than a convenience sample, and justification is required to extend the results to other groups, including any population to which the trial sample belongs, or to any individual, including an individual in the trial. Demanding ‘external validity’ is unhelpful because it expects too much of an RCT while undervaluing its potential contribution. RCTs do indeed require minimal assumptions and can operate with little prior knowledge. This is an advantage when persuading distrustful audiences, but it is a disadvantage for cumulative scientific progress, where prior knowledge should be built upon, not discarded. RCTs can play a role in building scientific knowledge and useful predictions but they can only do so as part of a cumulative program, combining with other methods, including conceptual and theoretical development, to discover not ‘what works’, but ‘why things work’.},
  keywords = {RCTs, Balance, Bias, Precision, External validity, Transportation of results, Health, Economic development},
}

@Book{OHagan2006a,
  author    = {Anthony O'Hagan and Caitlin E. Buck and Alireza Daneshkhah and J. Richard Eiser and Paul H. Garthwaite and David J. Jenkinson and Jeremy E. Oakley and Tim Rakow},
  title     = {Uncertain Judgements: Eliciting Experts' Probabilities},
  year      = {2006},
  publisher = {John Wiley and Sons},
}

@Article{OHagan2012,
  author   = {Anthony O’Hagan},
  title    = {Probabilistic uncertainty specification: Overview, elaboration techniques and their application to a mechanistic model of carbon flux},
  journal  = {Environmental Modelling \& Software},
  year     = {2012},
  volume   = {36},
  pages    = {35 - 48},
  note     = {Thematic issue on Expert Opinion in Environmental Modelling and Management},
  issn     = {1364-8152},
  doi      = {https://doi.org/10.1016/j.envsoft.2011.03.003},
  url      = {http://www.sciencedirect.com/science/article/pii/S1364815211000624},
  abstract = {It is widely recognised that the appropriate representation for expert judgements of uncertainty is as a probability distribution for the unknown quantity of interest. However, formal elicitation of probability distributions is a non-trivial task. We provide an overview of this field, including an outline of the process of eliciting knowledge from experts in probabilistic form. We explore approaches to probabilistic uncertainty specification including direct elicitation and Bayesian analysis. In particular, we introduce the generic technique of elaboration and present a variety of forms of elaboration, illustrated with a series of examples. The methods are applied to the expression of uncertainty in a case study. Mechanistic models are built in just about every area of science and technology, to represent complex physical processes. They are used to predict, understand and control those processes, and increasingly play a role in national and international policy making. As such models gain higher prominence, recipients of their forecasts are increasingly demanding to know how accurate they are. There is therefore a growing interest in quantifying the uncertainties in model predictions. Uncertainty in model outputs, as representations of reality, arise from uncertainty about model inputs (such as initial conditions, external forcing variables and parameters in model equations) and from uncertainty about model structure. Our case study is based on the Sheffield Dynamic Global Vegetation Model (SDGVM), which is used to estimate the combined carbon flux from vegetation in England and Wales in a given year. The extent to which vegetation acts as a carbon sink is an important component of the debate about climate change. We show how different approaches were used to characterise uncertainty in vegetation model parameters, soil conditions and land cover.},
  keywords = {Subjective probability, Elicitation, Elaboration, Expert judgement, Mechanistic model, Environmental model, Sensitivity analysis, Emulation, Carbon flux},
}

@Article{Girling2007,
  author   = {Alan J Girling and Richard J Lilford and David A Braunholtz and Wayne R Gillett},
  title    = {Sample-size calculations for trials that inform individual treatment decisions: a ‘true-choice’ approach},
  journal  = {Clinical Trials},
  year     = {2007},
  volume   = {4},
  number   = {1},
  pages    = {15-24},
  note     = {PMID: 17327242},
  doi      = {10.1177/1740774506075872},
  eprint   = {https://doi.org/10.1177/1740774506075872},
  url      = { 
        https://doi.org/10.1177/1740774506075872
    
},
  abstract = { Background Sample size decisions for clinical trials should be taken in such a way as to maximize informed choice by reducing scientific uncertainty about the consequences of an intervention.Purpose Recent approaches to trial design have focused on the potential decision impact of the trial when deciding whether the trial should be undertaken, and how large it ought to be. For the most part these approaches are concerned with the impact of trials either on clinical opinion or on collective reimbursement recommendations. Our purpose is to model the contribution of clinical trials to patient-level decision-making and to propose a way of assessing this contribution at the design stage.Methods The model is developed within the framework of Bayesian decision theory. It is presumed that some patients make choices that they would not have made in the presence of perfect information about the likely consequences. These ‘false’ choices would be reversed in response to a fully informative (ie, very large) trial of the competing interventions. By contrast, choices that would not change in response to a fully informative trial are termed ‘true’ choices since they accurately reflect patient preferences.Results An impact plot is proposed which maps how the expected numbers of ‘true’ and ‘false’ choices change in response to a trial of any given size. The approach is illustrated with reference to the choice of delivery mode for term breech presentation, using data obtained before the recent term breech trial. Applications in other contexts are indicated.Limitations No account is taken of the magnitude of expressed patient preferences for one treatment over another. The upside is that the need for detailed utility-elicitation is obviated.Conclusions The approach is a pragmatic aid to trial design in settings where patient preference drives the choice between alternative treatments. },
  groups   = {Fellowship app},
}

@Article{White2011a,
  author    = {White, Ian R. and Royston, Patrick and Wood, Angela M.},
  title     = {Multiple imputation using chained equations: Issues and guidance for practice},
  journal   = {Statistics in Medicine},
  year      = {2011},
  volume    = {30},
  number    = {4},
  pages     = {377--399},
  issn      = {1097-0258},
  doi       = {10.1002/sim.4067},
  url       = {http://dx.doi.org/10.1002/sim.4067},
  keywords  = {missing data, multiple imputation, fully conditional specification},
  publisher = {John Wiley \& Sons, Ltd.},
}

@Article{Rubin1978,
  author    = {Donald B. Rubin},
  title     = {Bayesian Inference for Causal Effects: The Role of Randomization},
  journal   = {The Annals of Statistics},
  year      = {1978},
  volume    = {6},
  number    = {1},
  pages     = {34-58},
  issn      = {00905364},
  url       = {http://www.jstor.org/stable/2958688},
  abstract  = {Causal effects are comparisons among values that would have been observed under all possible assignments of treatments to experimental units. In an experiment, one assignment of treatments is chosen and only the values under that assignment can be observed. Bayesian inference for causal effects follows from finding the predictive distribution of the values under the other assignments of treatments. This perspective makes clear the role of mechanisms that sample experimental units, assign treatments and record data. Unless these mechanisms are ignorable (known probabilistic functions of recorded values), the Bayesian must model them in the data analysis and, consequently, confront inferences for causal effects that are sensitive to the specification of the prior distribution of the data. Moreover, not all ignorable mechanisms can yield data from which inferences for causal effects are insensitive to prior specifications. Classical randomized designs stand out as especially appealing assignment mechanisms designed to make inference for causal effects straightforward by limiting the sensitivity of a valid Bayesian analysis.},
  groups    = {Fellowship app},
  publisher = {Institute of Mathematical Statistics},
}

@Article{Rohrer0,
  author   = {Julia M. Rohrer},
  title    = {Thinking Clearly About Correlations and Causation: Graphical Causal Models for Observational Data},
  journal  = {Advances in Methods and Practices in Psychological Science},
  year     = {2018},
  volume   = {0},
  number   = {0},
  pages    = {2515245917745629},
  doi      = {10.1177/2515245917745629},
  eprint   = {https://doi.org/10.1177/2515245917745629},
  url      = { 
        https://doi.org/10.1177/2515245917745629
    
},
  abstract = { Correlation does not imply causation; but often, observational data are the only option, even though the research question at hand involves causality. This article discusses causal inference based on observational data, introducing readers to graphical causal models that can provide a powerful tool for thinking more clearly about the interrelations between variables. Topics covered include the rationale behind the statistical control of third variables, common procedures for statistical control, and what can go wrong during their implementation. Certain types of third variables—colliders and mediators—should not be controlled for because that can actually move the estimate of an association away from the value of the causal effect of interest. More subtle variations of such harmful control include using unrepresentative samples, which can undermine the validity of causal conclusions, and statistically controlling for mediators. Drawing valid causal inferences on the basis of observational data is not a mechanistic procedure but rather always depends on assumptions that require domain knowledge and that can be more or less plausible. However, this caveat holds not only for research based on observational data, but for all empirical research endeavors. },
}

@Article{Hawe2009,
  author    = {Hawe, Penelope and Shiell, Alan and Riley, Therese},
  title     = {Theorising Interventions as Events in Systems},
  journal   = {American Journal of Community Psychology},
  year      = {2009},
  volume    = {43},
  number    = {3-4},
  pages     = {267--276},
  issn      = {1573-2770},
  doi       = {10.1007/s10464-009-9229-9},
  url       = {http://dx.doi.org/10.1007/s10464-009-9229-9},
  keywords  = {Intervention, Complexity, Social networks, Ecological, Community, Context},
  publisher = {Springer US},
}

@Article{Levati2016,
  author   = {Levati, Sara and Campbell, Pauline and Frost, Rachael and Dougall, Nadine and Wells, Mary and Donaldson, Cam and Hagen, Suzanne},
  title    = {Optimisation of complex health interventions prior to a randomised controlled trial: a scoping review of strategies used},
  journal  = {Pilot and Feasibility Studies},
  year     = {2016},
  volume   = {2},
  number   = {1},
  month    = {Mar},
  pages    = {17},
  issn     = {2055-5784},
  doi      = {10.1186/s40814-016-0058-y},
  url      = {https://doi.org/10.1186/s40814-016-0058-y},
  abstract = {Many complex intervention trials fail to show an intervention effect. Although this may be due to genuine ineffectiveness, it may also be the result of sub-optimal intervention design, implementation failure or a combination of these. Given current financial constraints and the pressure to reduce waste and increase value in health services research, pre-trial strategies are needed to reduce the likelihood of design or implementation failure and to maximise the intervention's potential for effectiveness. In this scoping review, we aimed to identify and synthesise the available evidence relating to the strategies and methods used to `optimise' complex interventions at the pre-trial stage.},
  day      = {15},
}

@Article{Chiem2014,
  author   = {Chiêm, Jean-Christophe and Van Durme, Thérèse and Vandendorpe, Florence and Schmitz, Olivier and Speybroeck, Niko and Cès, Sophie and Macq, Jean},
  title    = {Expert knowledge elicitation using computer simulation: the organization of frail elderly case management as an illustration},
  journal  = {Journal of Evaluation in Clinical Practice},
  year     = {2014},
  volume   = {20},
  number   = {4},
  pages    = {534--543},
  issn     = {1365-2753},
  doi      = {10.1111/jep.12101},
  url      = {http://dx.doi.org/10.1111/jep.12101},
  keywords = {agent-based modelling, case management, complexity, computer simulation, health care organization, knowledge},
}

@Article{Pearl2009,
  author    = {Pearl, Judea},
  title     = {Causal inference in statistics: An overview},
  journal   = {Statist. Surv.},
  year      = {2009},
  volume    = {3},
  pages     = {96--146},
  doi       = {10.1214/09-SS057},
  url       = {https://doi.org/10.1214/09-SS057},
  fjournal  = {Statistics Surveys},
  publisher = {The American Statistical Association, the Bernoulli Society, the Institute of Mathematical Statistics, and the Statistical Society of Canada},
}

@Article{Lindley2002,
  author    = {Lindley, Dennis V.},
  title     = {Seeing and Doing: the Concept of Causation},
  journal   = {International Statistical Review},
  year      = {2002},
  volume    = {70},
  number    = {2},
  pages     = {191--197},
  issn      = {1751-5823},
  doi       = {10.1111/j.1751-5823.2002.tb00355.x},
  url       = {http://dx.doi.org/10.1111/j.1751-5823.2002.tb00355.x},
  keywords  = {Causality, Multivariate distribution, Causal mechanism, Association, Regression, Simpson's paradox, Directed acyclic graph, Structural equation, Covariate, Counterfactual},
  publisher = {Blackwell Publishing Ltd},
}

@Article{Cooper2018,
  author   = {Cindy L Cooper and Amy Whitehead and Edward Pottrill and Steven A Julious and Stephen J Walters},
  title    = {Are pilot trials useful for predicting randomisation and attrition rates in definitive studies: A review of publicly funded trials},
  journal  = {Clinical Trials},
  year     = {2018},
  volume   = {0},
  number   = {0},
  pages    = {1740774517752113},
  note     = {PMID: 29361833},
  doi      = {10.1177/1740774517752113},
  eprint   = {https://doi.org/10.1177/1740774517752113},
  url      = { 
        https://doi.org/10.1177/1740774517752113
    
},
  abstract = { Background/aims:External pilot trials are recommended for testing the feasibility of main or confirmatory trials. However, there is little evidence that progress in external pilot trials actually predicts randomisation and attrition rates in the main trial. To assess the use of external pilot trials in trial design, we compared randomisation and attrition rates in publicly funded randomised controlled trials with rates in their pilots.Methods:Randomised controlled trials for which there was an external pilot trial were identified from reports published between 2004 and 2013 in the Health Technology Assessment Journal. Data were extracted from published papers, protocols and reports. Bland–Altman plots and descriptive statistics were used to investigate the agreement of randomisation and attrition rates between the full and external pilot trials.Results:Of 561 reports, 41 were randomised controlled trials with pilot trials and 16 met criteria for a pilot trial with sufficient data. Mean attrition and randomisation rates were 21.1\% and 50.4\%, respectively, in the pilot trials and 16.8\% and 65.2\% in the main. There was minimal bias in the pilot trial when predicting the main trial attrition and randomisation rate. However, the variation was large: the mean difference in the attrition rate between the pilot and main trial was -4.4\% with limits of agreement of -37.1\% to 28.2\%. Limits of agreement for randomisation rates were -47.8\% to 77.5\%.Conclusion:Results from external pilot trials to estimate randomisation and attrition rates should be used with caution as comparison of the difference in the rates between pilots and their associated full trial demonstrates high variability. We suggest using internal pilot trials wherever appropriate. },
}

@Article{Angrist1996,
  author    = {Joshua D. Angrist and Guido W. Imbens and Donald B. Rubin},
  title     = {Identification of Causal Effects Using Instrumental Variables},
  journal   = {Journal of the American Statistical Association},
  year      = {1996},
  volume    = {91},
  number    = {434},
  pages     = {444-455},
  issn      = {01621459},
  url       = {http://www.jstor.org/stable/2291629},
  abstract  = {We outline a framework for causal inference in settings where assignment to a binary treatment is ignorable, but compliance with the assignment is not perfect so that the receipt of treatment is nonignorable. To address the problems associated with comparing subjects by the ignorable assignment-an "intention-to-treat analysis"-we make use of instrumental variables, which have long been used by economists in the context of regression models with constant treatment effects. We show that the instrumental variables (IV) estimand can be embedded within the Rubin Causal Model (RCM) and that under some simple and easily interpretable assumptions, the IV estimand is the average causal effect for a subgroup of units, the compliers. Without these assumptions, the IV estimand is simply the ratio of intention-to-treat causal estimands with no interpretation as an average causal effect. The advantages of embedding the IV approach in the RCM are that it clarifies the nature of critical assumptions needed for a causal interpretation, and moreover allows us to consider sensitivity of the results to deviations from key assumptions in a straightforward manner. We apply our analysis to estimate the effect of veteran status in the Vietnam era on mortality, using the lottery number that assigned priority for the draft as an instrument, and we use our results to investigate the sensitivity of the conclusions to critical assumptions.},
  publisher = {[American Statistical Association, Taylor \& Francis, Ltd.]},
}

@Article{Emsley2010,
  author   = {Richard Emsley and Graham Dunn and Ian R White},
  title    = {Mediation and moderation of treatment effects in randomised controlled trials of complex interventions},
  journal  = {Statistical Methods in Medical Research},
  year     = {2010},
  volume   = {19},
  number   = {3},
  pages    = {237-270},
  note     = {PMID: 19608601},
  doi      = {10.1177/0962280209105014},
  eprint   = {https://doi.org/10.1177/0962280209105014},
  url      = { 
        https://doi.org/10.1177/0962280209105014
    
},
  abstract = { Complex intervention trials should be able to answer both pragmatic and explanatory questions in order to test the theories motivating the intervention and help understand the underlying nature of the clinical problem being tested. Key to this is the estimation of direct effects of treatment and indirect effects acting through intermediate variables which are measured post-randomisation. Using psychological treatment trials as an example of complex interventions, we review statistical methods which crucially evaluate both direct and indirect effects in the presence of hidden confounding between mediator and outcome. We review the historical literature on mediation and moderation of treatment effects. We introduce two methods from within the existing causal inference literature, principal stratification and structural mean models, and demonstrate how these can be applied in a mediation context before discussing approaches and assumptions necessary for attaining identifiability of key parameters of the basic causal model. Assuming that there is modification by baseline covariates of the effect of treatment (i.e. randomisation) on the mediator (i.e. covariate by treatment interactions), but no direct effect on the outcome of these treatment by covariate interactions leads to the use of instrumental variable methods. We describe how moderation can occur through post-randomisation variables, and extend the principal stratification approach to multiple group methods with explanatory models nested within the principal strata. We illustrate the new methodology with motivating examples of randomised trials from the mental health literature. },
}

@Article{Pearl2014,
  author    = {Judea Pearl and Elias Bareinboim},
  title     = {External Validity: From Do-Calculus to Transportability Across Populations},
  journal   = {Statistical Science},
  year      = {2014},
  volume    = {29},
  number    = {4},
  pages     = {579-595},
  issn      = {08834237, 21688745},
  url       = {http://www.jstor.org/stable/43288500},
  abstract  = {The generalizability of empirical findings to new environments, settings or populations, often called "external validity," is essential in most scientific explorations. This paper treats a particular problem of generalizability, called "transportability," defined as a license to transfer causal effects learned in experimental studies to a new population, in which only observational studies can be conducted. We introduce a formal representation called "selection diagrams" for expressing knowledge about differences and commonalities between populations of interest and, using this representation, we reduce questions of transportability to symbolic derivations in the docalculus. This reduction yields graph-based procedures for deciding, prior to observing any data, whether causal effects in the target population can be inferred from experimental findings in the study population. When the answer is affirmative, the procedures identify what experimental and observational findings need be obtained from the two populations, and how they can be combined to ensure bias-free transport.},
  publisher = {Institute of Mathematical Statistics},
}

@Article{Breiman2001,
  author   = {Breiman, Leo},
  title    = {Random Forests},
  journal  = {Machine Learning},
  year     = {2001},
  volume   = {45},
  number   = {1},
  month    = {Oct},
  pages    = {5--32},
  issn     = {1573-0565},
  doi      = {10.1023/A:1010933404324},
  url      = {https://doi.org/10.1023/A:1010933404324},
  abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund {\&} R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148--156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
  day      = {01},
}

@Article{Efron1986,
  author    = {B. Efron},
  title     = {Why Isn't Everyone a Bayesian?},
  journal   = {The American Statistician},
  year      = {1986},
  volume    = {40},
  number    = {1},
  pages     = {1-5},
  doi       = {10.1080/00031305.1986.10475342},
  eprint    = {http://www.tandfonline.com/doi/pdf/10.1080/00031305.1986.10475342},
  url       = { 
        http://www.tandfonline.com/doi/abs/10.1080/00031305.1986.10475342
    
},
  abstract  = { Abstract Originally a talk delivered at a conference on Bayesian statistics, this article attempts to answer the following question: why is most scientific data analysis carried out in a non-Bayesian framework? The argument consists mainly of some practical examples of data analysis, in which the Bayesian approach is difficult but Fisherian/frequentist solutions are relatively easy. There is a brief discussion of objectivity in statistical analyses and of the difficulties of achieving objectivity within a Bayesian framework. The article ends with a list of practical advantages of Fisherian/frequentist methods, which so far seem to have outweighed the philosophical superiority of Bayesianism. },
  publisher = {Taylor \& Francis},
}

@Article{Alfaro2013,
  author  = {Esteban Alfaro and Mat?as Gamez and Noelia Garc?a},
  title   = {adabag: An R Package for Classification with Boosting and Bagging},
  journal = {Journal of Statistical Software},
  year    = {2013},
  volume  = {54},
  issue   = {2},
}

@InProceedings{Riihimaeki2010,
  author    = {Jaakko Riihimäki and Aki Vehtari},
  title     = {Gaussian processes with monotonicity information},
  booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  year      = {2010},
  editor    = {Yee Whye Teh and Mike Titterington},
  volume    = {9},
  series    = {Proceedings of Machine Learning Research},
  publisher = {PMLR},
  month     = {13--15 May},
  pages     = {645--652},
  url       = {http://proceedings.mlr.press/v9/riihimaki10a.html},
  abstract  = {A method for using monotonicity information in multivariate Gaussian process regression and classification is proposed. Monotonicity information is introduced with virtual derivative observations, and the resulting posterior is approximated with expectation propagation. Behaviour of the method is illustrated with artificial regression examples, and the method is used in a real world health care classification problem to include monotonicity information with respect to one of the covariates.},
  address   = {Chia Laguna Resort, Sardinia, Italy},
  file      = {riihimaki10a.pdf:http\://proceedings.mlr.press/v9/riihimaki10a/riihimaki10a.pdf:PDF},
}

@InCollection{Guo2010,
  author    = {Shengbo Guo and Sanner, Scott and Bonilla, Edwin V},
  title     = {Gaussian Process Preference Elicitation},
  booktitle = {Advances in Neural Information Processing Systems 23},
  year      = {2010},
  editor    = {J. D. Lafferty and C. K. I. Williams and J. Shawe-Taylor and R. S. Zemel and A. Culotta},
  publisher = {Curran Associates, Inc.},
  pages     = {262--270},
  url       = {http://papers.nips.cc/paper/4141-gaussian-process-preference-elicitation.pdf},
}

@InProceedings{Guo2010a,
  author    = {Shengbo Guo and Scott Sanner},
  title     = {Real-time Multiattribute Bayesian Preference Elicitation with Pairwise Comparison Queries},
  booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  year      = {2010},
  editor    = {Yee Whye Teh and Mike Titterington},
  volume    = {9},
  series    = {Proceedings of Machine Learning Research},
  publisher = {PMLR},
  month     = {13--15 May},
  pages     = {289--296},
  url       = {http://proceedings.mlr.press/v9/guo10b.html},
  abstract  = {Preference elicitation (PE) is an important component of interactive decision support systems that aim to make optimal recommendations to users by actively querying their preferences.  In this paper, we outline five principles important for PE in real-world problems: (1) real-time, (2) multiattribute, (3) low cognitive load, (4) robust to noise, and (5) scalable.  In light of these requirements, we introduce an approximate PE framework based on TrueSkill for performing efficient closed-form Bayesian updates and query selection for a multiattribute utility belief state — a novel PE approach that naturally facilitates the efficient evaluation of value of information (VOI) heuristics for use in query selection strategies.  Our best VOI query strategy satisfies all five principles (in contrast to related work) and performs on par with the most accurate (and often computationally intensive) algorithms on experiments with synthetic and real-world datasets.},
  address   = {Chia Laguna Resort, Sardinia, Italy},
  file      = {guo10b.pdf:http\://proceedings.mlr.press/v9/guo10b/guo10b.pdf:PDF},
}

@Misc{Craig2008a,
  author = {Peter Craig and Paul Dieppe and Sally Macintyre and Susan Michie and Irwin Nazareth and Mark Petticrew},
  editor = {{Medical Research Council}},
  title  = {Developing and evaluating complex interventions: new guidance},
  year   = {2008},
  url    = {https://www.mrc.ac.uk/documents/pdf/developing-and-evaluating-complex-interventions/},
}

@Article{Rudolf736,
  author    = {Rudolf, M and Christie, D and McElhone, S and Sahota, P and Dixey, R and Walker, J and Wellings, C},
  title     = {WATCH IT: a community based programme for obese children and adolescents},
  journal   = {Archives of Disease in Childhood},
  year      = {2006},
  volume    = {91},
  number    = {9},
  pages     = {736--739},
  issn      = {0003-9888},
  doi       = {10.1136/adc.2005.089896},
  eprint    = {http://adc.bmj.com/content/91/9/736.full.pdf},
  url       = {http://adc.bmj.com/content/91/9/736},
  abstract  = {Background: The WATCH IT programme was developed to address the needs of obese children from disadvantaged communities in Leeds and has been running since January 2004. Results of the pilot phase, prior to a randomised controlled trial, are presented. Methods: A process evaluation to assess success of implementation was conducted in December 2004. User views (parent and child) were obtained by semi-structured interviews and focus groups. Change in BMI SD score was calculated for children attending between January 2004 and November 2005. Results: A total of 94 children (49 girls, 45 boys), mean age (SD) 12.2 (2.0) years attended. They were moderately to severely obese (mean BMI SDS 3.09 (0.45), with low quality of life and self-image scores. There was a significant reduction in overweight at 6 months (?BMI SD -0.07), especially for teenagers (?BMI SD -0.13) and girls (?BMI SD -0.07). The programme was successfully implemented. By December 2004 mean attendance was 2.1 (0.7) clinics per month, and sports sessions 3.3 (1.7) sessions per month. Fourteen children dropped out and non-attendance was low (only 7.5\% sessions missed in 12 months). Qualitative research indicated significant appreciation of the service, with reported increase in self-confidence and friendships, and reduction in self-harm. Conclusion: WATCH IT offers a model for a community based service for obese children. The programme suggests that effective care can be delivered by health trainers supervised by health professionals, and so potentially provides a cost effective programme within children{\textquoteright}s communities. These findings are encouraging, and need to be substantiated by extension to other locations and evaluation by randomised controlled trial.},
  publisher = {BMJ Publishing Group Ltd},
}

@Article{Simon2000,
  author    = {Richard Simon},
  title     = {[Clinical Trials and Sample Size Considerations: Another Perspective]: Comment},
  journal   = {Statistical Science},
  year      = {2000},
  volume    = {15},
  number    = {2},
  pages     = {103-105},
  issn      = {08834237},
  url       = {http://www.jstor.org/stable/2676722},
  publisher = {Institute of Mathematical Statistics},
}

@Article{Yoo2013,
  author   = {Hong Il Yoo and Denise Doiron},
  title    = {The use of alternative preference elicitation methods in complex discrete choice experiments},
  journal  = {Journal of Health Economics},
  year     = {2013},
  volume   = {32},
  number   = {6},
  pages    = {1166 - 1179},
  issn     = {0167-6296},
  doi      = {https://doi.org/10.1016/j.jhealeco.2013.09.009},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167629613001288},
  abstract = {We analyse stated preference data over nursing jobs collected from two different discrete choice experiments: a multi-profile case best-worst scaling experiment (BWS) prompting selection of the best and worst among alternative jobs, and a profile case BWS wherein the respondents choose the best and worst job attributes. The latter allows identification of additional utility parameters and is believed to be cognitively easier. Results suggest that respondents place greater value on pecuniary over non-pecuniary gains in the multi-profile case. There is little evidence that this discrepancy is induced by the extra cognitive burden of processing several profiles at once in the multi-profile case. We offer thoughts on other likely mechanisms.},
  keywords = {Discrete choice experiment, Preference elicitation, Rank-ordered data, Latent class logit, Best-worst scaling, Maximum-difference model},
}

@Article{Ali2012,
  author  = {Ali, Shehzad and Ronaldson, Sarah},
  title   = {Ordinal preference elicitation methods in health economics and health services research: using discrete choice experiments and ranking methods},
  journal = {British Medical Bulletin},
  year    = {2012},
  volume  = {103},
  number  = {1},
  pages   = {21-44},
  doi     = {10.1093/bmb/lds020},
  eprint  = {/oup/backfile/content_public/journal/bmb/103/1/10.1093/bmb/lds020/2/lds020.pdf},
  url     = { + http://dx.doi.org/10.1093/bmb/lds020},
}

@TechReport{Settles2010,
  author      = {Burr Settles},
  title       = {Active Learning Literature Survey},
  institution = {University of Wisconsin–Madison},
  year        = {2010},
  subtitle    = {Computer Sciences Technical Report 1648},
}

@Book{French2000,
  author    = {Simon French and David Rios Insua},
  title     = {Statistical Decision Theory},
  year      = {2000},
  series    = {Kendall's Library of Statistics},
  number    = {9},
  publisher = {Oxford University Press},
}

@Article{Krzysztofowicz1983,
  author   = {Roman Krzysztofowicz},
  title    = {Strength of preference and risk attitude in utility measurement},
  journal  = {Organizational Behavior and Human Performance},
  year     = {1983},
  volume   = {31},
  number   = {1},
  pages    = {88 - 113},
  issn     = {0030-5073},
  doi      = {https://doi.org/10.1016/0030-5073(83)90114-9},
  url      = {http://www.sciencedirect.com/science/article/pii/0030507383901149},
  abstract = {A relationship between a value function v (compatible with the theory of ordered value differences) and a utility function u (compatible with the expected utility theory) is explored. According to a behavioral interpretation, v encodes the strength of preference while u encodes the strength of preference and risk attitude. The results of two experiments (one conducted in a real-world setting and another in a laboratory) involving 24 cases and the data reported in the literature involving 10 cases support the constant relavie risk attitude hypothesis. The implied unique transformation between v and u is tested as a descriptive model and as a predictive model. The descriptive model is then used for inference concerning several behavioral hypotheses.},
}

@Article{Mason2017,
  author   = {Alexina J Mason and Manuel Gomes and Richard Grieve and Pinar Ulug and Janet T Powell and James Carpenter},
  title    = {Development of a practical approach to expert elicitation for randomised controlled trials with missing health outcomes: Application to the IMPROVE trial},
  journal  = {Clinical Trials},
  year     = {2017},
  volume   = {14},
  number   = {4},
  pages    = {357-367},
  note     = {PMID: 28675302},
  doi      = {10.1177/1740774517711442},
  eprint   = {https://doi.org/10.1177/1740774517711442},
  url      = { 
        https://doi.org/10.1177/1740774517711442
    
},
  abstract = { Background/aims:The analyses of randomised controlled trials with missing data typically assume that, after conditioning on the observed data, the probability of missing data does not depend on the patient’s outcome, and so the data are ‘missing at random’ . This assumption is usually implausible, for example, because patients in relatively poor health may be more likely to drop out. Methodological guidelines recommend that trials require sensitivity analysis, which is best informed by elicited expert opinion, to assess whether conclusions are robust to alternative assumptions about the missing data. A major barrier to implementing these methods in practice is the lack of relevant practical tools for eliciting expert opinion. We develop a new practical tool for eliciting expert opinion and demonstrate its use for randomised controlled trials with missing data.Methods:We develop and illustrate our approach for eliciting expert opinion with the IMPROVE trial (ISRCTN 48334791), an ongoing multi-centre randomised controlled trial which compares an emergency endovascular strategy versus open repair for patients with ruptured abdominal aortic aneurysm. In the IMPROVE trial at 3 months post-randomisation, 21\% of surviving patients did not complete health-related quality of life questionnaires (assessed by EQ-5D-3L). We address this problem by developing a web-based tool that provides a practical approach for eliciting expert opinion about quality of life differences between patients with missing versus complete data. We show how this expert opinion can define informative priors within a fully Bayesian framework to perform sensitivity analyses that allow the missing data to depend upon unobserved patient characteristics.Results:A total of 26 experts, of 46 asked to participate, completed the elicitation exercise. The elicited quality of life scores were lower on average for the patients with missing versus complete data, but there was considerable uncertainty in these elicited values. The missing at random analysis found that patients randomised to the emergency endovascular strategy versus open repair had higher average (95\% credible interval) quality of life scores of 0.062 (?0.005 to 0.130). Our sensitivity analysis that used the elicited expert information as pooled priors found that the gain in average quality of life for the emergency endovascular strategy versus open repair was 0.076 (?0.054 to 0.198).Conclusion:We provide and exemplify a practical tool for eliciting the expert opinion required by recommended approaches to the sensitivity analyses of randomised controlled trials. We show how this approach allows the trial analysis to fully recognise the uncertainty that arises from making alternative, plausible assumptions about the reasons for missing data. This tool can be widely used in the design, analysis and interpretation of future trials, and to facilitate this, materials are available for download. },
}

@Online{NIHR2017,
  author  = {{National Institute for Health Research}},
  title   = {Research for Patient Benefit (RfPB) Programme Guidance on Applying for Feasibility Studies},
  year    = {2017},
  url     = {https://www.nihr.ac.uk/funding-and-support/documents/funding-for-research-studies/research-programmes/RfPB/Guidance%20Documents/Guidance_on_feasibility_studies.pdf},
  comment = {Accessed 16/04/2018},
}

@Article{Dallow2018,
  author   = {Nigel Dallow and Nicky Best and Timothy H Montague},
  title    = {Better decision making in drug development through adoption of formal prior elicitation},
  journal  = {Pharmaceutical Statistics},
  year     = {2018},
  volume   = {0},
  number   = {0},
  doi      = {10.1002/pst.1854},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/pst.1854},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/pst.1854},
  abstract = {With the continued increase in the use of Bayesian methods in drug development, there is a need for statisticians to have tools to develop robust and defensible informative prior distributions. Whilst relevant empirical data should, where possible, provide the basis for such priors, it is often the case that limitations in data and/or our understanding may preclude direct construction of a data?based prior. Formal expert elicitation methods are a key technique that can be used to determine priors in these situations. Within GlaxoSmithKline, we have adopted a structured approach to prior elicitation on the basis of the SHELF elicitation framework and routinely use this in conjunction with calculation of probability of success (assurance) of the next study(s) to inform internal decision making at key project milestones. The aim of this paper is to share our experiences of embedding the use of prior elicitation within a large pharmaceutical company, highlighting both the benefits and challenges of prior elicitation through a series of case studies. We have found that putting team beliefs into the shape of a quantitative probability distribution provides a firm anchor for all internal decision making, enabling teams to provide investment boards with formally appropriate estimates of the probability of trial success as well as robust plans for interim decision rules where appropriate. As an added benefit, the elicitation process provides transparency about the beliefs and risks of the potential medicine, ultimately enabling better portfolio and company?wide decision making.},
  keywords = {assurance, Bayesian, prior elicitation, SHELF},
}

@Article{Crisp2018,
  author   = {Adam Crisp and Sam Miller and Douglas Thompson and Nicky Best},
  title    = {Practical experiences of adopting assurance as a quantitative framework to support decision making in drug development},
  journal  = {Pharmaceutical Statistics},
  year     = {2018},
  volume   = {0},
  number   = {0},
  doi      = {10.1002/pst.1856},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/pst.1856},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/pst.1856},
  abstract = {All clinical trials are designed for success of their primary objectives. Hence, evaluating the probability of success (PoS) should be a key focus at the design stage both to support funding approval from sponsor governance boards and to inform trial design itself. Use of assurance—that is, expected success probability averaged over a prior probability distribution for the treatment effect—to quantify PoS of a planned study has grown across the industry in recent years, and has now become routine within the authors' company. In this paper, we illustrate some of the benefits of systematically adopting assurance as a quantitative framework to support decision making in drug development through several case?studies where evaluation of assurance has proved impactful in terms of trial design and in supporting governance?board reviews of project proposals. In addition, we describe specific features of how the assurance framework has been implemented within our company, highlighting the critical role that prior elicitation plays in this process, and illustrating how the overall assurance calculation may be decomposed into a sequence of conditional PoS estimates which can provide greater insight into how and when different development options are able to discharge risk.},
  keywords = {Bayesian clinical trial design, informative priors, probability of success, prior elicitation},
}

@Article{Lee2008,
  author   = {J Jack Lee and Diane D Liu},
  title    = {A predictive probability design for phase II cancer clinical trials},
  journal  = {Clinical Trials},
  year     = {2008},
  volume   = {5},
  number   = {2},
  pages    = {93-106},
  note     = {PMID: 18375647},
  doi      = {10.1177/1740774508089279},
  eprint   = {https://doi.org/10.1177/1740774508089279},
  url      = { 
        https://doi.org/10.1177/1740774508089279
    
},
  abstract = { Background Two- or three-stage designs are commonly used in phase II cancer clinical trials. These designs possess good frequentist properties and allow early termination of the trial when the interim data indicate that the experimental regimen is inefficacious. The rigid study design, however, can be difficult to follow exactly because the response has to be evaluated at prespecified fixed number of patients.Purpose Our goal is to develop an efficient and flexible design that possesses desirable statistical properties.Methods A flexible design based on Bayesian predictive probability and the minimax criterion is constructed. A three-dimensional search algorithm is implemented to determine the design parameters.Results The new design controls type I and type II error rates, and allows continuous monitoring of the trial outcome. Consequently, under the null hypothesis when the experimental treatment is not efficacious, the design is more efficient in stopping the trial earlier, which results in a smaller expected sample size. Exact computation and simulation studies demonstrate that the predictive probability design possesses good operating characteristics.Limitations The predictive probability design is more computationally intensive than two- or three-stage designs. Similar to all designs with early stopping due to futility, the resulting estimate of treatment efficacy may be biased.Conclusions The predictive probability design is efficient and remains robust in controlling type I and type II error rates when the trial conduct deviates from the original design. It is more adaptable than traditional multi-stage designs in evaluating the study outcome, hence, it is easier to implement. S-PLUS/R programs are provided to assist the study design. Clinical Trials 2008; 5: 93—106. http://ctj.sagepub.com },
}

@Article{Storey2003,
  author    = {John D. Storey},
  title     = {The Positive False Discovery Rate: A Bayesian Interpretation and the q-Value},
  journal   = {The Annals of Statistics},
  year      = {2003},
  volume    = {31},
  number    = {6},
  pages     = {2013--2035},
  issn      = {00905364},
  url       = {http://www.jstor.org/stable/3448445},
  abstract  = {Multiple hypothesis testing is concerned with controlling the rate of false positives when testing several hypotheses simultaneously. One multiple hypothesis testing error measure is the false discovery rate (FDR), which is loosely defined to be the expected proportion of false positives among all significant hypotheses. The FDR is especially appropriate for exploratory analyses in which one is interested in finding several significant results among many tests. In this work, we introduce a modified version of the FDR called the "positive false discovery rate" (pFDR). We discuss the advantages and disadvantages of the pFDR and investigate its statistical properties. When assuming the test statistics follow a mixture distribution, we show that the pFDR can be written as a Bayesian posterior probability and can be connected to classification theory. These properties remain asymptotically true under fairly general conditions, even under certain forms of dependence. Also, a new quantity called the "q -value" is introduced and investigated, which is a natural "Bayesian posterior p-value," or rather the pFDR analogue of the p-value.},
  publisher = {Institute of Mathematical Statistics},
}

@Article{Morgan2018,
  author    = {Morgan, Ben AND Hejdenberg, Jennie AND Hinrichs-Krapels, Saba AND Armstrong, David},
  title     = {Do feasibility studies contribute to, or avoid, waste in research?},
  journal   = {PLOS ONE},
  year      = {2018},
  volume    = {13},
  number    = {4},
  month     = {04},
  pages     = {1-8},
  doi       = {10.1371/journal.pone.0195951},
  url       = {https://doi.org/10.1371/journal.pone.0195951},
  abstract  = {In the context of avoiding research waste, the conduct of a feasibility study before a clinical trial should reduce the risk that further resources will be committed to a trial that is likely to ‘fail’. However, there is little evidence indicating whether feasibility studies add to or reduce waste in research. Feasibility studies funded by the National Institute for Health Research’s (NIHR) Research for Patient Benefit (RfPB) programme were examined to determine how many had published their findings, how many had applied for further funding for a full trial and the timeframe in which both of these occurred. A total of 120 feasibility studies which had closed by May 2016 were identified and each Principal Investigator (PI) was sent a questionnaire of which 89 responses were received and deemed suitable for analysis. Based on self reported answers from the PIs a total of 57 feasibility studies were judged as feasible, 20 were judged not feasible and for 12 it was judged as uncertain whether a full trial was feasible. The RfPB programme had spent approximately £19.5m on the 89 feasibility studies of which 16 further studies had been subsequently funded to a total of £16.8m. The 20 feasibility studies which were judged as not feasible potentially saved up to approximately £20m of further research funding which would likely to have not completed successfully. The average RfPB feasibility study took 31 months (range 18 to 48) to complete and cost £219,048 (range £72,031 to £326,830) and the average full trial funded from an RfPB feasibility study took 42 months (range 26 to 55) to complete and cost £1,163,996 (range £321,403 to £2,099,813). The average timeframe of feasibility study and full trial was 72 months (range 56 to 91), however in addition to this time an average of 10 months (range -7 to 29) was taken between the end of the feasibility study and the application for the full trial, and a further average of 18 months (range 13 to 28) between the application for the full trial and the start of the full trial. Approximately 58% of the 89 feasibility studies had published their findings with the majority of the remaining studies still planning to publish. Due to the long time frames involved a number of studies were still in the process of publishing the feasibility findings and/or applying for a full trial. Feasibility studies are potentially useful at avoiding waste and de-risking funding investments of more expensive full trials, however there is a clear time delay and therefore some potential waste in the existing research pathway.},
  publisher = {Public Library of Science},
}

@Book{Keeney1976,
  author    = {Keeney, Ralph L. and Raiffa, Howard},
  title     = {Decisions with multiple objectives: preferences and value tradeoffs},
  year      = {1976},
  publisher = {John Wiley \& Sons},
}

@Article{Schie2014,
  author   = {Schie S. and Moerbeek M.},
  title    = {Re-estimating sample size in cluster randomised trials with active recruitment within clusters},
  journal  = {Statistics in Medicine},
  year     = {2014},
  volume   = {33},
  number   = {19},
  pages    = {3253-3268},
  doi      = {10.1002/sim.6172},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.6172},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.6172},
  abstract = {AbstractOften only a limited number of clusters can be obtained in cluster randomised trials, although many potential participants can be recruited within each cluster. Thus, active recruitment is feasible within the clusters. To obtain an efficient sample size in a cluster randomised trial, the cluster level and individual level variance should be known before the study starts, but this is often not the case. We suggest using an internal pilot study design to address this problem of unknown variances. A pilot can be useful to re?estimate the variances and re?calculate the sample size during the trial. Using simulated data, it is shown that an initially low or high power can be adjusted using an internal pilot with the type I error rate remaining within an acceptable range. The intracluster correlation coefficient can be re?estimated with more precision, which has a positive effect on the sample size. We conclude that an internal pilot study design may be used if active recruitment is feasible within a limited number of clusters. Copyright © 2014 John Wiley \& Sons, Ltd.},
  keywords = {a priori power analysis, hierarchical data, intracluster correlation coefficient, type I error},
}

@Article{Scannell2016,
  author    = {Scannell, Jack W. AND Bosley, Jim},
  title     = {When Quality Beats Quantity: Decision Theory, Drug Discovery, and the Reproducibility Crisis},
  journal   = {PLOS ONE},
  year      = {2016},
  volume    = {11},
  number    = {2},
  month     = {02},
  pages     = {1-21},
  doi       = {10.1371/journal.pone.0147215},
  url       = {https://doi.org/10.1371/journal.pone.0147215},
  abstract  = {A striking contrast runs through the last 60 years of biopharmaceutical discovery, research, and development. Huge scientific and technological gains should have increased the quality of academic science and raised industrial R&D efficiency. However, academia faces a "reproducibility crisis"; inflation-adjusted industrial R&D costs per novel drug increased nearly 100 fold between 1950 and 2010; and drugs are more likely to fail in clinical development today than in the 1970s. The contrast is explicable only if powerful headwinds reversed the gains and/or if many "gains" have proved illusory. However, discussions of reproducibility and R&D productivity rarely address this point explicitly. The main objectives of the primary research in this paper are: (a) to provide quantitatively and historically plausible explanations of the contrast; and (b) identify factors to which R&D efficiency is sensitive. We present a quantitative decision-theoretic model of the R&D process. The model represents therapeutic candidates (e.g., putative drug targets, molecules in a screening library, etc.) within a “measurement space", with candidates' positions determined by their performance on a variety of assays (e.g., binding affinity, toxicity, in vivo efficacy, etc.) whose results correlate to a greater or lesser degree. We apply decision rules to segment the space, and assess the probability of correct R&D decisions. We find that when searching for rare positives (e.g., candidates that will successfully complete clinical development), changes in the predictive validity of screening and disease models that many people working in drug discovery would regard as small and/or unknowable (i.e., an 0.1 absolute change in correlation coefficient between model output and clinical outcomes in man) can offset large (e.g., 10 fold, even 100 fold) changes in models’ brute-force efficiency. We also show how validity and reproducibility correlate across a population of simulated screening and disease models. We hypothesize that screening and disease models with high predictive validity are more likely to yield good answers and good treatments, so tend to render themselves and their diseases academically and commercially redundant. Perhaps there has also been too much enthusiasm for reductionist molecular models which have insufficient predictive validity. Thus we hypothesize that the average predictive validity of the stock of academically and industrially "interesting" screening and disease models has declined over time, with even small falls able to offset large gains in scientific knowledge and brute-force efficiency. The rate of creation of valid screening and disease models may be the major constraint on R&D productivity.},
  publisher = {Public Library of Science},
}

@Article{Dumas-Mallet2017,
  author    = {Dumas-Mallet, Estelle and Button, Katherine S. and Boraud, Thomas and Gonon, Francois and Munaf{\`o}, Marcus R.},
  title     = {Low statistical power in biomedical science: a review of three human research domains},
  journal   = {Royal Society Open Science},
  year      = {2017},
  volume    = {4},
  number    = {2},
  doi       = {10.1098/rsos.160254},
  eprint    = {http://rsos.royalsocietypublishing.org/content/4/2/160254.full.pdf},
  url       = {http://rsos.royalsocietypublishing.org/content/4/2/160254},
  abstract  = {Studies with low statistical power increase the likelihood that a statistically significant finding represents a false positive result. We conducted a review of meta-analyses of studies investigating the association of biological, environmental or cognitive parameters with neurological, psychiatric and somatic diseases, excluding treatment studies, in order to estimate the average statistical power across these domains. Taking the effect size indicated by a meta-analysis as the best estimate of the likely true effect size, and assuming a threshold for declaring statistical significance of 5\%, we found that approximately 50\% of studies have statistical power in the 0{\textendash}10\% or 11{\textendash}20\% range, well below the minimum of 80\% that is often considered conventional. Studies with low statistical power appear to be common in the biomedical sciences, at least in the specific subject areas captured by our search strategy. However, we also observe evidence that this depends in part on research methodology, with candidate gene studies showing very low average power and studies using cognitive/behavioural measures showing high average power. This warrants further investigation.},
  publisher = {The Royal Society},
}

@Article{Jia2015,
  author   = {Jia, Bin and Lynn, Henry S.},
  title    = {A sample size planning approach that considers both statistical significance and clinical significance},
  journal  = {Trials},
  year     = {2015},
  volume   = {16},
  number   = {1},
  month    = {May},
  pages    = {213},
  issn     = {1745-6215},
  doi      = {10.1186/s13063-015-0727-9},
  url      = {https://doi.org/10.1186/s13063-015-0727-9},
  abstract = {The CONSORT statement requires clinical trials to report confidence intervals, which help to assess the precision and clinical importance of the treatment effect. Conventional sample size calculations for clinical trials, however, only consider issues of statistical significance (that is, significance level and power).},
  day      = {12},
}

@Article{Tavernier2015,
  author    = {Tavernier, Elsa AND Giraudeau, Bruno},
  title     = {Sample Size Calculation: Inaccurate A Priori Assumptions for Nuisance Parameters Can Greatly Affect the Power of a Randomized Controlled Trial},
  journal   = {PLOS ONE},
  year      = {2015},
  volume    = {10},
  number    = {7},
  month     = {07},
  pages     = {1-8},
  doi       = {10.1371/journal.pone.0132578},
  url       = {https://doi.org/10.1371/journal.pone.0132578},
  abstract  = {We aimed to examine the extent to which inaccurate assumptions for nuisance parameters used to calculate sample size can affect the power of a randomized controlled trial (RCT). In a simulation study, we separately considered an RCT with continuous, dichotomous or time-to-event outcomes, with associated nuisance parameters of standard deviation, success rate in the control group and survival rate in the control group at some time point, respectively. For each type of outcome, we calculated a required sample size N for a hypothesized treatment effect, an assumed nuisance parameter and a nominal power of 80%. We then assumed a nuisance parameter associated with a relative error at the design stage. For each type of outcome, we randomly drew 10,000 relative errors of the associated nuisance parameter (from empirical distributions derived from a previously published review). Then, retro-fitting the sample size formula, we derived, for the pre-calculated sample size N, the real power of the RCT, taking into account the relative error for the nuisance parameter. In total, 23%, 0% and 18% of RCTs with continuous, binary and time-to-event outcomes, respectively, were underpowered (i.e., the real power was < 60%, as compared with the 80% nominal power); 41%, 16% and 6%, respectively, were overpowered (i.e., with real power > 90%). Even with proper calculation of sample size, a substantial number of trials are underpowered or overpowered because of imprecise knowledge of nuisance parameters. Such findings raise questions about how sample size for RCTs should be determined.},
  publisher = {Public Library of Science},
}

@Article{Norman2012,
  author  = {Geoffrey Norman and Sandra Monteiro and Suzette Salama},
  title   = {Sample size calculations: should the emperor’s clothes be off the peg or made to measure?},
  journal = {BMJ},
  year    = {2012},
}

@Article{Grieve2015,
  author   = {Grieve, Andrew P.},
  title    = {How to test hypotheses if you must},
  journal  = {Pharmaceutical Statistics},
  year     = {2015},
  volume   = {14},
  number   = {2},
  pages    = {139-150},
  doi      = {10.1002/pst.1667},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/pst.1667},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/pst.1667},
  abstract = {Drug development is not the only industrial?scientific enterprise subject to government regulations. In some fields of ecology and environmental sciences, the application of statistical methods is also regulated by ordinance. Over the past 20years, ecologists and environmental scientists have argued against an unthinking application of null hypothesis significance tests. More recently, Canadian ecologists have suggested a new approach to significance testing, taking account of the costs of both type I and type II errors. In this paper, we investigate the implications of this for testing in drug development and demonstrate that its adoption leads directly to the likelihood principle and Bayesian approaches. Copyright © 2015 John Wiley \& Sons, Ltd.},
  keywords = {hypothesis tests, null hypothesis significance tests, type I error, type II error, power, planning of experiments, sample sizing, Neyman–Pearson lemma, likelihood principle, sampling frame, Lindley's paradox, Bayesian test},
}

@Article{Vickers2003,
  author        = {Vickers, Andrew J.},
  title         = {Underpowering in randomized trials reporting a sample size calculation},
  journal       = {Journal of Clinical Epidemiology},
  year          = {2003},
  volume        = {56},
  number        = {8},
  month         = aug,
  pages         = {717--720},
  issn          = {0895-4356},
  doi           = {10.1016/s0895-4356(03)00141-0},
  url           = {http://dx.doi.org/10.1016/S0895-4356(03)00141-0},
  __markedentry = {[meddwilb:]},
  abstract      = {Objective: The objective of this study was to determine whether standard deviations (SDs) used in sample size calculations are smaller than those found in the resulting study sample, thereby leading to underpowered studies.Method: The predicted SD used in the sample size calculation and the actual SD of the study sample were recorded for randomized trials recently published in one of four major journals.Results: Sample SD was greater than predicted SD for 80% of endpoints. About one quarter of trials required five times as many patients as specified in the sample size calculation.Conclusion: Trials reporting sample size calculations for continuous endpoints published in the most reputable medical journals are often underpowered. There seems to be insufficient understanding that the SD of a sample of patients is a random variable, associated with imprecision, that cannot easily be extrapolated from one population to another.
Objective: The objective of this study was to determine whether standard deviations (SDs) used in sample size calculations are smaller than those found in the resulting study sample, thereby leading to underpowered studies.Method: The predicted SD used in the sample size calculation and the actual SD of the study sample were recorded for randomized trials recently published in one of four major journals.Results: Sample SD was greater than predicted SD for 80% of endpoints. About one quarter of trials required five times as many patients as specified in the sample size calculation.Conclusion: Trials reporting sample size calculations for continuous endpoints published in the most reputable medical journals are often underpowered. There seems to be insufficient understanding that the SD of a sample of patients is a random variable, associated with imprecision, that cannot easily be extrapolated from one population to another.},
  comment       = {doi: 10.1016/S0895-4356(03)00141-0},
  publisher     = {Elsevier},
}

@Article{Borm2007,
  author        = {Borm, George F. and van der Wilt, Gert J. and Kremer, Jan A. M. and Zielhuis, Gerhard A.},
  title         = {A generalized concept of power helped to choose optimal endpoints in clinical trials},
  journal       = {Journal of Clinical Epidemiology},
  year          = {2007},
  volume        = {60},
  number        = {4},
  month         = apr,
  pages         = {375--381},
  issn          = {0895-4356},
  doi           = {10.1016/j.jclinepi.2006.06.015},
  url           = {http://dx.doi.org/10.1016/j.jclinepi.2006.06.015},
  __markedentry = {[meddwilb:6]},
  abstract      = {ObjectivesA clinical trial may have multiple objectives. Sometimes the results for several parameters may need to be significant or meet certain other criteria. In such cases, it is important to evaluate the probability that all these objectives will be met, rather than the probability that each will be met. The purpose of this article is to introduce a definition of power that is tailored to handle this situation and that is helpful for the design of such trials.
ObjectivesA clinical trial may have multiple objectives. Sometimes the results for several parameters may need to be significant or meet certain other criteria. In such cases, it is important to evaluate the probability that all these objectives will be met, rather than the probability that each will be met. The purpose of this article is to introduce a definition of power that is tailored to handle this situation and that is helpful for the design of such trials.},
  comment       = {doi: 10.1016/j.jclinepi.2006.06.015},
  publisher     = {Elsevier},
}

@Article{Chen2013,
  author   = {Henian Chen and Nanhua Zhang and Xiaosun Lu and Sophie Chen},
  title    = {Caution regarding the choice of standard deviations to guide sample size calculations in clinical trials},
  journal  = {Clinical Trials},
  year     = {2013},
  volume   = {10},
  number   = {4},
  pages    = {522-529},
  note     = {PMID: 23794405},
  doi      = {10.1177/1740774513490250},
  eprint   = {https://doi.org/10.1177/1740774513490250},
  url      = { 
        https://doi.org/10.1177/1740774513490250
    
},
  abstract = { BackgroundThe method used to determine choice of standard deviation (SD) is inadequately reported in clinical trials. Underestimations of the population SD may result in underpowered clinical trials.PurposeThis study demonstrates how using the wrong method to determine population SD can lead to inaccurate sample sizes and underpowered studies, and offers recommendations to maximize the likelihood of achieving adequate statistical power.MethodsWe review the practice of reporting sample size and its effect on the power of trials published in major journals. Simulated clinical trials were used to compare the effects of different methods of determining SD on power and sample size calculations.ResultsPrior to 1996, sample size calculations were reported in just 1\%–42\% of clinical trials. This proportion increased from 38\% to 54\% after the initial Consolidated Standards of Reporting Trials (CONSORT) was published in 1996, and from 64\% to 95\% after the revised CONSORT was published in 2001. Nevertheless, underpowered clinical trials are still common. Our simulated data showed that all minimal and 25th-percentile SDs fell below 44 (the population SD), regardless of sample size (from 5 to 50). For sample sizes 5 and 50, the minimum sample SDs underestimated the population SD by 90.7\% and 29.3\%, respectively. If only one sample was available, there was less than 50\% chance that the actual power equaled or exceeded the planned power of 80\% for detecting a median effect size (Cohen’s d = 0.5) when using the sample SD to calculate the sample size. The proportions of studies with actual power of at least 80\% were about 95\%, 90\%, 85\%, and 80\% when we used the larger SD, 80\% upper confidence limit (UCL) of SD, 70\% UCL of SD, and 60\% UCL of SD to calculate the sample size, respectively. When more than one sample was available, the weighted average SD resulted in about 50\% of trials being underpowered; the proportion of trials with power of 80\% increased from 90\% to 100\% when the 75th percentile and the maximum SD from 10 samples were used. Greater sample size is needed to achieve a higher proportion of studies having actual power of 80\%.LimitationsThis study only addressed sample size calculation for continuous outcome variables.ConclusionsWe recommend using the 60\% UCL of SD, maximum SD, 80th-percentile SD, and 75th-percentile SD to calculate sample size when 1 or 2 samples, 3 samples, 4–5 samples, and more than 5 samples of data are available, respectively. Using the sample SD or average SD to calculate sample size should be avoided. },
}

@Article{Freedman1982,
  author   = {Freedman, L. S.},
  title    = {Tables of the number of patients required in clinical trials using the logrank test},
  journal  = {Statistics in Medicine},
  year     = {1982},
  volume   = {1},
  number   = {2},
  pages    = {121-129},
  doi      = {10.1002/sim.4780010204},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.4780010204},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.4780010204},
  abstract = {Abstract The logrank test is commonly used in the analysis of clinical trials in chronic diseases such as cancer. Existing tables for the number of patients required in such trials are based on the direct comparison of two proportions. This paper presents tables of numbers required in clinical trials using the logrank test and describes their use. The numbers required are considerably smaller than those in existing tables when the event‐free proportions are small, but otherwise comparable.},
  keywords = {Logrank test, Clinical trials, Power},
}

@Article{Edward1992,
  author   = {Lakatos Edward and Lan K. K. Gordon},
  title    = {A comparison of sample size methods for the logrank statistic},
  journal  = {Statistics in Medicine},
  year     = {1992},
  volume   = {11},
  number   = {2},
  pages    = {179-191},
  doi      = {10.1002/sim.4780110205},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.4780110205},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.4780110205},
  abstract = {Abstract Several methods are available for sample size calculation for clinical trials when survival curves are to be compared using the logrank statistic. We discuss advantages and disadvantages of some of these methods, and present simulation results under exponential, proportional hazards and non‐proportional hazard situations.},
}

@Article{Landau2018,
  author   = {Sabine Landau and Richard Emsley and Graham Dunn},
  title    = {Beyond total treatment effects in randomised controlled trials: Baseline measurement of intermediate outcomes needed to reduce confounding in mediation investigations},
  journal  = {Clinical Trials},
  year     = {2018},
  volume   = {15},
  number   = {3},
  pages    = {247-256},
  note     = {PMID: 29552919},
  doi      = {10.1177/1740774518760300},
  eprint   = {https://doi.org/10.1177/1740774518760300},
  url      = { 
        https://doi.org/10.1177/1740774518760300
    
},
  abstract = { Background:Random allocation avoids confounding bias when estimating the average treatment effect. For continuous outcomes measured at post-treatment as well as prior to randomisation (baseline), analyses based on (A) post-treatment outcome alone, (B) change scores over the treatment phase or (C) conditioning on baseline values (analysis of covariance) provide unbiased estimators of the average treatment effect. The decision to include baseline values of the clinical outcome in the analysis is based on precision arguments, with analysis of covariance known to be most precise. Investigators increasingly carry out explanatory analyses to decompose total treatment effects into components that are mediated by an intermediate continuous outcome and a non-mediated part. Traditional mediation analysis might be performed based on (A) post-treatment values of the intermediate and clinical outcomes alone, (B) respective change scores or (C) conditioning on baseline measures of both intermediate and clinical outcomes.Methods:Using causal diagrams and Monte Carlo simulation, we investigated the performance of the three competing mediation approaches. We considered a data generating model that included three possible confounding processes involving baseline variables: The first two processes modelled baseline measures of the clinical variable or the intermediate variable as common causes of post-treatment measures of these two variables. The third process allowed the two baseline variables themselves to be correlated due to past common causes. We compared the analysis models implied by the competing mediation approaches with this data generating model to hypothesise likely biases in estimators, and tested these in a simulation study. We applied the methods to a randomised trial of pragmatic rehabilitation in patients with chronic fatigue syndrome, which examined the role of limiting activities as a mediator.Results:Estimates of causal mediation effects derived by approach (A) will be biased if one of the three processes involving baseline measures of intermediate or clinical outcomes is operating. Necessary assumptions for the change score approach (B) to provide unbiased estimates under either process include the independence of baseline measures and change scores of the intermediate variable. Finally, estimates provided by the analysis of covariance approach (C) were found to be unbiased under all the three processes considered here. When applied to the example, there was evidence of mediation under all methods but the estimate of the indirect effect depended on the approach used with the proportion mediated varying from 57\% to 86\%.Conclusion:Trialists planning mediation analyses should measure baseline values of putative mediators as well as of continuous clinical outcomes. An analysis of covariance approach is recommended to avoid potential biases due to confounding processes involving baseline measures of intermediate or clinical outcomes, and not simply for increased precision. },
}

@Article{Lamont2018,
  author   = {Andrea Lamont and Michael D Lyons and Thomas Jaki and Elizabeth Stuart and Daniel J Feaster and Kukatharmini Tharmaratnam and Daniel Oberski and Hemant Ishwaran and Dawn K Wilson and M Lee Van Horn},
  title    = {Identification of predicted individual treatment effects in randomized clinical trials},
  journal  = {Statistical Methods in Medical Research},
  year     = {2018},
  volume   = {27},
  number   = {1},
  pages    = {142-157},
  doi      = {10.1177/0962280215623981},
  eprint   = {https://doi.org/10.1177/0962280215623981},
  url      = { 
        https://doi.org/10.1177/0962280215623981
    
},
  abstract = { In most medical research, treatment effectiveness is assessed using the average treatment effect or some version of subgroup analysis. The practice of individualized or precision medicine, however, requires new approaches that predict how an individual will respond to treatment, rather than relying on aggregate measures of effect. In this study, we present a conceptual framework for estimating individual treatment effects, referred to as predicted individual treatment effects. We first apply the predicted individual treatment effect approach to a randomized controlled trial designed to improve behavioral and physical symptoms. Despite trivial average effects of the intervention, we show substantial heterogeneity in predicted individual treatment response using the predicted individual treatment effect approach. The predicted individual treatment effects can be used to predict individuals for whom the intervention may be most effective (or harmful). Next, we conduct a Monte Carlo simulation study to evaluate the accuracy of predicted individual treatment effects. We compare the performance of two methods used to obtain predictions: multiple imputation and non-parametric random decision trees. Results showed that, on average, both predictive methods produced accurate estimates at the individual level; however, the random decision trees tended to underestimate the predicted individual treatment effect for people at the extreme and showed more variability in predictions across repetitions compared to the imputation approach. Limitations and future directions are discussed. },
}

@Article{Breukelen2015,
  author   = {Gerard JP van Breukelen and Math JJM Candel},
  title    = {Efficient design of cluster randomized and multicentre trials with unknown intraclass correlation},
  journal  = {Statistical Methods in Medical Research},
  year     = {2015},
  volume   = {24},
  number   = {5},
  pages    = {540-556},
  note     = {PMID: 21937473},
  doi      = {10.1177/0962280211421344},
  eprint   = {https://doi.org/10.1177/0962280211421344},
  url      = { 
        https://doi.org/10.1177/0962280211421344
    
},
  abstract = { For cluster randomized and multicentre trials evaluating the effect of a treatment on persons nested within clusters, equations have been published to compute the optimal sample sizes at the cluster and person level as a function of sampling costs and intraclass correlation (ICC). Here, optimal means maximum power and precision for a given sampling budget, or minimum sampling costs for a given power and precision. However, the ICC is usually unknown, and the optimal sample sizes depend strongly on this ICC. To overcome this local optimality problem, this study presents Maximin designs (MMDs) based on relative efficiency (RE) and efficiency. These designs perform well over a range of possible ICC values either in terms of RE compared with the locally optimal designs, or in terms of minimum efficiency (maximum variance) of the treatment effect estimator. The use of MMDs is illustrated using information from many cluster randomized trials in primary care. It is concluded that MMDs and the optimal design for an ICC halfway its assumed range are efficient for a range of ICC values and recommendable for practical use. This requires that trial reports mention the study cost per cluster and person. },
}

@Article{Miller2018,
  author   = {Miller, Frank and Zohar, Sarah and Stallard, Nigel and Madan, Jason and Posch, Martin and Hee, Siew Wan and Pearce, Michael and Vågerö, Mårten and Day, Simon},
  title    = {Approaches to sample size calculation for clinical trials in rare diseases},
  journal  = {Pharmaceutical Statistics},
  year     = {2018},
  volume   = {17},
  number   = {3},
  pages    = {214-230},
  doi      = {10.1002/pst.1848},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/pst.1848},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/pst.1848},
  abstract = {We discuss 3 alternative approaches to sample size calculation: traditional sample size calculation based on power to show a statistically significant effect, sample size calculation based on assurance, and sample size based on a decision‐theoretic approach. These approaches are compared head‐to‐head for clinical trial situations in rare diseases. Specifically, we consider 3 case studies of rare diseases (Lyell disease, adult‐onset Still disease, and cystic fibrosis) with the aim to plan the sample size for an upcoming clinical trial. We outline in detail the reasonable choice of parameters for these approaches for each of the 3 case studies and calculate sample sizes. We stress that the influence of the input parameters needs to be investigated in all approaches and recommend investigating different sample size approaches before deciding finally on the trial size. Highly influencing for the sample size are choice of treatment effect parameter in all approaches and the parameter for the additional cost of the new treatment in the decision‐theoretic approach. These should therefore be discussed extensively.},
  keywords = {assurance, clinical trial, decision theory, rare disease, sample size calculation},
}

@Article{Brakenhoff2018,
  author   = {TB Brakenhoff and KCB Roes and S Nikolakopoulos},
  title    = {Bayesian sample size re-estimation using power priors},
  journal  = {Statistical Methods in Medical Research},
  year     = {2018},
  volume   = {0},
  number   = {0},
  pages    = {0962280218772315},
  note     = {PMID: 29717945},
  doi      = {10.1177/0962280218772315},
  eprint   = {https://doi.org/10.1177/0962280218772315},
  url      = { 
        https://doi.org/10.1177/0962280218772315
    
},
  abstract = { The sample size of a randomized controlled trial is typically chosen in order for frequentist operational characteristics to be retained. For normally distributed outcomes, an assumption for the variance needs to be made which is usually based on limited prior information. Especially in the case of small populations, the prior information might consist of only one small pilot study. A Bayesian approach formalizes the aggregation of prior information on the variance with newly collected data. The uncertainty surrounding prior estimates can be appropriately modelled by means of prior distributions. Furthermore, within the Bayesian paradigm, quantities such as the probability of a conclusive trial are directly calculated. However, if the postulated prior is not in accordance with the true variance, such calculations are not trustworthy. In this work we adapt previously suggested methodology to facilitate sample size re-estimation. In addition, we suggest the employment of power priors in order for operational characteristics to be controlled. },
}

@Article{McCray2017,
  author   = {McCray, Gareth P. J. and Titman, Andrew C. and Ghaneh, Paula and Lancaster, Gillian A.},
  title    = {Sample size re-estimation in paired comparative diagnostic accuracy studies with a binary response},
  journal  = {BMC Medical Research Methodology},
  year     = {2017},
  volume   = {17},
  number   = {1},
  month    = {Jul},
  pages    = {102},
  issn     = {1471-2288},
  doi      = {10.1186/s12874-017-0386-5},
  url      = {https://doi.org/10.1186/s12874-017-0386-5},
  abstract = {The sample size required to power a study to a nominal level in a paired comparative diagnostic accuracy study, i.e. studies in which the diagnostic accuracy of two testing procedures is compared relative to a gold standard, depends on the conditional dependence between the two tests - the lower the dependence the greater the sample size required. A priori, we usually do not know the dependence between the two tests and thus cannot determine the exact sample size required. One option is to use the implied sample size for the maximal negative dependence, giving the largest possible sample size. However, this is potentially wasteful of resources and unnecessarily burdensome on study participants as the study is likely to be overpowered. A more accurate estimate of the sample size can be determined at a planned interim analysis point where the sample size is re-estimated.},
  day      = {14},
}

@Article{Manju2015,
  author   = {Md. Abu Manju and Math JJM Candel and Martijn PF Berger},
  title    = {Optimal and maximin sample sizes for multicentre cost-effectiveness trials},
  journal  = {Statistical Methods in Medical Research},
  year     = {2015},
  volume   = {24},
  number   = {5},
  pages    = {513-539},
  note     = {PMID: 25656551},
  doi      = {10.1177/0962280215569293},
  eprint   = {https://doi.org/10.1177/0962280215569293},
  url      = { 
        https://doi.org/10.1177/0962280215569293
    
},
  abstract = { This paper deals with the optimal sample sizes for a multicentre trial in which the cost-effectiveness of two treatments in terms of net monetary benefit is studied. A bivariate random-effects model, with the treatment-by-centre interaction effect being random and the main effect of centres fixed or random, is assumed to describe both costs and effects. The optimal sample sizes concern the number of centres and the number of individuals per centre in each of the treatment conditions. These numbers maximize the efficiency or power for given research costs or minimize the research costs at a desired level of efficiency or power. Information on model parameters and sampling costs are required to calculate these optimal sample sizes. In case of limited information on relevant model parameters, sample size formulas are derived for so-called maximin sample sizes which guarantee a power level at the lowest study costs. Four different maximin sample sizes are derived based on the signs of the lower bounds of two model parameters, with one case being worst compared to others. We numerically evaluate the efficiency of the worst case instead of using others. Finally, an expression is derived for calculating optimal and maximin sample sizes that yield sufficient power to test the cost-effectiveness of two treatments. },
}

@Article{Protheroe2000,
  author    = {Protheroe, Joanne and Smeeth, Liam and Fahey, Tom and Montgomery, Alan A and Peters, Tim J},
  title     = {The impact of patients{\textquoteright} preferences on the treatment of atrial fibrillation: observational study of patient based decision analysisCommentary: patients, preferences, and evidence},
  journal   = {BMJ},
  year      = {2000},
  volume    = {320},
  number    = {7246},
  pages     = {1380--1384},
  issn      = {0959-8138},
  doi       = {10.1136/bmj.320.7246.1380},
  eprint    = {https://www.bmj.com/content/320/7246/1380.full.pdf},
  url       = {https://www.bmj.com/content/320/7246/1380},
  abstract  = {Objective: To investigate the impact of patients{\textquoteright} preferences for the treatment of atrial fibrillation, by using individualised decision analysis combining probability and utility assessments into a decision tree.Design: Observational study based on interviews with patients.Setting: Eight general practices in Avon.Participants: 260 randomly selected patients aged 70{\textendash}85 years with atrial fibrillation.Main outcome measures: Patients{\textquoteright} treatment preferences regarding anticoagulation treatment (warfarin) after individualised decision analysis; comparison of these preferences with treatment guidelines on the basis of comorbidity and absolute risk and compared with current prescription.Results: Of 195 eligible patients, 97 participated in decision making using decision analysis. Among these 97, the decision analysis indicated that 59 (61\%; 95\% confidence interval 50\% to 71\%) would prefer anticoagulation treatment{\textemdash}considerably fewer than those who would be recommended treatment according to guidelines. There was marked disagreement between the decision analysis and guideline recommendations (kappa =0.25 or less). Of 38 patients whose decision analysis indicated a preference for anticoagulation, 17 (45\%) were being prescribed warfarin; on the other hand, 28 (47\%) of 59 patients were not being prescribed warfarin although the results of their decision analysis suggested they wanted to be.Conclusions: In the context of shared decision making, individualised decision analysis is valuable in a sizeable proportion of elderly patients with atrial fibrillation. Taking account of patients{\textquoteright} preferences would lead to fewer prescriptions for warfarin than under published guideline recommendations. Decision analysis as a shared decision making tool should be evaluated in a randomised controlled trial.The impact of patients{\textquoteright} preferences on the treatment of atrial fibrillation: observational study of patient based decision analysisObjective: To investigate the impact of patients{\textquoteright} preferences for the treatment of atrial fibrillation, by using individualised decision analysis combining probability and utility assessments into a decision tree.Design: Observational study based on interviews with patients.Setting: Eight general practices in Avon.Participants: 260 randomly selected patients aged 70{\textendash}85 years with atrial fibrillation.Main outcome measures: Patients{\textquoteright} treatment preferences regarding anticoagulation treatment (warfarin) after individualised decision analysis; comparison of these preferences with treatment guidelines on the basis of comorbidity and absolute risk and compared with current prescription.Results: Of 195 eligible patients, 97 participated in decision making using decision analysis. Among these 97, the decision analysis indicated that 59 (61\%; 95\% confidence interval 50\% to 71\%) would prefer anticoagulation treatment{\textemdash}considerably fewer than those who would be recommended treatment according to guidelines. There was marked disagreement between the decision analysis and guideline recommendations (kappa =0.25 or less). Of 38 patients whose decision analysis indicated a preference for anticoagulation, 17 (45\%) were being prescribed warfarin; on the other hand, 28 (47\%) of 59 patients were not being prescribed warfarin although the results of their decision analysis suggested they wanted to be.Conclusions: In the context of shared decision making, individualised decision analysis is valuable in a sizeable proportion of elderly patients with atrial fibrillation. Taking account of patients{\textquoteright} preferences would lead to fewer prescriptions for warfarin than under published guideline recommendations. Decision analysis as a shared decision making tool should be evaluated in a randomised controlled trial.Commentary: patients, preferences, and evidence},
  publisher = {BMJ Publishing Group Ltd},
}

@Article{Anscombe1963,
  author    = {F. J. Anscombe},
  title     = {Sequential Medical Trials},
  journal   = {Journal of the American Statistical Association},
  year      = {1963},
  volume    = {58},
  number    = {302},
  pages     = {365-383},
  issn      = {01621459},
  url       = {http://www.jstor.org/stable/2283272},
  abstract  = {In an extended review of Sequential Medical Trials by P. Armitage, the statistical principles which should govern the analysis of experimental observations, and the planning of experiments, are discussed. It is suggested that the operating-characteristic concepts of the Neyman-Pearson theory of tests are inappropriate to the analysis and interpretation of experimental data; the likelihood principle should be followed instead. The planning of medical trials under an ethical injunction against unnecessary continuance of inferior treatments is studied in some detail. The propriety of such trials is considered.},
  publisher = {[American Statistical Association, Taylor \& Francis, Ltd.]},
}

@Article{Breukelen2018,
  author   = {Breukelen, Gerard J.P. and Candel, Math J.J.M.},
  title    = {Efficient design of cluster randomized trials with treatment-dependent costs and treatment-dependent unknown variances},
  journal  = {Statistics in Medicine},
  year     = {2018},
  volume   = {37},
  number   = {21},
  pages    = {3027-3046},
  doi      = {10.1002/sim.7824},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.7824},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.7824},
  abstract = {Cluster randomized trials evaluate the effect of a treatment on persons nested within clusters, where treatment is randomly assigned to clusters. Current equations for the optimal sample size at the cluster and person level assume that the outcome variances and/or the study costs are known and homogeneous between treatment arms. This paper presents efficient yet robust designs for cluster randomized trials with treatment-dependent costs and treatment-dependent unknown variances, and compares these with 2 practical designs. First, the maximin design (MMD) is derived, which maximizes the minimum efficiency (minimizes the maximum sampling variance) of the treatment effect estimator over a range of treatment-to-control variance ratios. The MMD is then compared with the optimal design for homogeneous variances and costs (balanced design), and with that for homogeneous variances and treatment-dependent costs (cost-considered design). The results show that the balanced design is the MMD if the treatment-to control cost ratio is the same at both design levels (cluster, person) and within the range for the treatment-to-control variance ratio. It still is highly efficient and better than the cost-considered design if the cost ratio is within the range for the squared variance ratio. Outside that range, the cost-considered design is better and highly efficient, but it is not the MMD. An example shows sample size calculation for the MMD, and the computer code (SPSS and R) is provided as supplementary material. The MMD is recommended for trial planning if the study costs are treatment-dependent and homogeneity of variances cannot be assumed.},
  keywords = {cluster randomized trial, heterogeneous variance, maximin design, optimal design, sample size, study costs},
}

@Article{Storer1992,
  author    = {Barry E. Storer},
  title     = {A Class of Phase II Designs with Three Possible Outcomes},
  journal   = {Biometrics},
  year      = {1992},
  volume    = {48},
  number    = {1},
  pages     = {55--60},
  issn      = {0006341X, 15410420},
  url       = {http://www.jstor.org/stable/2532738},
  abstract  = {In the Phase II design of Fleming (1982, Biometrics 38, 143-151), the possible outcomes are to reject H1:p ⩽ p1 or to reject H2: p ⩾ p2, where p1 < p2. The design is constrained by specifying that Pr(reject H1|p1) ⩽ α and Pr(reject H2|p2) ⩽ β. This can lead to some ambiguity as to the appropriate practical decision at the end of the trial, as the confidence region for p may include a substantial part of the interval between p1 and p2. We propose a class of designs wherein an allowable outcome is not to reject either H1 or H2. An additional constraint is placed on the design by specifying Pr(reject Hj|pm) ⩽ γ (j = 1, 2), where p1 < pm < p2. This type of design can provide a more realistic basis for decision making following the trial, although traditional values of α and/or β need to be viewed from a somewhat different perspective in order to maintain a reasonable sample size. Some optimal single-stage and sequential multistage designs are presented.},
  publisher = {[Wiley, International Biometric Society]},
}

@Article{Brown2012,
  author    = {Michael J. Brown and Christy Chuang-Stein and Simon Kirby},
  title     = {Designing Studies to Find Early Signals of Efficacy},
  journal   = {Journal of Biopharmaceutical Statistics},
  year      = {2012},
  volume    = {22},
  number    = {6},
  pages     = {1097-1108},
  note      = {PMID: 23075010},
  doi       = {10.1080/10543406.2011.570466},
  eprint    = {https://doi.org/10.1080/10543406.2011.570466},
  url       = { 
        https://doi.org/10.1080/10543406.2011.570466
    
},
  abstract  = { We introduce the idea of a design to detect signals of efficacy in early phase clinical trials. Such a design features three possible decisions: to kill the compound; to continue with staged development; or to continue with accelerated development of the compound. We describe how such studies improve the trade-off between the two errors of killing a compound with good efficacy and committing to a complete full development program for a compound that has no efficacy and describe how they can be designed. We argue that such studies could be used to screen compounds at the proof-of-concept state, reduce late Phase 2 attrition, and speed up the development of highly efficacious drugs. },
  publisher = {Taylor \& Francis},
}

@Article{Emerson1987,
  author   = {Emerson, John D. and Tritchler, David},
  title    = {The three-decision problem in medical decision making},
  journal  = {Statistics in Medicine},
  year     = {1987},
  volume   = {6},
  number   = {2},
  pages    = {101-112},
  doi      = {10.1002/sim.4780060202},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.4780060202},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.4780060202},
  abstract = {Abstract Medical researchers and policy makers face decisions that require a choice from among two or more alternatives. Whereas traditional hypothesis tests cannot always serve the needs of the practitioner who needs to make a decision, a problem formulation that assigns losses to various incorrect decisions offers several advantages. With three possible decisions this approach offers a precise representation of the pragmatic and explanatory views of decision making. It enables the investigator to incorporate in the problem specification his attitudes about the seriousness of various errors by guiding him, before he sees the data, to a choice of asymmetric tail probabilities. It also suggests a reformulation of the P-value that can accommodate some of the difficulties practitioners face.},
  keywords = {Decision theory, Loss functions, Three-decision problem, P-value, Pragmatic approach to treatment comparison},
}

@Article{Kirby2016,
  author   = {Kirby, Simon and Chuang-Stein, Christy},
  title    = {A comparison of five approaches to decision-making for a first clinical trial of efficacy},
  journal  = {Pharmaceutical Statistics},
  year     = {2016},
  volume   = {16},
  number   = {1},
  pages    = {37-44},
  doi      = {10.1002/pst.1775},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/pst.1775},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/pst.1775},
  abstract = {The first trial of clinical efficacy is an important step in the development of a compound. Such a trial gives the first indication of whether a compound is likely to have the efficacy needed to be successful. Good decisions dictate that good compounds have a large probability of being progressed and poor compounds have a large probability of being stopped. In this paper, we consider and contrast five approaches to decision-making that have been used. To illustrate the use of the five approaches, we conduct a comparison for two plausible scenarios with associated assumptions for sample sizing. The comparison shows some large differences in performance characteristics of the different procedures. Which decision-making procedures and associated performance characteristics are preferred will depend on the focus of interest and the decision maker's attitude to risk. Copyright © 2016 John Wiley \& Sons, Ltd.},
  keywords = {hypothesis testing, early signal of efficacy, LPDAT, target value, confidence intervals},
}

@Article{Edwards1997,
  author  = {S J L Edwards and R J Lilford and David Braunholtz and Jennifer Jackson},
  title   = {Why “underpowered” trials are not necessarily unethical},
  journal = {The Lancet},
  year    = {1997},
  volume  = {350},
  pages   = {804-07},
  doi     = {10.1016/S0140-6736(97)02290-3},
  url     = {https://doi.org/10.1016/S0140-6736(97)02290-3},
}

@Article{Claxton1999,
  author   = {Karl Claxton},
  title    = {The irrelevance of inference: a decision-making approach to the stochastic evaluation of health care technologies},
  journal  = {Journal of Health Economics},
  year     = {1999},
  volume   = {18},
  number   = {3},
  pages    = {341 - 364},
  issn     = {0167-6296},
  doi      = {https://doi.org/10.1016/S0167-6296(98)00039-3},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167629698000393},
  abstract = {The literature which considers the statistical properties of cost-effectiveness analysis has focused on estimating the sampling distribution of either an incremental cost-effectiveness ratio or incremental net benefit for classical inference. However, it is argued here that rules of inference are arbitrary and entirely irrelevant to the decisions which clinical and economic evaluations claim to inform. Decisions should be based only on the mean net benefits irrespective of whether differences are statistically significant or fall outside a Bayesian range of equivalence. Failure to make decisions in this way by accepting the arbitrary rules of inference will impose costs which can be measured in terms of resources or health benefits forgone. The distribution of net benefit is only relevant to deciding whether more information is required. A framework for decision making and establishing the value of additional information is presented which is consistent with the decision rules in CEA. This framework can distinguish the simultaneous but conceptually separate steps of deciding which alternatives should be chosen, given existing information, from the question of whether more information should be acquired. It also ensures that the type of information acquired is driven by the objectives of the health care system, is consistent with the budget constraint on service provision and that research is designed efficiently.},
  keywords = {Cost-effectiveness analysis, Statistics, Clinical trials},
}

@Article{Bowater2010,
  author   = {Bowater, Russell J. and Lilford, Richard J.},
  title    = {Clinical effectiveness in cardiovascular trials in relation to the importance to the patient of the end-points measured},
  journal  = {Journal of Evaluation in Clinical Practice},
  year     = {2010},
  volume   = {17},
  number   = {4},
  pages    = {547-553},
  doi      = {10.1111/j.1365-2753.2010.01492.x},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1365-2753.2010.01492.x},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1365-2753.2010.01492.x},
  abstract = {Abstract Background  In order to increase the statistical power of a trial design, it is common practice for trialists to place a strong emphasis on end-points other than mortality. To assess the validity of this practice, the aim of this study was to investigate the relationship between the effectiveness of cardiovascular interventions in clinical trials and the importance to the patient of the end-point used to measure this effectiveness. Methods  A reanalysis of a database of cardiovascular trials used in an earlier study. This database consists of all randomized controlled trials published in six leading journals for cardiovascular research between January 2002 and June 2003 that involve the exposure of humans to any therapeutic or primary preventive intervention for cardiovascular disease and report at least one composite end-point. Results  On the basis of analysing the study data using a statistical modelling approach, the best estimate of the percentage of trials that have a true (not estimated) relative risk of less than 0.8 for the end-point of moderate outcome, major outcome and death are 63\%, 39\% and 0\% respectively, and the upper 95\% confidence limits on these three percentages are 73\%, 43\% and 15\% respectively. Conclusions  The proportion of interventions tested in the cardiovascular trials considered in this study that have a substantial beneficial effect relative to the control decreases as the end-point used to measure this effect becomes more important to the patient, that is, as the end-point severity increases from minor outcome to major outcome to death. Also, the analysis presented in this study strongly suggests that most of these interventions are likely to have close to no genuine effect on mortality relative to the control. Overall, this study substantially strengthens the empirical basis for the belief that the actions of trialists in choosing to study and heavily emphasize end-points other than mortality in cardiovascular trials and in particular end-points that are of only moderate or major importance to patients, may mislead practitioners and policy-makers about the overall effectiveness of the intervention concerned.},
  keywords = {cardiovascular disease, composite end-points, heterogeneity, meta-analysis, random effects, randomized controlled trials},
}

@Article{Altman1980,
  author    = {Douglas G. Altman},
  title     = {Statistics And Ethics In Medical Research: III How Large A Sample?},
  journal   = {The British Medical Journal},
  year      = {1980},
  volume    = {281},
  number    = {6251},
  pages     = {1336--1338},
  issn      = {00071447},
  url       = {http://www.jstor.org/stable/25442095},
  publisher = {BMJ},
}

@Article{Thall2004,
  author   = {Thall, Peter F. and Cook, John D.},
  title    = {Dose-Finding Based on Efficacy–Toxicity Trade-Offs},
  journal  = {Biometrics},
  year     = {2004},
  volume   = {60},
  number   = {3},
  pages    = {684-693},
  doi      = {10.1111/j.0006-341X.2004.00218.x},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.0006-341X.2004.00218.x},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.0006-341X.2004.00218.x},
  abstract = {Summary We present an adaptive Bayesian method for dose-finding in phase I/II clinical trials based on trade-offs between the probabilities of treatment efficacy and toxicity. The method accommodates either trinary or bivariate binary outcomes, as well as efficacy probabilities that possibly are nonmonotone in dose. Doses are selected for successive patient cohorts based on a set of efficacy–toxicity trade-off contours that partition the two-dimensional outcome probability domain. Priors are established by solving for hyperparameters that optimize the fit of the model to elicited mean outcome probabilities. For trinary outcomes, the new algorithm is compared to the method of Thall and Russell (1998, Biometrics54, 251–264) by application to a trial of rapid treatment for ischemic stroke. The bivariate binary outcome case is illustrated by a trial of graft-versus-host disease treatment in allogeneic bone marrow transplantation. Computer simulations show that, under a wide rage of dose-outcome scenarios, the new method has high probabilities of making correct decisions and treats most patients at doses with desirable efficacy–toxicity trade-offs.},
  keywords = {Adaptive design, Bayesian design, Biologic agents, Dose-finding, Phase I clinical trial, Phase II clinical trial},
}

@Article{Willan1994,
  author   = {Andrew R. Willan},
  title    = {Power function arguments in support of an alternative approach for analyzing management trials},
  journal  = {Controlled Clinical Trials},
  year     = {1994},
  volume   = {15},
  number   = {3},
  pages    = {211-219},
  doi      = {10.1016/0197-2456(94)90058-2},
  url      = {https://www.sciencedirect.com/science/article/pii/0197245694900582?via%3Dihub},
  abstract = {Power function arguments are used in support of an alternative approach for analyzing management trials. Central to the arguments is the recognition that, based on interpreting the smallest clinically important difference as a point of indifference, management trials require different power functions than explanatory trials. This interpretation of the smallest clinically important difference implies a specific ideal power function, and the alternative approach, by testing a nonzero null hypothesis at the appropriate level, is designed to achieve a power function with intuitive appeal and some optimal properties with respect to this ideal function. The alternative approach, by moving away from the traditional null hypothesis, better reflects clinical considerations, and eliminates the conflict between statistical and clinical significance. In addition, with adequate sample size, a decision in favor of the superior treatment will be made with a predetermined high level of confidence, if the therapies differ significantly.},
}

@Article{doi:10.1177/0962280215606155,
  author   = {Christian Holm Hansen and Pamela Warner and Richard A Parker and Brian R Walker and Hilary OD Critchley and Christopher J Weir},
  title    = {Development of a Bayesian response-adaptive trial design for the Dexamethasone for Excessive Menstruation study},
  journal  = {Statistical Methods in Medical Research},
  year     = {2017},
  volume   = {26},
  number   = {6},
  pages    = {2681-2699},
  doi      = {10.1177/0962280215606155},
  eprint   = {https://doi.org/10.1177/0962280215606155},
  url      = { 
        https://doi.org/10.1177/0962280215606155
    
},
  abstract = { It is often unclear what specific adaptive trial design features lead to an efficient design which is also feasible to implement. This article describes the preparatory simulation study for a Bayesian response-adaptive dose-finding trial design. Dexamethasone for Excessive Menstruation aims to assess the efficacy of Dexamethasone in reducing excessive menstrual bleeding and to determine the best dose for further study. To maximise learning about the dose response, patients receive placebo or an active dose with randomisation probabilities adapting based on evidence from patients already recruited. The dose-response relationship is estimated using a flexible Bayesian Normal Dynamic Linear Model. Several competing design options were considered including: number of doses, proportion assigned to placebo, adaptation criterion, and number and timing of adaptations. We performed a fractional factorial study using SAS software to simulate virtual trial data for candidate adaptive designs under a variety of scenarios and to invoke WinBUGS for Bayesian model estimation. We analysed the simulated trial results using Normal linear models to estimate the effects of each design feature on empirical type I error and statistical power. Our readily-implemented approach using widely available statistical software identified a final design which performed robustly across a range of potential trial scenarios. },
}

@Article{Christen2008,
  author   = {Christen, J. Andres and MÜLLER, Peter and Wathen, Kyle and Wolf, Judith},
  title    = {Bayesian randomized clinical trials: A decision-theoretic sequential design},
  journal  = {Canadian Journal of Statistics},
  year     = {2004},
  volume   = {32},
  number   = {4},
  pages    = {387-402},
  doi      = {10.2307/3316023},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.2307/3316023},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.2307/3316023},
  abstract = {Abstract The authors propose a Bayesian decision-theoretic framework justifying randomization in clinical trials. Noting that the decision maker is often unable or unwilling to specify a unique utility function, they develop a sequential myopic design that includes randomization justified by the consideration of a set of utility functions. Randomization is introduced over all nondominated treatments, allowing for interim removal of treatments and early stopping. The authors illustrate their approach in the context of a study to find the optimal dose of pegylated interferon for platinum resistant ovarian cancer. They also develop an algorithm to implement their methodology in a phase II clinical trial comparing several competing experimental treatments.},
  keywords = {Backward induction, clinical trials, robustness, utility functions},
}

@Misc{rstan,
  author = {{Stan Development Team}},
  title  = {{RStan}: the {R} interface to {Stan}},
  year   = {2016},
  note   = {R package version 2.14.1},
  url    = {http://mc-stan.org/},
}

@Article{Flight2016,
  author   = {Flight, Laura and Allison, Annabel and Dimairo, Munyaradzi and Lee, Ellen and Mandefield, Laura and Walters, Stephen J.},
  title    = {Recommendations for the analysis of individually randomised controlled trials with clustering in one arm -- a case of continuous outcomes},
  journal  = {BMC Medical Research Methodology},
  year     = {2016},
  volume   = {16},
  number   = {1},
  month    = {Nov},
  pages    = {165},
  issn     = {1471-2288},
  doi      = {10.1186/s12874-016-0249-5},
  url      = {https://doi.org/10.1186/s12874-016-0249-5},
  abstract = {In an individually randomised controlled trial where the treatment is delivered by a health professional it seems likely that the effectiveness of the treatment, independent of any treatment effect, could depend on the skill, training or even enthusiasm of the health professional delivering it. This may then lead to a potential clustering of the outcomes for patients treated by the same health professional, but similar clustering may not occur in the control arm. Using four case studies, we aim to provide practical guidance and recommendations for the analysis of trials with some element of clustering in one arm.},
  day      = {29},
}

@Article{Kahan2016,
  author   = {Kahan, Brennan C. and Forbes, Gordon and Ali, Yunus and Jairath, Vipul and Bremner, Stephen and Harhay, Michael O. and Hooper, Richard and Wright, Neil and Eldridge, Sandra M. and Leyrat, Cl{\'e}mence},
  title    = {Increased risk of type I errors in cluster randomised trials with small or medium numbers of clusters: a review, reanalysis, and simulation study},
  journal  = {Trials},
  year     = {2016},
  volume   = {17},
  number   = {1},
  month    = {Sep},
  pages    = {438},
  issn     = {1745-6215},
  doi      = {10.1186/s13063-016-1571-2},
  url      = {https://doi.org/10.1186/s13063-016-1571-2},
  abstract = {Cluster randomised trials (CRTs) are commonly analysed using mixed-effects models or generalised estimating equations (GEEs). However, these analyses do not always perform well with the small number of clusters typical of most CRTs. They can lead to increased risk of a type I error (finding a statistically significant treatment effect when it does not exist) if appropriate corrections are not used.},
  day      = {06},
}

@Article{Grayling2018,
  author   = {Grayling, Michael J. and Mander, Adrian P. and Wason, James M. S.},
  title    = {Blinded and unblinded sample size reestimation procedures for stepped-wedge cluster randomized trials},
  journal  = {Biometrical Journal},
  year     = {2018},
  volume   = {60},
  number   = {5},
  pages    = {903-916},
  doi      = {10.1002/bimj.201700125},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/bimj.201700125},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/bimj.201700125},
  abstract = {Abstract The ability to accurately estimate the sample size required by a stepped-wedge (SW) cluster randomized trial (CRT) routinely depends upon the specification of several nuisance parameters. If these parameters are misspecified, the trial could be overpowered, leading to increased cost, or underpowered, enhancing the likelihood of a false negative. We address this issue here for cross-sectional SW-CRTs, analyzed with a particular linear-mixed model, by proposing methods for blinded and unblinded sample size reestimation (SSRE). First, blinded estimators for the variance parameters of a SW-CRT analyzed using the Hussey and Hughes model are derived. Following this, procedures for blinded and unblinded SSRE after any time period in a SW-CRT are detailed. The performance of these procedures is then examined and contrasted using two example trial design scenarios. We find that if the two key variance parameters were underspecified by 50\%, the SSRE procedures were able to increase power over the conventional SW-CRT design by up to 41\%, resulting in an empirical power above the desired level. Thus, though there are practical issues to consider, the performance of the procedures means researchers should consider incorporating SSRE in to future SW-CRTs.},
  keywords = {blinded, cluster randomized trial, internal pilot, sample size re-estimation, stepped-wedge},
}

@Article{Pearce2018,
  author   = {Pearce, Michael and Hee, Siew Wan and Madan, Jason and Posch, Martin and Day, Simon and Miller, Frank and Zohar, Sarah and Stallard, Nigel},
  title    = {Value of information methods to design a clinical trial in a small population to optimise a health economic utility function},
  journal  = {BMC Medical Research Methodology},
  year     = {2018},
  volume   = {18},
  number   = {1},
  month    = {Feb},
  pages    = {20},
  issn     = {1471-2288},
  doi      = {10.1186/s12874-018-0475-0},
  url      = {https://doi.org/10.1186/s12874-018-0475-0},
  abstract = {Most confirmatory randomised controlled clinical trials (RCTs) are designed with specified power, usually 80{\%} or 90{\%}, for a hypothesis test conducted at a given significance level, usually 2.5{\%} for a one-sided test. Approval of the experimental treatment by regulatory agencies is then based on the result of such a significance test with other information to balance the risk of adverse events against the benefit of the treatment to future patients. In the setting of a rare disease, recruiting sufficient patients to achieve conventional error rates for clinically reasonable effect sizes may be infeasible, suggesting that the decision-making process should reflect the size of the target population.},
  day      = {08},
}

@Article{Stallard2003,
  author   = {Stallard, Nigel},
  title    = {Decision-Theoretic Designs for Phase II Clinical Trials Allowing for Competing Studies},
  journal  = {Biometrics},
  year     = {2003},
  volume   = {59},
  number   = {2},
  pages    = {402-409},
  doi      = {10.1111/1541-0420.00047},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/1541-0420.00047},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1541-0420.00047},
  abstract = {Summary This article describes an approach to optimal design of phase II clinical trials using Bayesian decision theory. The method proposed extends that suggested by Stallard (1998, Biometrics54, 279–294) in which designs were obtained to maximize a gain function including the cost of drug development and the benefit from a successful therapy. Here, the approach is extended by the consideration of other potential therapies, the development of which is competing for the same limited resources. The resulting optimal designs are shown to have frequentist properties much more similar to those traditionally used in phase II trials.},
  keywords = {Backward induction, Cost-benefit analysis, Optimal stopping, Sequential procedure},
}

@Book{Santner2003,
  author    = {Santner, Thomas J. and Williams, Brian J. and Notz, William I.},
  title     = {The Design and Analysis of Computer Experiments},
  year      = {2003},
  publisher = {Springer-Verlag New York, Inc.},
}

@Article{Krige1951,
  author    = {Krige, D.G.},
  title     = {A statistical approach to some basic mine valuation problems on the Witwatersrand},
  journal   = {Journal of the Southern African Institute of Mining and Metallurgy},
  year      = {1951},
  language  = {English},
  volume    = {52},
  number    = {6},
  pages     = {119-139},
  issn      = {0038-223X},
  url       = {https://journals.co.za/content/saimm/52/6/AJA0038223X_4792},
  abstract  = {Certain fundamental concepts in the application of statistics to mine valuation on the Witwatersrand are discussed, and general conclusions are drawn regarding the application of the lognormal curve to the frequency distribution of gold values. An indication is given of the reliability of present valuation methods on the Rand. It is shown that the existing over- and under-valuation of blocks of ore listed as high-grade and low-grade, respectively, can be explained statistically. Suggestions are made for the elimination of such errors and for the improvement of the general standard of mine valuation by the use of statistical theory.},
  publisher = {Southern African Institute of Mining and Metallurgy},
  type      = {Journal Article},
}

@Article{Lancaster2015,
  author   = {Lancaster, Gillian A.},
  title    = {Pilot and feasibility studies come of age!},
  journal  = {Pilot and Feasibility Studies},
  year     = {2015},
  volume   = {1},
  number   = {1},
  month    = {Jan},
  pages    = {1},
  issn     = {2055-5784},
  doi      = {10.1186/2055-5784-1-1},
  url      = {https://doi.org/10.1186/2055-5784-1-1},
  abstract = {This editorial introduces the new, online, open-access journal Pilot and Feasibility Studies. The journal considers manuscripts on any aspect of the design and analysis of pilot and feasibility studies, as well as protocols for pilot and feasibility studies, and discussions and reviews of methodological issues around the planning and reporting of such studies. These studies are generally carried out in preparation for future large-scale definitive randomised controlled trials or observational studies and address key issues of uncertainty. Objectives for conducting pilot and feasibility studies therefore differ from those of the future large-scale study and should be clearly expressed. The journal provides a dedicated place for publication of this important work as well as a forum for discussion of methodological issues that will lead to increased scientific rigour in this area.},
  day      = {12},
}

@Manual{Dutang2015,
  author = {Dutang, Christophe and Savicky, Petr},
  title  = {randtoolbox: Generating and Testing Random Numbers},
  year   = {2015},
  note   = {R package version 1.17},
}

@Article{DeMartini2010,
  author    = {De Martini, Daniele},
  title     = {Conservative Sample Size Estimation in Nonparametrics},
  journal   = {Journal of Biopharmaceutical Statistics},
  year      = {2010},
  volume    = {21},
  number    = {1},
  pages     = {24-41},
  doi       = {10.1080/10543400903453343},
  eprint    = {https://doi.org/10.1080/10543400903453343},
  url       = { 
        https://doi.org/10.1080/10543400903453343
    
},
  abstract  = { Due to the uncertainty of the results of phase II trials, underpowered phase III trials are often planned. In recent literature the conservative approach for sample size estimation was proposed. Some authors, in the parametric framework, make use of the lower bound of the effect size for conservatively estimating the true power, and so the sample sizes. Here, we present a general bootstrap method for conservatively estimating, on the basis of phase II data, the sample size needed for a phase III trial. The method we propose is based on the use of nonparametric lower bounds for the true power of the test. A wide study is shown for comparing the performances of the new method in estimating the power of the Wilcoxon rank-sum test with those given by standard techniques based on the asymptotic normality of the test statistic. Results indicate that when the phase II sample size is around the ideal sample size for the phase III, the bootstrap provides better results than the other techniques. Since the method is general, it could be used for planning clinical trials for testing superiority, for testing noninferiority, and for more complicated situations, e.g., for testing multiple endpoints. },
  publisher = {Taylor \& Francis},
}

@Article{DeMartini2010a,
  author   = {De Martini, Daniele},
  title    = {Adapting by calibration the sample size of a phase III trial on the basis of phase II data},
  journal  = {Pharmaceutical Statistics},
  year     = {2010},
  volume   = {10},
  number   = {2},
  pages    = {89-95},
  doi      = {10.1002/pst.410},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/pst.410},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/pst.410},
  abstract = {Abstract The problem of estimating the sample size for a phase III trial on the basis of existing phase II data is considered, where data from phase II cannot be combined with those of the new phase III trial. Focus is on the test for comparing the means of two independent samples. A launching criterion is adopted in order to evaluate the relevance of phase II results: phase III is run if the effect size estimate is higher than a threshold of clinical importance. The variability in sample size estimation is taken into consideration. Then, the frequentist conservative strategies with a fixed amount of conservativeness and Bayesian strategies are compared. A new conservative strategy is introduced and is based on the calibration of the optimal amount of conservativeness – calibrated optimal strategy (COS). To evaluate the results we compute the Overall Power (OP) of the different strategies, as well as the mean and the MSE of sample size estimators. Bayesian strategies have poor characteristics since they show a very high mean and/or MSE of sample size estimators. COS clearly performs better than the other conservative strategies. Indeed, the OP of COS is, on average, the closest to the desired level; it is also the highest. COS sample size is also the closest to the ideal phase III sample size MI, showing averages and MSEs lower than those of the other strategies. Costs and experimental times are therefore considerably reduced and standardized. However, if the ideal sample size MI is to be estimated the phase II sample size n should be around the ideal phase III sample size, i.e. n⩾2MI/3. Copyright © 2010 John Wiley \& Sons, Ltd.},
  keywords = {calibration, conservativeness, launch threshold, optimal conservative parameter, overall power, sample size estimation, variability},
}

@Article{Roberts2015,
  author   = {Roberts, Chris and Batistatou, Evridiki and Roberts, Stephen A.},
  title    = {Design and analysis of trials with a partially nested design and a binary outcome measure},
  journal  = {Statistics in Medicine},
  year     = {2015},
  volume   = {35},
  number   = {10},
  pages    = {1616-1636},
  doi      = {10.1002/sim.6828},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.6828},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.6828},
  abstract = {Where treatments are administered to groups of patients or delivered by therapists, outcomes for patients in the same group or treated by the same therapist may be more similar, leading to clustering. Trials of such treatments should take account of this effect. Where such a treatment is compared with an un-clustered treatment, the trial has a partially nested design. This paper compares statistical methods for this design where the outcome is binary. Investigation of consistency reveals that a random coefficient model with a random effect for group or therapist is not consistent with other methods for a null treatment effect, and so this model is not recommended for this design. Small sample performance of a cluster-adjusted test of proportions, a summary measures test and logistic generalised estimating equations and random intercept models are investigated through simulation. The expected treatment effect is biased for the logistic models. Empirical test size of two-sided tests is raised only slightly, but there are substantial biases for one-sided tests. Three formulae are proposed for calculating sample size and power based on (i) the difference of proportions, (ii) the log-odds ratio or (iii) the arc-sine transformation of proportions. Calculated power from these formulae is compared with empirical power from a simulations study. Logistic models appeared to perform better than those based on proportions with the likelihood ratio test performing best in the range of scenarios considered. For these analyses, the log-odds ratio method of calculation of power gave an approximate lower limit for empirical power. © 2015 The Authors. Statistics in Medicine published by John Wiley \& Sons Ltd.},
  keywords = {partially nested trials, clustering, binary outcomes, sample size, power},
}

@Article{Soares2018,
  author   = {Marta O. Soares and Linda Sharples and Alec Morton and Karl Claxton and Laura Bojke},
  title    = {Experiences of Structured Elicitation for Model-Based Cost-Effectiveness Analyses},
  journal  = {Value in Health},
  year     = {2018},
  volume   = {21},
  number   = {6},
  pages    = {715 - 723},
  issn     = {1098-3015},
  doi      = {https://doi.org/10.1016/j.jval.2018.01.019},
  url      = {http://www.sciencedirect.com/science/article/pii/S1098301518302274},
  abstract = {Background
Empirical evidence supporting the cost-effectiveness estimates of particular health care technologies may be limited, or it may even be missing entirely. In these situations, additional information, often in the form of expert judgments, is needed to reach a decision. There are formal methods to quantify experts’ beliefs, termed as structured expert elicitation (SEE), but only limited research is available in support of methodological choices. Perhaps as a consequence, the use of SEE in the context of cost-effectiveness modelling is limited.
Objectives
This article reviews applications of SEE in cost-effectiveness modelling with the aim of summarizing the basis for methodological choices made in each application and recording the difficulties and challenges reported by the authors in the design, conduct, and analyses.
Methods
The methods used in each application were extracted along with the criteria used to support methodological and practical choices and any issues or challenges discussed in the text. Issues and challenges were extracted using an open field, and then categorised and grouped for reporting.
Results
The review demonstrates considerable heterogeneity in methods used, and authors acknowledge great methodological uncertainty in justifying their choices. Specificities of the context area emerging as potentially important in determining further methodological research in elicitation are between- expert variation and its interpretation, the fact that substantive experts in the area may not be trained in quantitative subjects, that judgments are often needed on various parameter types, the need for some form of assessment of validity, and the need for more integration with behavioural research to devise relevant debiasing strategies.
Conclusions
This review of experiences of SEE highlights a number of specificities/constraints that can shape the development of guidance and target future research efforts in this area.},
  keywords = {Bayesian, cost effectiveness, decision modeling, elicitation, expert judgment, subjective},
}

@Article{doi:10.1177/1740774518796156,
  author   = {Anna Kearney and Nicola L Harman and Anna Rosala-Hallas and Claire Beecher and Jane M Blazeby and Peter Bower and Mike Clarke and William Cragg and Sinead Duane and Heidi Gardner and Patricia Healy and Lisa Maguire and Nicola Mills and Leila Rooshenas and Ceri Rowlands and Shaun Treweek and Akke Vellinga and Paula R Williamson and Carrol Gamble},
  title    = {Development of an online resource for recruitment research in clinical trials to organise and map current literature},
  journal  = {Clinical Trials},
  year     = {2018},
  volume   = {15},
  number   = {6},
  pages    = {533-542},
  note     = {PMID: 30165760},
  doi      = {10.1177/1740774518796156},
  eprint   = {https://doi.org/10.1177/1740774518796156},
  url      = { 
        https://doi.org/10.1177/1740774518796156
    
},
  abstract = { BackgroundRecruiting the target number of participants within the pre-specified time frame agreed with funders remains a common challenge in the completion of a successful clinical trial and addressing this is an important methodological priority. While there is growing research around recruitment, navigating this literature to support an evidence-based approach remains difficult. The Online resource for Recruitment Research in Clinical triAls project aims to create an online searchable database of recruitment research to improve access to existing evidence and to identify gaps for future research.MethodsMEDLINE (Ovid), Scopus, Cochrane Database of Systematic Reviews and Cochrane Methodology Register, Science Citation Index Expanded and Social Sciences Citation Index within the ISI Web of Science and Education Resources Information Center were searched in January 2015. Search strategy results were screened by title and abstract, and full text obtained for potentially eligible articles. Studies reporting or evaluating strategies, interventions or methods used to recruit patients were included along with case reports and studies exploring reasons for patient participation or non-participation. Eligible articles were categorised as systematic reviews, nested randomised controlled trials and other designs evaluating the effects of recruitment strategies (Level 1); studies that report the use of recruitment strategies without an evaluation of impact (Level 2); or articles reporting factors affecting recruitment without presenting a particular recruitment strategy (Level 3). Articles were also assigned to 1, or more, of 42 predefined recruitment domains grouped under 6 categories.ResultsMore than 60,000 records were retrieved by the search, resulting in 56,030 unique titles and abstracts for screening, with a further 23 found through hand searches. A total of 4570 full text articles were checked; 2804 were eligible. Six percent of the included articles evaluated the effectiveness of a recruitment strategy (Level 1), with most of these assessing aspects of participant information, either its method of delivery (33\%) or its content and format (28\%).DiscussionRecruitment to clinical trials remains a common challenge and an important area for future research. The online resource for Recruitment Research in Clinical triAls project provides a searchable, online database of research relevant to recruitment. The project has identified the need for researchers to evaluate their recruitment strategies to improve the evidence base and broaden the narrow focus of existing research to help meet the complex challenges faced by those recruiting to clinical trials. },
  groups   = {Seminar?},
}

@Article{Fay2013,
  author  = {Michael P. Fay},
  title   = {An alternative property for evaluating sample size for normal data using preliminary data},
  journal = {Clinical Trials},
  year    = {2013},
  volume  = {10},
  issue   = {6},
  pages   = {990-1},
  doi     = {10.1177/1740774513506965},
}

@Article{Chalder1993,
  author  = {Chalder, T and Berelowitz, G and Hirsch, S and Pawlikowska, T and Wallace, P and Wessely, S},
  title   = {Development of a fatigue scale},
  journal = {Journal of Psychometric Research},
  year    = {1993},
  volume  = {37},
  number  = {2},
  pages   = {147-153},
}

@Article{McHorney1993,
  author    = {Colleen A. McHorney and John E. Ware and Anastasia E. Raczek},
  title     = {The MOS 36-Item Short-Form Health Survey (SF-36): II. Psychometric and Clinical Tests of Validity in Measuring Physical and Mental Health Constructs},
  journal   = {Medical Care},
  year      = {1993},
  volume    = {31},
  number    = {3},
  pages     = {247--263},
  issn      = {00257079},
  url       = {http://www.jstor.org/stable/3765819},
  abstract  = {Cross-sectional data from the Medical Outcomes Study (MOS) were analyzed to test the validity of the MOS 36-Item Short-Form Health Survey (SF-36) scales as measures of physical and mental health constructs. Results from traditional psychometric and clinical tests of validity were compared. Principal components analysis was used to test for hypothesized physical and mental health dimensions. For purposes of clinical tests of validity, clinical criteria defined mutually exclusive adult patient groups differing in severity of medical and psychiatric conditions. Scales shown in the components analysis to primarily measure physical health (physical functioning and role limitations-physical) best distinguished groups differing in severity of chronic medical condition and had the most pure physical health interpretation. Scales shown to primarily measure mental health (mental health and role limitations-emotional) best distinguished groups differing in the presence and severity of psychiatric disorders and had the most pure mental health interpretation. The social functioning, vitality, and general health perceptions scales measured both physical and mental health components and, thus, had the most complex interpretation. These results are useful in establishing guidelines for the interpretation of each scale and in documenting the size of differences between clinical groups that should be considered very large.},
  publisher = {Lippincott Williams \& Wilkins},
}

@Article{Cheng2003,
  author  = {Cheng, Yi and Su, Fusheng and Berry, Donald A.},
  title   = {Choosing sample size for a clinical trial using decision analysis},
  journal = {Biometrika},
  year    = {2003},
  volume  = {90},
  number  = {4},
  pages   = {923-936},
  doi     = {10.1093/biomet/90.4.923},
  eprint  = {/oup/backfile/content_public/journal/biomet/90/4/10.1093/biomet/90.4.923/2/900923.pdf},
  url     = {http://dx.doi.org/10.1093/biomet/90.4.923},
}

@Article{Sully2013,
  author   = {Sully, Ben G. O. and Julious, Steven A. and Nicholl, Jon},
  title    = {A reinvestigation of recruitment to randomised, controlled, multicenter trials: a review of trials funded by two UK funding agencies},
  journal  = {Trials},
  year     = {2013},
  volume   = {14},
  number   = {1},
  month    = {Jun},
  pages    = {166},
  issn     = {1745-6215},
  doi      = {10.1186/1745-6215-14-166},
  url      = {https://doi.org/10.1186/1745-6215-14-166},
  abstract = {Randomised controlled trials (RCTs) are the gold standard assessment for health technologies. A key aspect of the design of any clinical trial is the target sample size. However, many publicly-funded trials fail to reach their target sample size. This study seeks to assess the current state of recruitment success and grant extensions in trials funded by the Health Technology Assessment (HTA) program and the UK Medical Research Council (MRC).},
  day      = {09},
}

@Article{Corrigan2003,
  author   = {Patrick W. Corrigan and Mark S. Salzer},
  title    = {The conflict between random assignment and treatment preference: implications for internal validity},
  journal  = {Evaluation and Program Planning},
  year     = {2003},
  volume   = {26},
  number   = {2},
  pages    = {109 - 121},
  note     = {Evaluating Educational Technology},
  issn     = {0149-7189},
  doi      = {https://doi.org/10.1016/S0149-7189(03)00014-4},
  url      = {http://www.sciencedirect.com/science/article/pii/S0149718903000144},
  abstract = {The gold standard for most clinical and services outcome studies is random assignment to treatment condition because this kind of design diminishes many threats to internal validity. Although we agree with the power of randomized clinical trials, we argue in this paper that random assignment raises other, unanticipated threats to internal validity as a result of failing to consider treatment preference in research participant behavior. Treatment preference arises from an individual's knowledge and appraisal of treatment options. Treatment preferences impact: (1) the recruitment phase because people consider whether they want to participate in a study that involves the possibility of receiving an undesirable treatment or waiting for treatment, (2) degree of engagement in the intervention condition, and (3) attrition from the study. The benefits and limitations of research strategies that augment randomization while respecting treatment preference are reviewed including: approaches that enhance enrollment and engagement; pilot testing assumptions about randomization; and partially randomized clinical trials.},
  keywords = {Random assignment, Treatment preference, Internal validity},
}

@Article{Terris-Prestholt2019,
  author   = {Terris-Prestholt, Fern and Neke, Nyasule and Grund, Jonathan M. and Plotkin, Marya and Kuringe, Evodius and Osaki, Haika and Ong, Jason J. and Tucker, Joseph D. and Mshana, Gerry and Mahler, Hally and Weiss, Helen A. and Wambura, Mwita and {The VMMC study team}},
  title    = {Using discrete choice experiments to inform the design of complex interventions},
  journal  = {Trials},
  year     = {2019},
  volume   = {20},
  number   = {1},
  month    = {Mar},
  pages    = {157},
  issn     = {1745-6215},
  doi      = {10.1186/s13063-019-3186-x},
  url      = {https://doi.org/10.1186/s13063-019-3186-x},
  abstract = {Complex health interventions must incorporate user preferences to maximize their potential effectiveness. Discrete choice experiments (DCEs) quantify the strength of user preferences and identify preference heterogeneity across users. We present the process of using a DCE to supplement conventional qualitative formative research in the design of a demand creation intervention for voluntary medical male circumcision (VMMC) to prevent HIV in Tanzania.},
  day      = {04},
  groups   = {Fellowship app},
}

@Article{Sculpher382,
  author    = {Sculpher, Mark and Bryan, Stirling and Fry, Pat and de Winter, Patricia and Payne, Heather and Emberton, Mark},
  title     = {Patients{\textquoteright} preferences for the management of non-metastatic prostate cancer: discrete choice experiment},
  journal   = {BMJ},
  year      = {2004},
  volume    = {328},
  number    = {7436},
  pages     = {382},
  issn      = {0959-8138},
  doi       = {10.1136/bmj.37972.497234.44},
  eprint    = {https://www.bmj.com/content/328/7436/382.full.pdf},
  url       = {https://www.bmj.com/content/328/7436/382},
  abstract  = {Objective To establish which attributes of conservative treatments for prostate cancer are most important to men. Design Discrete choice experiment. Setting Two London hospitals. Participants 129 men with non-metastatic prostate cancer, mean age 70 years; 69 of 118 (58\%) with T stage 1 or 2 cancer at diagnosis. Main outcome measures Men{\textquoteright}s preferences for, and trade-offs between, the attributes of diarrhoea, hot flushes, ability to maintain an erection, breast swelling or tenderness, physical energy, sex drive, life expectancy, and out of pocket expenses. Results The men{\textquoteright}s responses to changes in attributes were all statistically significant. When asked to assume a starting life expectancy of five years, the men were willing to make trade-offs between life expectancy and side effects. On average, they were most willing to give up life expectancy to avoid limitations in physical energy (mean three months) and least willing to trade life expectancy to avoid hot flushes (mean 0.6 months to move from a moderate to mild level or from mild to none). Conclusions Men with prostate cancer are willing to participate in a relatively complex exercise that weighs up the advantages and disadvantages of various conservative treatments for their condition. They were willing to trade off some life expectancy to be relieved of the burden of troublesome side effects such as limitations in physical energy.},
  groups    = {Fellowship app},
  publisher = {BMJ Publishing Group Ltd},
}

@Article{Langberg2019,
  author   = {Emil Mørup Langberg and Lise Dyhr and Annette Sofie Davidsen},
  title    = {Development of the concept of patient-centredness – A systematic review},
  journal  = {Patient Education and Counseling},
  year     = {2019},
  issn     = {0738-3991},
  doi      = {https://doi.org/10.1016/j.pec.2019.02.023},
  url      = {http://www.sciencedirect.com/science/article/pii/S073839911830733X},
  abstract = {Objective
Patient-centredness is often linked to high-quality patient care, but the concept is not well-defined. This study aims to provide an overview of how patient-centredness has been defined in the literature since Mead and Bower’s review in 2000, and to provide an updated definition of the concept.
Method & design
We performed a systematic literature search in PubMed to identify original articles with a sufficient definition of patient-centredness. We analysed extracted data defining patient-centredness.
Results
Eighty articles were included. The dimensions “biopsychosocial”, “patient-as-person”, “sharing power and responsibility” and “therapeutic alliance” corresponded to four of five dimensions described by Mead and Bower. “Coordinated care” was a new dimension.
Conclusion
The identified dimensions are encompassed by three elements: the patient, the doctor-patient relationship and the framework of care i.e. the health care system. The additional focus on coordinated care could reflect increasing complexity of the health care system.
Practice implications
Narrowing down the understanding of patient-centredness to these three focus areas, viz. 1) understanding of the patients’ experience of the illness in their life situation, 2) the professional’s relationship with the patient, and 3) coordination of care in the system, could make the operationalisation and implementation of a patient-centred approach more manageable.},
  groups   = {Fellowship app},
  keywords = {Patient-centredness, Biopsychosocial, Shared decision-making, Therapeutic alliance, Coordinated care, Systematic review},
}

@Article{Tan2019,
  author       = {Tan, Woan Shin and Bajpai, Ram and Ho, Andy Hau Yan and Low, Chan Kee and Car, Josip},
  title        = {Retrospective cohort analysis of real-life decisions about end-of-life care preferences in a Southeast Asian country},
  journal      = {BMJ Open},
  year         = {2019},
  volume       = {9},
  number       = {2},
  issn         = {2044-6055},
  doi          = {10.1136/bmjopen-2018-024662},
  eprint       = {https://bmjopen.bmj.com/content/9/2/e024662.full.pdf},
  url          = {https://bmjopen.bmj.com/content/9/2/e024662},
  abstract     = {Objective To describe the end-of-life care preferences of individuals, and to examine the influence of age and gender on these preferences.Design, setting and participants A retrospective cohort study was conducted. Participants included all adults (>=21 years old) (n=3380) who had completed a statement of their preferences as part of a national Advance Care Planning (ACP) programme in Singapore. Data were extracted from the national and Tan Tock Seng Hospital ACP database.Main measures End-of-life care preferences were obtained from the ACP document and differentiated by health status (healthy, chronically ill or diagnosed with advanced illnesses). To analyse the data, descriptive statistics and logistic regression analysis were used.Results Across healthy and chronically ill patients, the majority did not opt for cardiopulmonary resuscitation (CPR) or other life-sustaining measures. Among individuals with advanced illnesses, 94\% preferred not to attempt CPR but 69\% still preferred to receive some form of active medical treatment. Approximately 40\% chose to be cared for, and to die at home. Age and sex significantly predict preferences in those with advanced illnesses. Older age (\&gt;=75 years) showed higher odds for home as preferred place of care (OR 1.52; 95\% CI 1.23 to 1.89) and place of death (OR 1.29; 95\% CI 1.03 to 1.61) and lower odds for CPR (OR 0.31; 95\% CI 0.18 to 0.54) and full treatment (OR 0.32; 95\% CI 0.17 to 0.62). Being female was associated with lower odds for home as preferred place of care (OR 0.69; 95\% CI 0.57 to 0.84) and place of death (OR 0.70; 95\% CI 0.57 to 0.85) and higher odds for full treatment (OR 2.35; 95\% CI 1.18 to 4.68).Conclusion The majority preferred to not proceed with life-sustaining treatments, but there was still a strong preference to receive some form of limited treatment. Better understanding of end-of-life care preferences through ACP can better guide end-of-life care programme planning, and resource allocation decisions.},
  elocation-id = {e024662},
  groups       = {Fellowship app},
  publisher    = {British Medical Journal Publishing Group},
}

@Article{Sackett1999,
  author  = {Sackett, D. L. and Straus, S. E.},
  title   = {{Applying evidence to the individual patient}},
  journal = {Annals of Oncology},
  year    = {1999},
  volume  = {10},
  number  = {1},
  month   = {01},
  pages   = {29-32},
  issn    = {0923-7534},
  doi     = {10.1023/A:1008308211595},
  eprint  = {http://oup.prod.sis.lan/annonc/article-pdf/10/1/29/19477069/10-1-29.pdf},
  url     = {https://dx.doi.org/10.1023/A:1008308211595},
  groups  = {Fellowship app},
}

@Article{Bonnettl737,
  author       = {Bonnett, Laura J and Snell, Kym I E and Collins, Gary S and Riley, Richard D},
  title        = {Guide to presenting clinical prediction models for use in clinical settings},
  journal      = {BMJ},
  year         = {2019},
  volume       = {365},
  issn         = {0959-8138},
  doi          = {10.1136/bmj.l737},
  eprint       = {https://www.bmj.com/content/365/bmj.l737.full.pdf},
  url          = {https://www.bmj.com/content/365/bmj.l737},
  elocation-id = {l737},
  groups       = {Fellowship app},
  publisher    = {BMJ Publishing Group Ltd},
}

@Article{Senn1993,
  author    = {Stephen Senn},
  title     = {Inherent difficulties with active control equivalence studies},
  journal   = {Statistics in Medicine},
  year      = {1993},
  volume    = {12},
  number    = {24},
  month     = {dec},
  pages     = {2367--2375},
  doi       = {10.1002/sim.4780122412},
  groups    = {Fellowship app},
  publisher = {Wiley},
}

@Article{Halpern2002,
  author    = {Scott D. Halpern},
  title     = {The Continuing Unethical Conduct of Underpowered Clinical Trials},
  journal   = {{JAMA}},
  year      = {2002},
  volume    = {288},
  number    = {3},
  month     = {jul},
  pages     = {358},
  doi       = {10.1001/jama.288.3.358},
  publisher = {American Medical Association ({AMA})},
}

@Article{Lilford2002,
  author   = {Lilford, Richard J.},
  title    = {The Ethics of Underpowered Clinical Trials},
  journal  = {JAMA},
  year     = {2002},
  volume   = {288},
  number   = {17},
  month    = {11},
  pages    = {2118-2119},
  issn     = {0098-7484},
  doi      = {10.1001/jama.288.17.2118},
  eprint   = {https://jamanetwork.com/journals/jama/articlepdf/1032093/jlt1106.pdf},
  url      = {https://doi.org/10.1001/jama.288.17.2118},
  abstract = {{To the Editor: I have a number of concerns
about the article by Mr Halpern and colleagues. First,
much of their argument is based on "exposure of participants to the risks
and burdens of human research." On the contrary, I believe that RCTs should
be carried out when there is equipoise; ie, when the treatments being compared
are equally likely to be beneficial. Randomized trials under this condition
(at least when the comparator treatments are otherwise widely available) simply
extend the patient's choice. The premise
that patients somehow make a sacrifice for the common good when they participate
in a trial is therefore wrong.}},
}

@Article{Lee2019,
  author    = {Kim May Lee and James Wason and Nigel Stallard},
  title     = {To add or not to add a new treatment arm to a multiarm study: A decision-theoretic framework},
  journal   = {Statistics in Medicine},
  year      = {2019},
  month     = {may},
  doi       = {10.1002/sim.8194},
  publisher = {Wiley},
}

@Article{Magirr2012,
  author    = {D. Magirr and T. Jaki and J. Whitehead},
  title     = {A generalized Dunnett test for multi-arm multi-stage clinical studies with treatment selection},
  journal   = {Biometrika},
  year      = {2012},
  volume    = {99},
  number    = {2},
  month     = {mar},
  pages     = {494--501},
  doi       = {10.1093/biomet/ass002},
  publisher = {Oxford University Press ({OUP})},
}

@Article{Bacharach1975,
  author    = {Michael Bacharach},
  title     = {Group Decisions in the Face of Differences of Opinion},
  journal   = {Management Science},
  year      = {1975},
  volume    = {22},
  number    = {2},
  month     = {oct},
  pages     = {182--191},
  doi       = {10.1287/mnsc.22.2.182},
  groups    = {Fellowship app},
  publisher = {Institute for Operations Research and the Management Sciences ({INFORMS})},
}

@Article{Fairhurst2019,
  author    = {K. Fairhurst and J. M. Blazeby and S. Potter and C. Gamble and C. Rowlands and K. N. L. Avery},
  title     = {Value of surgical pilot and feasibility study protocols},
  journal   = {{BJS}},
  year      = {2019},
  month     = {may},
  doi       = {10.1002/bjs.11167},
  publisher = {Wiley},
}

@Article{Ren2018,
  author    = {Shijie Ren and Jeremy E. Oakley and John W. Stevens},
  title     = {Incorporating Genuine Prior Information about Between-Study Heterogeneity in Random Effects Pairwise and Network Meta-analyses},
  journal   = {Medical Decision Making},
  year      = {2018},
  volume    = {38},
  number    = {4},
  month     = {mar},
  pages     = {531--542},
  doi       = {10.1177/0272989x18759488},
  publisher = {{SAGE} Publications},
}

@Article{Blackstone1986,
  author    = {Eugene H. Blackstone and David C. Naftel and Malcolm E. Turner},
  title     = {The Decomposition of Time-Varying Hazard into Phases, Each Incorporating a Separate Stream of Concomitant Information},
  journal   = {Journal of the American Statistical Association},
  year      = {1986},
  volume    = {81},
  number    = {395},
  month     = {sep},
  pages     = {615--624},
  doi       = {10.1080/01621459.1986.10478314},
  groups    = {Fellowship app},
  publisher = {Informa {UK} Limited},
}

@Article{Pajouheshnia2019,
  author    = {Romin Pajouheshnia and Rolf H H Groenwold and Linda M Peelen and Johannes B Reitsma and Karel G M Moons},
  title     = {When and how to use data from randomised trials to develop or validate prognostic models},
  journal   = {{BMJ}},
  year      = {2019},
  month     = {may},
  pages     = {l2154},
  doi       = {10.1136/bmj.l2154},
  groups    = {Fellowship app},
  publisher = {{BMJ}},
}

@Article{Relton2010,
  author    = {C. Relton and D. Torgerson and A. O{\textquotesingle}Cathain and J. Nicholl},
  title     = {Rethinking pragmatic randomised controlled trials: introducing the "cohort multiple randomised controlled trial" design},
  journal   = {{BMJ}},
  year      = {2010},
  volume    = {340},
  number    = {mar19 1},
  month     = {mar},
  pages     = {c1066--c1066},
  doi       = {10.1136/bmj.c1066},
  groups    = {Fellowship app},
  publisher = {{BMJ}},
}

@Article{Moons2009,
  author    = {K. G M Moons and P. Royston and Y. Vergouwe and D. E Grobbee and D. G Altman},
  title     = {Prognosis and prognostic research: what, why, and how?},
  journal   = {{BMJ}},
  year      = {2009},
  volume    = {338},
  number    = {feb23 1},
  month     = {feb},
  pages     = {b375--b375},
  doi       = {10.1136/bmj.b375},
  groups    = {Fellowship app},
  publisher = {{BMJ}},
}

@Article{Brewin1989,
  author    = {C. R. Brewin and C. Bradley},
  title     = {Patient preferences and randomised clinical trials.},
  journal   = {{BMJ}},
  year      = {1989},
  volume    = {299},
  number    = {6694},
  month     = {jul},
  pages     = {313--315},
  doi       = {10.1136/bmj.299.6694.313},
  groups    = {Fellowship app},
  publisher = {{BMJ}},
}

@Article{Barrett2005,
  author    = {Bruce Barrett and David Brown and Marlon Mundt and Roger Brown},
  title     = {Sufficiently Important Difference: Expanding the Framework of Clinical Significance},
  journal   = {Medical Decision Making},
  year      = {2005},
  volume    = {25},
  number    = {3},
  month     = {may},
  pages     = {250--261},
  doi       = {10.1177/0272989x05276863},
  groups    = {Fellowship app},
  publisher = {{SAGE} Publications},
}

@Article{Bower2005,
  author    = {Peter Bower and Michael King and Irwin Nazareth and Fiona Lampe and Bonnie Sibbald},
  title     = {Patient preferences in randomised controlled trials: Conceptual framework and implications for research},
  journal   = {Social Science {\&} Medicine},
  year      = {2005},
  volume    = {61},
  number    = {3},
  month     = {aug},
  pages     = {685--695},
  doi       = {10.1016/j.socscimed.2004.12.010},
  groups    = {Fellowship app},
  publisher = {Elsevier {BV}},
}

@Article{Long2008,
  author    = {Qi Long and Roderick J Little and Xihong Lin},
  title     = {Causal Inference in Hybrid Intervention Trials Involving Treatment Choice},
  journal   = {Journal of the American Statistical Association},
  year      = {2008},
  volume    = {103},
  number    = {482},
  month     = {jun},
  pages     = {474--484},
  doi       = {10.1198/016214507000000662},
  groups    = {Fellowship app},
  publisher = {Informa {UK} Limited},
}

@Article{Knox2019,
  author    = {Dean Knox and Teppei Yamamoto and Matthew A. Baum and Adam J. Berinsky},
  title     = {Design, Identification, and Sensitivity Analysis for Patient Preference Trials},
  journal   = {Journal of the American Statistical Association},
  year      = {2019},
  month     = {feb},
  pages     = {1--27},
  doi       = {10.1080/01621459.2019.1585248},
  groups    = {Fellowship app},
  publisher = {Informa {UK} Limited},
}

@Article{Cameron2016,
  author    = {Briana Cameron and Denise A Esserman},
  title     = {Sample size and power for a stratified doubly randomized preference design},
  journal   = {Statistical Methods in Medical Research},
  year      = {2016},
  volume    = {27},
  number    = {7},
  month     = {nov},
  pages     = {2168--2184},
  doi       = {10.1177/0962280216677573},
  groups    = {Fellowship app},
  publisher = {{SAGE} Publications},
}

@Article{Ruecker1989,
  author    = {Gerta Rücker},
  title     = {A two-stage trial design for testing treatment, self-selection and treatment preference effects},
  journal   = {Statistics in Medicine},
  year      = {1989},
  volume    = {8},
  number    = {4},
  month     = {apr},
  pages     = {477--485},
  doi       = {10.1002/sim.4780080411},
  groups    = {Fellowship app},
  publisher = {Wiley},
}

@Article{Silverman1996,
  author    = {W.A Silverman and D.G Altman},
  title     = {Patients{\textquotesingle} preferences and randomised trials},
  journal   = {The Lancet},
  year      = {1996},
  volume    = {347},
  number    = {8995},
  month     = {jan},
  pages     = {171--174},
  doi       = {10.1016/s0140-6736(96)90347-5},
  groups    = {Fellowship app},
  publisher = {Elsevier {BV}},
}

@Article{Mcpherson1997,
  author    = {Klim Mcpherson and Annie R Britton and John E Wennberg},
  title     = {Are Randomized Controlled Trials Controlled? Patient Preferences and Unblind Trials},
  journal   = {Journal of the Royal Society of Medicine},
  year      = {1997},
  volume    = {90},
  number    = {12},
  month     = {dec},
  pages     = {652--656},
  doi       = {10.1177/014107689709001205},
  groups    = {Fellowship app},
  publisher = {{SAGE} Publications},
}

@Article{Walter2012,
  author    = {S.D. Walter and R.M. Turner and P. Macaskill and K.J. McCaffery and L. Irwig},
  title     = {Optimal allocation of participants for the estimation of selection, preference and treatment effects in the two-stage randomised trial design},
  journal   = {Statistics in Medicine},
  year      = {2012},
  volume    = {31},
  number    = {13},
  month     = {feb},
  pages     = {1307--1322},
  doi       = {10.1002/sim.4486},
  groups    = {Fellowship app},
  publisher = {Wiley},
}

@Article{Altman1995,
  author    = {D.G. Altman and J. Whitehead and M.K.B. Parmar and S.P. Stenning and P.M. Fayers and D. Machin},
  title     = {Randomised consent designs in cancer clinical trials},
  journal   = {European Journal of Cancer},
  year      = {1995},
  volume    = {31},
  number    = {12},
  month     = {nov},
  pages     = {1934--1944},
  doi       = {10.1016/0959-8049(95)00470-x},
  groups    = {Fellowship app},
  publisher = {Elsevier {BV}},
}

@Article{Sidani2015,
  author    = {Souraya Sidani and Mary Fox and Dana Epstein},
  title     = {Conducting a two-stage preference trial: Utility and challenges},
  journal   = {International Journal of Nursing Studies},
  year      = {2015},
  volume    = {52},
  number    = {5},
  month     = {may},
  pages     = {1017--1024},
  doi       = {10.1016/j.ijnurstu.2015.02.006},
  groups    = {Fellowship app},
  publisher = {Elsevier {BV}},
}

@Article{Wason2014,
  author    = {James M. S. Wason and Lorenzo Trippa},
  title     = {A comparison of Bayesian adaptive randomization and multi-stage designs for multi-arm clinical trials},
  journal   = {Statistics in Medicine},
  year      = {2014},
  volume    = {33},
  number    = {13},
  month     = {jan},
  pages     = {2206--2221},
  doi       = {10.1002/sim.6086},
  groups    = {Fellowship app},
  publisher = {Wiley},
}

@Article{Guo2018,
  author    = {Beibei Guo and Yeonhee Park and Suyu Liu},
  title     = {A utility-based Bayesian phase I-{II} design for immunotherapy trials with progression-free survival end point},
  journal   = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  year      = {2018},
  volume    = {68},
  number    = {2},
  month     = {may},
  pages     = {411--425},
  doi       = {10.1111/rssc.12288},
  groups    = {Fellowship app},
  publisher = {Wiley},
}

@Article{Rosenberger2012,
  author    = {William F. Rosenberger and Oleksandr Sverdlov and Feifang Hu},
  title     = {Adaptive Randomization for Clinical Trials},
  journal   = {Journal of Biopharmaceutical Statistics},
  year      = {2012},
  volume    = {22},
  number    = {4},
  month     = {jul},
  pages     = {719--736},
  doi       = {10.1080/10543406.2012.676535},
  groups    = {Fellowship app},
  publisher = {Informa {UK} Limited},
}

@Article{Thall2007,
  author    = {Peter F. Thall and J. Kyle Wathen},
  title     = {Practical Bayesian adaptive randomisation in clinical trials},
  journal   = {European Journal of Cancer},
  year      = {2007},
  volume    = {43},
  number    = {5},
  month     = {mar},
  pages     = {859--866},
  doi       = {10.1016/j.ejca.2007.01.006},
  groups    = {Fellowship app},
  publisher = {Elsevier {BV}},
}

@Article{Chapman1999,
  author   = {Chapman, Gretchen B. and Elstein, Arthur S. and Kuzel, Timothy M. and Nadler, Robert B. and Sharifi, Roohollah and Bennett, Charles L.},
  title    = {A multi-attribute model of prostate cancer patients' preferences for health states},
  journal  = {Quality of Life Research},
  year     = {1999},
  volume   = {8},
  number   = {3},
  month    = {May},
  pages    = {171--180},
  issn     = {1573-2649},
  doi      = {10.1023/A:1008850610569},
  url      = {https://doi.org/10.1023/A:1008850610569},
  abstract = {Multi-attribute utility theory (MAUT) provides a way to model decisions involving trade-offs among different aspects or goals of a problem. We used MAUT to model prostate cancer patients' preferences for their own health state and we compared this model to patients' global judgments of health state utility. 57 patients with prostate cancer (mean age = 70) at two Chicago Veterans Administration health clinics were asked to evaluate health states described in terms of five health attributes affected by prostate cancer: pain, mood, sexual function, bladder and bowel function, and fatigue and energy. Each attribute had three levels that were used to form three clinically realistic health state descriptions (A = high, B = moderate, C = low). A fourth personalized health description (P) matched the patient's current health. We first measured patients' preferences using time trade-off (TTO) judgments for the three health states (A, B, and C) and for their own current health state (P). The TTO for the patient's own health state (P) was standardized by comparing it to TTO judgments for states A and C. We next constructed a multi-attribute model using the relative importance of the five attributes. The MAU scores were moderately correlated with the TTO preference judgments for the personalized state (Pearson r = 0.38, N = 57, p < 0.01). Thus, patients' preference judgments are moderately consistent and systematic. MAUT appears to be a potentially feasible method for evaluating preferences of prostate cancer patients and may prove helpful in assisting with patient decision making.},
  day      = {01},
  groups   = {Fellowship app},
}

@Manual{Blocker2018,
  author = {Alexander W Blocker},
  title  = {fastGHQuad: Fast 'Rcpp' Implementation of Gauss-Hermite Quadrature},
  year   = {2018},
  note   = {R package version 1.0},
  url    = {https://CRAN.R-project.org/package=fastGHQuad},
}

@Manual{Genz2017,
  author = {Alan Genz and Frank Bretz and Tetsuhisa Miwa and Xuefei Mi and Friedrich Leisch and Fabian Scheipl and Torsten Hothorn},
  title  = {{mvtnorm}: Multivariate Normal and t Distributions},
  year   = {2017},
  note   = {R package version 1.0-6},
  url    = {https://CRAN.R-project.org/package=mvtnorm},
}

@Manual{Narasimhan2018,
  author = {Balasubramanian Narasimhan and Steven G. Johnson and Thomas Hahn and Annie Bouvier and Kiên Kiêu},
  title  = {cubature: Adaptive Multivariate Integration over Hypercubes},
  year   = {2018},
  note   = {R package version 2.0.3},
  url    = {https://CRAN.R-project.org/package=cubature},
}

@Article{Kent2018,
  author    = {David M Kent and Ewout Steyerberg and David van Klaveren},
  title     = {Personalized evidence based medicine: predictive approaches to heterogeneous treatment effects},
  journal   = {{BMJ}},
  year      = {2018},
  month     = {dec},
  pages     = {k4245},
  doi       = {10.1136/bmj.k4245},
  groups    = {Fellowship app},
  publisher = {{BMJ}},
}

@Article{Eddelbuettel2011,
  author  = {Dirk Eddelbuettel and Romain Fran\c{c}ois},
  title   = {{Rcpp}: Seamless {R} and {C++} Integration},
  journal = {Journal of Statistical Software},
  year    = {2011},
  volume  = {40},
  number  = {8},
  pages   = {1--18},
  doi     = {10.18637/jss.v040.i08},
  url     = {http://www.jstatsoft.org/v40/i08/},
}

@Article{Lambert2000,
  author    = {M Lambert},
  title     = {Incorporating patient preferences into randomized trials},
  journal   = {Journal of Clinical Epidemiology},
  year      = {2000},
  volume    = {53},
  number    = {2},
  month     = {feb},
  pages     = {163--166},
  doi       = {10.1016/s0895-4356(99)00146-8},
  groups    = {Fellowship app},
  publisher = {Elsevier {BV}},
}

@Article{McCaffery2010,
  author    = {Kirsten J. McCaffery and Robin Turner and Petra Macaskill and Stephen D. Walter and Siew Foong Chan and Les Irwig},
  title     = {Determining the Impact of Informed Choice},
  journal   = {Medical Decision Making},
  year      = {2010},
  volume    = {31},
  number    = {2},
  month     = {nov},
  pages     = {229--236},
  doi       = {10.1177/0272989x10379919},
  groups    = {Fellowship app},
  publisher = {{SAGE} Publications},
}

@Article{2008a,
  title     = {Patients{\textquotesingle} preferences within randomised trials: systematic review and patient level meta-analysis},
  journal   = {{BMJ}},
  year      = {2008},
  volume    = {337},
  number    = {oct31 1},
  month     = {oct},
  pages     = {a1864--a1864},
  doi       = {10.1136/bmj.a1864},
  groups    = {Fellowship app},
  publisher = {{BMJ}},
}

@Article{McCann2010,
  author    = {Sharon K McCann and Marion K Campbell and Vikki A Entwistle},
  title     = {Reasons for participating in randomised controlled trials: conditional altruism and considerations for self},
  journal   = {Trials},
  year      = {2010},
  volume    = {11},
  number    = {1},
  month     = {mar},
  doi       = {10.1186/1745-6215-11-31},
  groups    = {Fellowship app},
  publisher = {Springer Nature},
}

@Article{Edginton2017,
  author    = {Elizabeth Edginton and Rebecca Walwyn and Kayleigh Burton and Robert Cicero and Liz Graham and Sadie Reed and Sandy Tubeuf and Maureen Twiddy and Alex Wright-Hughes and Lynda Ellis and Dot Evans and Tom Hughes and Nick Midgley and Paul Wallis and David Cottrell},
  title     = {{TIGA}-{CUB} {\textendash} manualised psychoanalytic child psychotherapy versus treatment as usual for children aged 5{\textendash}11 years with treatment-resistant conduct disorders and their primary carers: study protocol for a randomised controlled feasibility trial},
  journal   = {Trials},
  year      = {2017},
  volume    = {18},
  number    = {1},
  month     = {sep},
  doi       = {10.1186/s13063-017-2166-2},
  publisher = {Springer Nature},
}

@Article{Acuna2019,
  author    = {Sergio A. Acuna and Tyler R. Chesney and Nancy N. Baxter},
  title     = {Incorporating Patient Preferences in Noninferiority Trials},
  journal   = {{JAMA}},
  year      = {2019},
  month     = {jun},
  doi       = {10.1001/jama.2019.7059},
  groups    = {Fellowship app},
  publisher = {American Medical Association ({AMA})},
}

@Article{Donovan2019,
  author    = {Jenny L. Donovan and Brent Opmeer and Grace J. Young and Nicola Mills and Richard M. Martin and J. Athene Lane and Chris Metcalfe and Tim J. Peters and Michael Davis and Emma L. Turner and Eleanor Walsh and David E. Neal and Freddie C. Hamdy and Peter Holding and Malcolm Mason and James W.F. Catto and Derek J. Rosario and John Staffurth and Howard Kynaston and Owen Hughes and Prasad Bollina and Alan Doherty and Vincent Gnanapragasam and Roger Kockelbergh and Alan Paul and Edgar Paez and David Gillatt and Edward Rowe and Jon Oxley},
  title     = {Factors associated with trial recruitment, preferences, and treatments received were elucidated in a comprehensive cohort study},
  journal   = {Journal of Clinical Epidemiology},
  year      = {2019},
  volume    = {113},
  month     = {sep},
  pages     = {200--213},
  doi       = {10.1016/j.jclinepi.2019.05.036},
  groups    = {Fellowship app},
  publisher = {Elsevier {BV}},
}

@Article{Gillies2019,
  author    = {Katie Gillies and Marion K. Campbell},
  title     = {Development and evaluation of decision aids for people considering taking part in a clinical trial: a conceptual framework},
  journal   = {Trials},
  year      = {2019},
  volume    = {20},
  number    = {1},
  month     = {jul},
  doi       = {10.1186/s13063-019-3489-y},
  groups    = {Fellowship app},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Hunter2019,
  author    = {David J. Hunter and Dan L. Longo},
  title     = {The Precision of Evidence Needed to Practice {\textquotedblleft}Precision Medicine{\textquotedblright}},
  journal   = {New England Journal of Medicine},
  year      = {2019},
  volume    = {380},
  number    = {25},
  month     = {jun},
  pages     = {2472--2474},
  doi       = {10.1056/nejme1906088},
  groups    = {Fellowship app},
  publisher = {Massachusetts Medical Society},
}

@Comment{jabref-meta: databaseType:biblatex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:PACE\;0\;1\;\;\;\;;
1 StaticGroup:Barkham\;0\;1\;\;\;\;;
1 StaticGroup:SSR\;0\;1\;\;\;\;;
1 StaticGroup:Complex Interventions\;0\;1\;\;\;\;;
1 StaticGroup:Multilevel\;0\;1\;\;\;\;;
1 StaticGroup:Bayesian\;0\;1\;\;\;\;;
1 StaticGroup:Decision theory\;0\;1\;\;\;\;;
1 StaticGroup:Phase II\;0\;1\;\;\;\;;
1 StaticGroup:Pilot/feasibility\;0\;1\;\;\;\;;
1 StaticGroup:Optimisation\;0\;1\;\;\;\;;
1 StaticGroup:Multiple endpoints\;0\;1\;\;\;\;;
1 StaticGroup:Examples\;0\;1\;\;\;\;;
1 StaticGroup:Elicitation\;0\;1\;\;\;\;;
1 StaticGroup:multi-D / sim SS\;0\;1\;\;\;\;;
1 StaticGroup:Seminar?\;0\;1\;\;\;\;;
1 StaticGroup:Fellowship app\;0\;1\;\;\;\;;
}

@Comment{jabref-meta: groupsversion:
3;
}
